{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1n2KwjcR5O31eWx4BtUhOCHcx4bdXbaIU","timestamp":1758639824825}],"authorship_tag":"ABX9TyOtDt4rvHCL5w36KefYqo55"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!cd /tmp && pip install beautifulsoup4 html5lib python-dateutil requests pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"i7T3aCYPp8Xp","executionInfo":{"status":"ok","timestamp":1758641159413,"user_tz":300,"elapsed":11574,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"57cc0a4e-270e-4df2-a17e-14c118a66faf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (1.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib) (0.5.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n"]}]},{"cell_type":"code","source":["!pip install pdfplumber -q\n","\n","import pandas as pd\n","import requests\n","import pdfplumber\n","from io import BytesIO\n","from datetime import datetime\n","import os\n","import time\n","from google.colab import drive\n","import re\n","\n","# Mount Google Drive\n","print(\"Mounting Google Drive...\")\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# Set output directory\n","output_dir = '/content/drive/MyDrive/PressConferences'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","os.chdir(output_dir)\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# Verify write access to output directory\n","test_file = os.path.join(output_dir, 'test_write.txt')\n","try:\n","    with open(test_file, 'w') as f:\n","        f.write('Test write to Google Drive')\n","    print(f\"Write test successful: {test_file}\")\n","    os.remove(test_file)\n","except Exception as e:\n","    print(f\"Error writing to Google Drive: {e}\")\n","    exit()\n","\n","# Hardcoded list of FOMC meeting dates from April 2011 to July 2025\n","fomc_dates = [\n","    # 2011\n","    '2011-04-27', '2011-06-22', '2011-08-09', '2011-09-21', '2011-11-02', '2011-12-13',\n","    # 2012\n","    '2012-01-25', '2012-03-13', '2012-04-25', '2012-06-20', '2012-08-01', '2012-09-13', '2012-12-12',\n","    # 2013\n","    '2013-03-20', '2013-05-01', '2013-06-19', '2013-07-31', '2013-09-18', '2013-10-30', '2013-12-18',\n","    # 2014\n","    '2014-01-29', '2014-03-19', '2014-04-30', '2014-06-18', '2014-07-30', '2014-09-17', '2014-10-29', '2014-12-17',\n","    # 2015\n","    '2015-01-28', '2015-03-18', '2015-04-29', '2015-06-17', '2015-07-29', '2015-09-17', '2015-10-28', '2015-12-16',\n","    # 2016\n","    '2016-01-27', '2016-03-16', '2016-04-27', '2016-06-15', '2016-07-27', '2016-09-21', '2016-11-02', '2016-12-14',\n","    # 2017\n","    '2017-02-01', '2017-03-15', '2017-05-03', '2017-06-14', '2017-07-26', '2017-09-20', '2017-11-01', '2017-12-13',\n","    # 2018\n","    '2018-01-31', '2018-03-21', '2018-05-02', '2018-06-13', '2018-08-01', '2018-09-26', '2018-11-08', '2018-12-19',\n","    # 2019\n","    '2019-01-30', '2019-03-20', '2019-05-01', '2019-06-19', '2019-07-31', '2019-09-18', '2019-10-30', '2019-12-11',\n","    # 2020\n","    '2020-01-29', '2020-03-15', '2020-04-29', '2020-06-10', '2020-07-29', '2020-09-16', '2020-11-05', '2020-12-16',\n","    # 2021\n","    '2021-01-27', '2021-03-17', '2021-04-28', '2021-06-16', '2021-07-28', '2021-09-22', '2021-11-03', '2021-12-15',\n","    # 2022\n","    '2022-01-26', '2022-03-16', '2022-05-04', '2022-06-15', '2022-07-27', '2022-09-21', '2022-11-02', '2022-12-14',\n","    # 2023\n","    '2023-02-01', '2023-03-22', '2023-05-03', '2023-06-14', '2023-07-26', '2023-09-20', '2023-11-01', '2023-12-13',\n","    # 2024\n","    '2024-01-31', '2024-03-20', '2024-05-01', '2024-06-12', '2024-07-31', '2024-09-18', '2024-11-07', '2024-12-18',\n","    # 2025\n","    '2025-01-29', '2025-03-19', '2025-05-07', '2025-06-18', '2025-07-30'\n","]\n","\n","# Filter dates to ensure they are on or before the current date (September 18, 2025)\n","current_date = datetime(2025, 9, 18)\n","fomc_dates = [date for date in fomc_dates if datetime.strptime(date, '%Y-%m-%d') <= current_date]\n","\n","# DEBUG MODE: Set to True to scrape only the most recent conference for testing\n","DEBUG_MODE = False\n","\n","if DEBUG_MODE:\n","    fomc_dates = [fomc_dates[-1]]  # Only process the most recent date\n","    print(f\"DEBUG MODE: Only processing {fomc_dates[0]}\")\n","else:\n","    print(f\"Processing {len(fomc_dates)} total dates\")\n","\n","# Load existing CSV if it exists\n","csv_path = os.path.join(output_dir, 'fomc_press_conferences.csv')\n","existing_dates = set()\n","existing_ids = set()\n","try:\n","    existing_df = pd.read_csv(csv_path)\n","    existing_ids = set(existing_df['id'])\n","    print(f\"Loaded existing CSV with {len(existing_ids)} records\")\n","except FileNotFoundError:\n","    print(\"No existing CSV found, starting fresh\")\n","except Exception as e:\n","    print(f\"Error reading existing CSV: {e}\")\n","    existing_df = pd.DataFrame(columns=['id', 'date', 'source_url', 'text', 'speaker'])\n","\n","# Function to determine speaker based on date\n","def get_speaker(date_str):\n","    date = datetime.strptime(date_str, '%Y-%m-%d')\n","    if date < datetime(2014, 2, 1):\n","        return 'Bernanke'\n","    elif date < datetime(2018, 2, 1):\n","        return 'Yellen'\n","    else:\n","        return 'Powell'\n","\n","# Function to clean text - removes page numbers and fixes spacing issues\n","def clean_text(text):\n","    # Remove various page headers and footers\n","    text = re.sub(r'Page\\s+\\d+\\s+of\\s+\\d+', '', text)\n","    text = re.sub(r'\\d+\\s+of\\s+\\d+', '', text)\n","    text = re.sub(r'Transcript of Chair.*?Press Conference.*?\\d{4}', '', text)\n","    text = re.sub(r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\s+Chairman.*?Press Conference.*?FINAL', '', text)\n","    text = re.sub(r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\s+Chair.*?Press Conference.*?FINAL', '', text)\n","    text = re.sub(r'FINAL\\s*Page', '', text)\n","\n","    # Fix specific mid-word spacing issues that occur in PDF extraction\n","    # Only fix cases where we know the words should be joined\n","    text = re.sub(r'\\bFederal\\s+Open\\s+M\\s+arket\\b', 'Federal Open Market', text)\n","    text = re.sub(r'\\bFederal\\s+R\\s+eserve\\b', 'Federal Reserve', text)\n","    text = re.sub(r'\\bC\\s+ommittee\\b', 'Committee', text)\n","    text = re.sub(r'\\bM\\s+arket\\b', 'Market', text)\n","    text = re.sub(r'\\bR\\s+eserve\\b', 'Reserve', text)\n","    text = re.sub(r'\\bP\\s+owell\\b', 'Powell', text)\n","    text = re.sub(r'\\bY\\s+ellen\\b', 'Yellen', text)\n","    text = re.sub(r'\\bB\\s+ernanke\\b', 'Bernanke', text)\n","\n","    # Clean up multiple spaces and tabs, but preserve single spaces between words\n","    text = re.sub(r'[ \\t]+', ' ', text)\n","\n","    # Clean up multiple newlines\n","    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n","\n","    # Remove duplicate lines while preserving structure\n","    lines = text.split('\\n')\n","    seen = set()\n","    cleaned_lines = []\n","    for line in lines:\n","        line_stripped = line.strip()\n","        if line_stripped and line_stripped not in seen:\n","            cleaned_lines.append(line_stripped)\n","            seen.add(line_stripped)\n","        elif line_stripped == '' and (not cleaned_lines or cleaned_lines[-1] != ''):\n","            # Only add empty lines if the previous line wasn't empty\n","            cleaned_lines.append('')\n","\n","    return '\\n'.join(cleaned_lines).strip()\n","\n","# Function to extract text from PDF using pdfplumber\n","def extract_text_from_pdf(pdf_content):\n","    with pdfplumber.open(BytesIO(pdf_content)) as pdf:\n","        full_text = ''\n","        for page in pdf.pages:\n","            page_text = page.extract_text()\n","            if page_text:\n","                full_text += page_text + '\\n'\n","    return full_text\n","\n","# Function to extract both chair's text and other speakers' text from PDF\n","def extract_text_by_speaker(pdf_content, date_str):\n","    # Use pdfplumber instead of PyPDF2 for cleaner text extraction\n","    full_text = extract_text_from_pdf(pdf_content)\n","\n","    # Log first 500 characters of raw text for debugging\n","    print(f\"Raw text preview for {date_str} (first 500 chars): {full_text[:500]}\")\n","\n","    speaker = get_speaker(date_str)\n","    chair_text = ''\n","    other_text = ''\n","\n","    # Split text into lines for processing\n","    lines = full_text.split('\\n')\n","    current_speaker = None\n","    current_text = []\n","\n","    for line in lines:\n","        line = line.strip()\n","        if not line:\n","            continue\n","\n","        # Check if this line indicates the chair is speaking\n","        if (line.startswith(f'CHAIR {speaker.upper()}.') or\n","            line.startswith(f'CHAIRMAN {speaker.upper()}.') or\n","            line.startswith('CHAIR POWELL.') or\n","            line.startswith('CHAIR BERNANKE.') or\n","            line.startswith('CHAIR YELLEN.') or\n","            line.startswith('CHAIRWOMAN YELLEN.')):\n","\n","            # Save previous speaker's text\n","            if current_speaker and current_text:\n","                text_block = '\\n'.join(current_text)\n","                if current_speaker == 'chair':\n","                    chair_text += text_block + '\\n'\n","                else:\n","                    other_text += text_block + '\\n'\n","\n","            current_speaker = 'chair'\n","            current_text = [line]\n","\n","        # Check if this line indicates someone else is speaking (more permissive patterns)\n","        elif (line.startswith('QUESTION.') or\n","              line.startswith('MS.') or line.startswith('MR.') or\n","              line.startswith('QUESTIONER.') or\n","              # Look for reporter names or other officials\n","              re.match(r'^[A-Z][A-Z\\s]+[A-Z]\\.', line) or\n","              # Look for patterns like \"STEVE LIESMAN.\" or \"HOWARD SCHNEIDER.\"\n","              re.match(r'^[A-Z]+\\s+[A-Z]+\\.', line)):\n","\n","            # Save previous speaker's text\n","            if current_speaker and current_text:\n","                text_block = '\\n'.join(current_text)\n","                if current_speaker == 'chair':\n","                    chair_text += text_block + '\\n'\n","                else:\n","                    other_text += text_block + '\\n'\n","\n","            current_speaker = 'other'\n","            current_text = [line]\n","\n","        # Continue with current speaker\n","        elif current_speaker and line:\n","            current_text.append(line)\n","\n","    # Don't forget the last speaker's text\n","    if current_speaker and current_text:\n","        text_block = '\\n'.join(current_text)\n","        if current_speaker == 'chair':\n","            chair_text += text_block + '\\n'\n","        else:\n","            other_text += text_block + '\\n'\n","\n","    # Clean both texts\n","    chair_text = clean_text(chair_text)\n","    other_text = clean_text(other_text)\n","\n","    print(f\"Chair text length for {date_str}: {len(chair_text)}\")\n","    print(f\"Other text length for {date_str}: {len(other_text)}\")\n","    print(f\"Other text preview for {date_str} (first 200 chars): {other_text[:200]}\")\n","\n","    return chair_text, other_text\n","\n","# Function to scrape press conference for a given date\n","def scrape_press_conf(date):\n","    date_obj = datetime.strptime(date, '%Y-%m-%d')\n","    date_str = date_obj.strftime('%Y%m%d')\n","\n","    # Check if both records already exist\n","    chair_id = f\"presschair_{date_str}\"\n","    other_id = f\"pressother_{date_str}\"\n","\n","    if chair_id in existing_ids and other_id in existing_ids:\n","        print(f\"Both records for {date} already exist, skipping\")\n","        return []\n","\n","    # URL format\n","    url = f'https://www.federalreserve.gov/mediacenter/files/FOMCpresconf{date_str}.pdf'\n","\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","        print(f\"Response status for {date} at {url}: {response.status_code}, Content length: {len(response.content)}\")\n","\n","        chair_text, other_text = extract_text_by_speaker(response.content, date)\n","\n","        results = []\n","        speaker_name = get_speaker(date)\n","\n","        # Add chair record if it has meaningful content and doesn't exist\n","        if len(chair_text) > 50 and \"CHAIR\" in chair_text and chair_id not in existing_ids:\n","            results.append({\n","                'id': chair_id,\n","                'date': date,\n","                'source_url': url,\n","                'text': chair_text,\n","                'speaker': speaker_name\n","            })\n","            print(f\"Chair record: {len(chair_text)} characters for {date}\")\n","\n","        # Add other speakers record if it has meaningful content and doesn't exist\n","        if len(other_text) > 50 and other_id not in existing_ids:\n","            results.append({\n","                'id': other_id,\n","                'date': date,\n","                'source_url': url,\n","                'text': other_text,\n","                'speaker': 'Other'\n","            })\n","            print(f\"Other speakers record: {len(other_text)} characters for {date}\")\n","\n","        if results:\n","            print(f\"Success: Created {len(results)} records for {date}\")\n","            return results\n","        else:\n","            print(f\"No new records needed for {date}\")\n","            return []\n","\n","    except requests.exceptions.HTTPError as e:\n","        print(f\"HTTP Error for {date} at {url}: {e} (Status: {e.response.status_code if e.response else 'No response'})\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request error for {date} at {url}: {e}\")\n","    except Exception as e:\n","        print(f\"General error for {date} at {url}: {e}\")\n","\n","    print(f\"Failed to scrape press conference for {date}\")\n","    return []\n","\n","# Scrape and save data\n","data = []\n","successful_scrapes = 0\n","failed_scrapes = 0\n","successes_by_year = {}\n","\n","for date in fomc_dates:\n","    year = date.split('-')[0]\n","    if year not in successes_by_year:\n","        successes_by_year[year] = 0\n","\n","    results = scrape_press_conf(date)\n","    if results:\n","        data.extend(results)\n","        successful_scrapes += len(results)\n","        successes_by_year[year] += len(results)\n","    else:\n","        failed_scrapes += 1\n","\n","    # Save incrementally every 10 dates or at the end\n","    if len(data) >= 20 or (date == fomc_dates[-1] and data):\n","        new_df = pd.DataFrame(data)\n","        try:\n","            if os.path.exists(csv_path):\n","                existing_df = pd.read_csv(csv_path)\n","                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n","            else:\n","                combined_df = new_df\n","            combined_df.to_csv(csv_path, index=False)\n","            print(f\"Saved {len(data)} new records to {csv_path}\")\n","            data = []  # Clear data after saving\n","        except Exception as e:\n","            print(f\"Error saving to CSV: {e}\")\n","\n","    time.sleep(1)  # Avoid overwhelming the server\n","\n","# Print summary of successes by year\n","print(\"\\nSuccessful scrapes by year:\")\n","for year, count in successes_by_year.items():\n","    print(f\"{year}: {count} records\")\n","\n","# Print overall summary\n","print(f\"\\nScraping complete: {successful_scrapes} successful records, {failed_scrapes} failed dates\")\n","if os.path.exists(csv_path):\n","    try:\n","        final_df = pd.read_csv(csv_path)\n","        print(f\"Final CSV contains {len(final_df)} records\")\n","        print(f\"File size: {os.path.getsize(csv_path) / (1024 * 1024):.2f} MB\")\n","\n","        # Show breakdown by speaker\n","        speaker_counts = final_df['speaker'].value_counts()\n","        print(\"\\nRecords by speaker:\")\n","        for speaker, count in speaker_counts.items():\n","            print(f\"{speaker}: {count} records\")\n","\n","    except Exception as e:\n","        print(f\"Error reading final CSV: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5NM6M_YQMyF","outputId":"a2fc387f-b91e-4e83-ebc1-e1bc7319a8b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/PressConferences\n","Write test successful: /content/drive/MyDrive/PressConferences/test_write.txt\n","Processing 113 total dates\n","No existing CSV found, starting fresh\n","Response status for 2011-04-27 at https://www.federalreserve.gov/mediacenter/files/FOMCpresconf20110427.pdf: 200, Content length: 76622\n"]}]}]}