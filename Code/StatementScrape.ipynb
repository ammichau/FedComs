{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMc7C7ELoOhtGLaAbVGuoI6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Rot40QDe4DD","executionInfo":{"status":"ok","timestamp":1758722246541,"user_tz":300,"elapsed":21968,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"24d79f78-0ee0-4181-e0b8-f5210999bdbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/Statements\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import os\n","output_dir = '/content/drive/MyDrive/Statements'\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","os.chdir(output_dir)\n","\n","# Verify the current working directory\n","print(f\"Current working directory: {os.getcwd()}\")"]},{"cell_type":"code","source":["!cd /tmp && pip install beautifulsoup4 html5lib python-dateutil requests pandas\n","!pip install dropbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"T7jiOQBMfEf0","outputId":"d1e6df4a-20af-420c-f109-062d6398be31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (1.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib) (0.5.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Collecting dropbox\n","  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.12/dist-packages (from dropbox) (2.32.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from dropbox) (1.17.0)\n","Collecting stone<3.3.3,>=2 (from dropbox)\n","  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.16.2->dropbox) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.16.2->dropbox) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.16.2->dropbox) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.16.2->dropbox) (2025.8.3)\n","Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.12/dist-packages (from stone<3.3.3,>=2->dropbox) (3.11)\n","Downloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: stone, dropbox\n","Successfully installed dropbox-12.0.2 stone-3.3.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from datetime import datetime\n","import os\n","import time\n","from google.colab import drive\n","import re\n","import uuid\n","from requests.adapters import HTTPAdapter\n","from urllib3.util.retry import Retry\n","\n","\n","# Mount Google Drive\n","print(\"Mounting Google Drive...\")\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# Set output directory\n","output_dir = '/content/drive/MyDrive/Statements'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","os.chdir(output_dir)\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# Verify write access to output directory\n","test_file = os.path.join(output_dir, 'test_write.txt')\n","try:\n","    with open(test_file, 'w') as f:\n","        f.write('Test write to Google Drive')\n","    print(f\"Write test successful: {test_file}\")\n","    os.remove(test_file)\n","except Exception as e:\n","    print(f\"Error writing to Google Drive: {e}\")\n","    exit()\n","\n","# Hardcoded list of FOMC meeting dates from 2000 to January 2025\n","fomc_dates = [\n","    # 2000\n","    '2000-02-02', '2000-03-21', '2000-05-16', '2000-06-28', '2000-08-22', '2000-10-03', '2000-11-15', '2000-12-19',\n","    # 2001\n","    '2001-01-31', '2001-03-20', '2001-05-15', '2001-06-27', '2001-08-21', '2001-10-02', '2001-11-06', '2001-12-11',\n","    # 2002\n","    '2002-01-30', '2002-03-19', '2002-05-07', '2002-06-26', '2002-08-13', '2002-09-24', '2002-11-06', '2002-12-10',\n","    # 2003\n","    '2003-01-29', '2003-03-18', '2003-05-06', '2003-06-25', '2003-08-12', '2003-09-16', '2003-10-28', '2003-12-09',\n","    # 2004\n","    '2004-01-28', '2004-03-16', '2004-05-04', '2004-06-30', '2004-08-10', '2004-09-21', '2004-11-10', '2004-12-14',\n","    # 2005\n","    '2005-02-02', '2005-03-22', '2005-05-03', '2005-06-30', '2005-08-09', '2005-09-20', '2005-11-01', '2005-12-13',\n","    # 2006\n","    '2006-01-31', '2006-03-28', '2006-05-10', '2006-06-29', '2006-08-08', '2006-09-20', '2006-10-25', '2006-12-12',\n","    # 2007\n","    '2007-01-31', '2007-03-21', '2007-05-09', '2007-06-28', '2007-08-07', '2007-09-18', '2007-10-31', '2007-12-11',\n","    # 2008\n","    '2008-01-30', '2008-03-18', '2008-04-30', '2008-06-25', '2008-08-05', '2008-09-16', '2008-10-29', '2008-12-16',\n","    # 2009\n","    '2009-01-28', '2009-03-18', '2009-04-29', '2009-06-24', '2009-08-12', '2009-09-23', '2009-11-04', '2009-12-16',\n","    # 2010\n","    '2010-01-27', '2010-03-16', '2010-04-28', '2010-06-23', '2010-08-10', '2010-09-21', '2010-11-03', '2010-12-14',\n","    # 2011\n","    '2011-01-26', '2011-03-15', '2011-04-27', '2011-06-22', '2011-08-09', '2011-09-21', '2011-11-02', '2011-12-13',\n","    # 2012\n","    '2012-01-25', '2012-03-13', '2012-04-25', '2012-06-20', '2012-08-01', '2012-09-13', '2012-12-12',\n","    # 2013\n","    '2013-03-20', '2013-05-01', '2013-06-19', '2013-07-31', '2013-09-18', '2013-10-30', '2013-12-18',\n","    # 2014\n","    '2014-01-29', '2014-03-19', '2014-04-30', '2014-06-18', '2014-07-30', '2014-09-17', '2014-10-29', '2014-12-17',\n","    # 2015\n","    '2015-01-28', '2015-03-18', '2015-04-29', '2015-06-17', '2015-07-29', '2015-09-17', '2015-10-28', '2015-12-16',\n","    # 2016\n","    '2016-01-27', '2016-03-16', '2016-04-27', '2016-06-15', '2016-07-27', '2016-09-21', '2016-11-02', '2016-12-14',\n","    # 2017\n","    '2017-02-01', '2017-03-15', '2017-05-03', '2017-06-14', '2017-07-26', '2017-09-20', '2017-11-01', '2017-12-13',\n","    # 2018\n","    '2018-01-31', '2018-03-21', '2018-05-02', '2018-06-13', '2018-08-01', '2018-09-26', '2018-11-08', '2018-12-19',\n","    # 2019\n","    '2019-01-30', '2019-03-20', '2019-05-01', '2019-06-19', '2019-07-31', '2019-09-18', '2019-10-30', '2019-12-11',\n","    # 2020\n","    '2020-01-29', '2020-03-15', '2020-04-29', '2020-06-10', '2020-07-29', '2020-09-16', '2020-11-05', '2020-12-16',\n","    # 2021\n","    '2021-01-27', '2021-03-17', '2021-04-28', '2021-06-16', '2021-07-28', '2021-09-22', '2021-11-03', '2021-12-15',\n","    # 2022\n","    '2022-01-26', '2022-03-16', '2022-05-04', '2022-06-15', '2022-07-27', '2022-09-21', '2022-11-02', '2022-12-14',\n","    # 2023\n","    '2023-02-01', '2023-03-22', '2023-05-03', '2023-06-14', '2023-07-26', '2023-09-20', '2023-11-01', '2023-12-13',\n","    # 2024\n","    '2024-01-31', '2024-03-20', '2024-05-01', '2024-06-12', '2024-07-31', '2024-09-18', '2024-11-07', '2024-12-18',\n","    # 2025\n","    '2025-01-29'\n","]\n","\n","# Filter dates to ensure they are on or before the current date (September 18, 2025)\n","current_date = datetime(2025, 9, 18)\n","fomc_dates = [date for date in fomc_dates if datetime.strptime(date, '%Y-%m-%d') <= current_date]\n","\n","# Load existing CSV if it exists\n","csv_path = os.path.join(output_dir, 'fomc_statements.csv')\n","existing_dates = set()\n","try:\n","    existing_df = pd.read_csv(csv_path)\n","    existing_dates = set(existing_df['date'])\n","    print(f\"Loaded existing CSV with {len(existing_dates)} records\")\n","except FileNotFoundError:\n","    print(\"No existing CSV found, starting fresh\")\n","except Exception as e:\n","    print(f\"Error reading existing CSV: {e}\")\n","    existing_df = pd.DataFrame(columns=['id', 'date', 'source_url', 'text'])\n","\n","# Set up requests session with retry logic\n","session = requests.Session()\n","retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n","session.mount('https://', HTTPAdapter(max_retries=retries))\n","\n","# Function to clean text and remove duplicates\n","def clean_text(text):\n","    # Remove JavaScript warning\n","    text = re.sub(r'Please enable JavaScript if it is disabled in your browser.*?\\n', '', text, flags=re.DOTALL)\n","    # Remove excessive newlines and redundant sections\n","    text = re.sub(r'\\n\\s*\\n', '\\n', text.strip())\n","    # Remove repeated sections by keeping only the first occurrence\n","    lines = text.split('\\n')\n","    seen = set()\n","    cleaned_lines = []\n","    for line in lines:\n","        if line.strip() and line.strip() not in seen:\n","            cleaned_lines.append(line)\n","            seen.add(line.strip())\n","        elif line.strip() == '':\n","            cleaned_lines.append(line)\n","    return '\\n'.join(cleaned_lines)\n","\n","# Function to scrape statements for a given date\n","def scrape_statements(date):\n","    date_obj = datetime.strptime(date, '%Y-%m-%d')\n","    year = date_obj.year\n","    date_str = date_obj.strftime('%Y%m%d')\n","    statement_id = f\"statement_{date_str}\"  # Create ID in format statement_YYYYMMDD\n","\n","    # Define URL formats based on year and specific dates\n","    url_monetary = f'https://www.federalreserve.gov/boarddocs/press/monetary/{year}/{date_str}/'\n","    url_newsevents = f'https://www.federalreserve.gov/newsevents/pressreleases/monetary{date_str}a.htm'\n","    url_general = f'https://www.federalreserve.gov/boarddocs/press/general/{year}/{date_str}/'\n","\n","    # Specific URLs for known cases\n","    specific_urls = {\n","        '2000-02-02': 'https://www.federalreserve.gov/boarddocs/press/general/2000/20000202/',\n","        '2002-03-19': 'https://www.federalreserve.gov/boarddocs/press/general/2002/20020319/',\n","        '2005-05-03': 'https://www.federalreserve.gov/boarddocs/press/monetary/2005/20050503/',\n","        '2007-06-28': 'https://www.federalreserve.gov/newsevents/pressreleases/monetary20070618a.htm'\n","    }\n","\n","    # Select URLs to try based on year and specific cases\n","    if date in specific_urls:\n","        urls_to_try = [specific_urls[date]]\n","    elif year <= 2002:\n","        urls_to_try = [url_general, url_monetary, url_newsevents]\n","    elif year <= 2005:\n","        urls_to_try = [url_monetary, url_newsevents, url_general]\n","    else:\n","        urls_to_try = [url_newsevents, url_monetary]\n","\n","    for url in urls_to_try:\n","        try:\n","            response = session.get(url, timeout=10)\n","            response.raise_for_status()\n","            print(f\"Response status for {date} at {url}: {response.status_code}, Content length: {len(response.text)}\")\n","\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # Try multiple selectors to extract content\n","            content_selectors = [\n","                ('div', {'id': 'content'}),\n","                ('div', {'class': 'article'}),\n","                ('div', {'class': 'col-xs-12 col-sm-12 col-md-10'}),\n","                ('div', {'class': 'panel-body'}),\n","                ('div', {'class': 'panel panel-default'}),\n","                ('body', {})\n","            ]\n","\n","            text = None\n","            for tag, attrs in content_selectors:\n","                content = soup.find(tag, attrs)\n","                if content:\n","                    text = content.get_text(separator='\\n', strip=True)\n","                    text = clean_text(text)\n","                    if len(text) > 50 and \"Federal Open Market Committee\" in text:\n","                        print(f\"Success: Scraped {len(text)} characters for {date} from {url}\")\n","                        return {'id': statement_id, 'date': date, 'source_url': url, 'text': text}\n","\n","            # Fallback to body text if no specific content div is found\n","            if not text:\n","                text = soup.get_text(separator='\\n', strip=True)\n","                text = clean_text(text)\n","                if len(text) > 50 and \"Federal Open Market Committee\" in text:\n","                    print(f\"Success (fallback): Scraped {len(text)} characters for {date} from {url}\")\n","                    return {'id': statement_id, 'date': date, 'source_url': url, 'text': text}\n","\n","            print(f\"No meaningful content found for {date} at {url} (text length: {len(text) if text else 0})\")\n","        except requests.exceptions.HTTPError as e:\n","            print(f\"HTTP Error for {date} at {url}: {e} (Status: {e.response.status_code if e.response else 'No response'})\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Request error for {date} at {url}: {e}\")\n","        except Exception as e:\n","            print(f\"General error for {date} at {url}: {e}\")\n","\n","    print(f\"Failed to scrape statement for {date}\")\n","    return None\n","\n","# Scrape and save data\n","data = []\n","successful_scrapes = 0\n","failed_scrapes = 0\n","\n","for date in fomc_dates:\n","    if date in existing_dates:\n","        print(f\"Skipping {date}: already in CSV\")\n","        continue\n","\n","    result = scrape_statements(date)\n","    if result:\n","        data.append(result)\n","        successful_scrapes += 1\n","    else:\n","        failed_scrapes += 1\n","\n","    # Save incrementally every 10 dates or at the end\n","    if len(data) >= 10 or (date == fomc_dates[-1] and data):\n","        new_df = pd.DataFrame(data)\n","        try:\n","            if os.path.exists(csv_path):\n","                existing_df = pd.read_csv(csv_path)\n","                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n","            else:\n","                combined_df = new_df\n","            combined_df.to_csv(csv_path, index=False)\n","            print(f\"Saved {len(data)} new records to {csv_path}\")\n","            data = []  # Clear data after saving\n","        except Exception as e:\n","            print(f\"Error saving to CSV: {e}\")\n","\n","    time.sleep(1)  # Avoid overwhelming the server\n","\n","# Print summary\n","print(f\"\\nScraping complete: {successful_scrapes} successful, {failed_scrapes} failed\")\n","if os.path.exists(csv_path):\n","    try:\n","        final_df = pd.read_csv(csv_path)\n","        print(f\"Final CSV contains {len(final_df)} records\")\n","        print(f\"File size: {os.path.getsize(csv_path) / (1024 * 1024):.2f} MB\")\n","    except Exception as e:\n","        print(f\"Error reading final CSV: {e}\")"],"metadata":{"id":"bwkBEp8Qny5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T0KC7IZ-irFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Additional Clean\n","import pandas as pd\n","import re\n","import os\n","\n","# Define paths\n","input_file = '/content/drive/MyDrive/FedComs/Statements/fomc_statements.csv'\n","output_dir = '/content/drive/MyDrive/Statements'\n","\n","# Read the original statements\n","print(\"Reading FOMC statements...\")\n","df = pd.read_csv(input_file)\n","print(f\"Loaded {len(df)} statements\")\n","\n","# Delete statement_20081216\n","print(\"\\nDeleting statement_20081216...\")\n","df = df[df['id'] != 'statement_20081216'].copy()\n","print(f\"Statements remaining: {len(df)}\")\n","\n","# Convert date to datetime for easier filtering\n","df['date'] = pd.to_datetime(df['date'])\n","\n","def clean_text_old(text, date):\n","    \"\"\"Clean text for statements from 2005-12-13 and prior.\"\"\"\n","    # Delete text before and including \"For immediate release\"\n","    match = re.search(r'For immediate release', text, re.IGNORECASE)\n","    if match:\n","        text = text[match.end():]\n","\n","    # Delete text after and including \"YYYY Monetary policy\" where YYYY is the year\n","    year = date.year\n","    pattern = rf'{year}\\s+Monetary policy'\n","    match = re.search(pattern, text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","def clean_text_middle(text):\n","    \"\"\"Clean text for statements from 2006-01-31 to 2020-03-05.\"\"\"\n","    # Delete text before and including the first mention of \"share\"\n","    match = re.search(r'share', text, re.IGNORECASE)\n","    if match:\n","        text = text[match.end():]\n","\n","    # Delete text after and including \"Last Update\"\n","    match = re.search(r'Last Update', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","def clean_text_recent(text):\n","    \"\"\"Clean text for statements after 2020-03-05.\"\"\"\n","    # Find \"For release\" first\n","    for_release_match = re.search(r'For release', text, re.IGNORECASE)\n","\n","    if for_release_match:\n","        # Look for \"share\" after \"For release\"\n","        text_after_release = text[for_release_match.end():]\n","        share_match = re.search(r'share', text_after_release, re.IGNORECASE)\n","\n","        if share_match:\n","            # Calculate position in original text\n","            start_pos = for_release_match.end() + share_match.end()\n","            text = text[start_pos:]\n","    else:\n","        # If no \"For release\" found, just look for \"share\"\n","        match = re.search(r'share', text, re.IGNORECASE)\n","        if match:\n","            text = text[match.end():]\n","\n","    # Delete text after and including \"Last Update\"\n","    match = re.search(r'Last Update', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    # Delete text after and including \"For media inquiries\"\n","    match = re.search(r'For media inquiries', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","# Apply cleaning based on date\n","print(\"\\nCleaning statements...\")\n","cleaned_texts = []\n","\n","for idx, row in df.iterrows():\n","    if idx % 20 == 0:\n","        print(f\"Processing statement {idx+1}/{len(df)}...\")\n","\n","    date = row['date']\n","    text = row['text']\n","\n","    # Apply appropriate cleaning function based on date\n","    if date <= pd.Timestamp('2005-12-13'):\n","        cleaned_text = clean_text_old(text, date)\n","    elif date <= pd.Timestamp('2020-03-05'):\n","        cleaned_text = clean_text_middle(text)\n","    else:\n","        cleaned_text = clean_text_recent(text)\n","\n","    cleaned_texts.append(cleaned_text)\n","\n","# Create new dataframe with cleaned text\n","df_cleaned = df.copy()\n","df_cleaned['text'] = cleaned_texts\n","\n","# Convert date back to string format to match original\n","df_cleaned['date'] = df_cleaned['date'].dt.strftime('%Y-%m-%d')\n","\n","# Save cleaned statements\n","output_file = os.path.join(output_dir, 'fomc_statements_cleaned.csv')\n","df_cleaned.to_csv(output_file, index=False)\n","\n","print(f\"\\nCleaned statements saved to: {output_file}\")\n","print(f\"Total statements: {len(df_cleaned)}\")\n","\n","# Display some statistics about the cleaning\n","print(\"\\n\" + \"=\"*70)\n","print(\"CLEANING STATISTICS\")\n","print(\"=\"*70)\n","\n","# Calculate average text length before and after\n","original_lengths = df['text'].str.len()\n","cleaned_lengths = df_cleaned['text'].str.len()\n","\n","print(f\"\\nAverage original text length: {original_lengths.mean():.0f} characters\")\n","print(f\"Average cleaned text length: {cleaned_lengths.mean():.0f} characters\")\n","print(f\"Average reduction: {(original_lengths.mean() - cleaned_lengths.mean()):.0f} characters ({((1 - cleaned_lengths.mean()/original_lengths.mean())*100):.1f}%)\")\n","\n","# Show examples from each period\n","print(\"\\n\" + \"=\"*70)\n","print(\"EXAMPLES OF CLEANED TEXT\")\n","print(\"=\"*70)\n","\n","# Example from old period (2005 and prior)\n","old_example = df_cleaned[df_cleaned['date'] <= '2005-12-13'].iloc[-1]\n","print(f\"\\nOLD PERIOD (2005 and prior)\")\n","print(f\"Date: {old_example['date']}\")\n","print(f\"First 200 characters: {old_example['text'][:200]}...\")\n","\n","# Example from middle period (2006-2020-03-05)\n","middle_example = df_cleaned[(df_cleaned['date'] > '2005-12-13') & (df_cleaned['date'] <= '2020-03-05')].iloc[0]\n","print(f\"\\nMIDDLE PERIOD (2006 to March 2020)\")\n","print(f\"Date: {middle_example['date']}\")\n","print(f\"First 200 characters: {middle_example['text'][:200]}...\")\n","\n","# Example from recent period (after 2020-03-05)\n","recent_example = df_cleaned[df_cleaned['date'] > '2020-03-05'].iloc[0]\n","print(f\"\\nRECENT PERIOD (after March 2020)\")\n","print(f\"Date: {recent_example['date']}\")\n","print(f\"First 200 characters: {recent_example['text'][:200]}...\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Cleaning complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEHXo600lmqs","executionInfo":{"status":"ok","timestamp":1759440220310,"user_tz":300,"elapsed":52,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"65e1a8c1-92dc-4362-d7b1-ca0327dc91ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading FOMC statements...\n","Loaded 199 statements\n","\n","Deleting statement_20081216...\n","Statements remaining: 198\n","\n","Cleaning statements...\n","Processing statement 1/198...\n","Processing statement 21/198...\n","Processing statement 41/198...\n","Processing statement 61/198...\n","Processing statement 81/198...\n","Processing statement 101/198...\n","Processing statement 121/198...\n","Processing statement 141/198...\n","Processing statement 161/198...\n","Processing statement 181/198...\n","\n","Cleaned statements saved to: /content/drive/MyDrive/Statements/fomc_statements_cleaned.csv\n","Total statements: 198\n","\n","======================================================================\n","CLEANING STATISTICS\n","======================================================================\n","\n","Average original text length: 4247 characters\n","Average cleaned text length: 2621 characters\n","Average reduction: 1627 characters (38.3%)\n","\n","======================================================================\n","EXAMPLES OF CLEANED TEXT\n","======================================================================\n","\n","OLD PERIOD (2005 and prior)\n","Date: 2005-12-13\n","First 200 characters: The Federal Open Market Committee decided today to raise its target for the federal funds rate by 25 basis points to 4-1/4 percent.\n","Despite elevated energy prices and hurricane-related disruptions, th...\n","\n","MIDDLE PERIOD (2006 to March 2020)\n","Date: 2006-01-31\n","First 200 characters: The Federal Open Market Committee decided today to raise its target for the federal funds rate by 25 basis points to 4-1/2 percent.\n","Although recent economic data have been uneven, the expansion in eco...\n","\n","RECENT PERIOD (after March 2020)\n","Date: 2020-03-15\n","First 200 characters: The coronavirus outbreak has harmed communities and disrupted economic activity in many countries, including the United States. Global financial conditions have also been significantly affected. Avail...\n","\n","======================================================================\n","Cleaning complete!\n"]}]}]}