{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPt3oChS+K4KQeUwvn/2lHd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFaPFzjam4rx","executionInfo":{"status":"ok","timestamp":1760024156903,"user_tz":300,"elapsed":95225,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"d1bec1e6-1ad4-45d3-9df3-cc5f1212587f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","======================================================================\n","LOADING DICTIONARIES\n","======================================================================\n","Dictionaries loaded successfully!\n","Labor indicators: ['General Labor', 'Employment', 'Unemployment', 'Participation', 'Wages', 'Vacancies', 'Quits', 'Layoffs', 'Hiring']\n","Inflation categories: ['General Inflation', 'Core Measures', 'Headline Measures', 'Sectoral Measures', 'Producer Price Index', 'Wage Inflation', 'Inflation Expectations', 'Commodity Prices']\n","\n","======================================================================\n","STARTING UNIFIED CLASSIFICATION\n","======================================================================\n","\n","Skipping transcripts (disabled in config)\n","\n","Skipping statements (disabled in config)\n","\n","======================================================================\n","PROCESSING: MINUTES\n","======================================================================\n","Reading from: /content/drive/MyDrive/FedComs/Minutes/fomc_minutes_cleaned.csv\n","Loaded 199 records\n","Processing record 1/199\n","Processing record 11/199\n","Processing record 21/199\n","Processing record 31/199\n","Processing record 41/199\n","Processing record 51/199\n","Processing record 61/199\n","Processing record 71/199\n","Processing record 81/199\n","Processing record 91/199\n","Processing record 101/199\n","Processing record 111/199\n","Processing record 121/199\n","Processing record 131/199\n","Processing record 141/199\n","Processing record 151/199\n","Processing record 161/199\n","Processing record 171/199\n","Processing record 181/199\n","Processing record 191/199\n","\n","✓ Summary saved to: /content/drive/MyDrive/FedComs/Minutes/minutes_content.csv\n","  Shape: (199, 77)\n","\n","✓ Total sentences: 43678\n","  Classification distribution:\n","classification\n","Neither      30448\n","Inflation     5905\n","Labor         5568\n","Both          1757\n","\n","✓ Validation set saved to: /content/drive/MyDrive/FedComs/Validation_Sets/minutes_validate.csv\n","  Size: 45\n","\n","Skipping speeches (disabled in config)\n","\n","Skipping press_conferences (disabled in config)\n","\n","======================================================================\n","ALL PROCESSING COMPLETE!\n","======================================================================\n"]}],"source":["\"\"\"\n","Unified Content Classification Script\n","Classifies labor and inflation content across all Fed communication sources\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import random\n","import time\n","from glob import glob\n","\n","# Set random seed for reproducibility\n","seed = int(time.time())\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","# ============================================================================\n","# CONFIGURATION - SET WHICH SOURCES TO PROCESS\n","# ============================================================================\n","\n","SOURCES_TO_PROCESS = {\n","    'transcripts': False,    # Set to False to skip\n","    'statements': False,\n","    'minutes': True,\n","    'speeches': False,\n","    'press_conferences': False\n","}\n","\n","# ============================================================================\n","# PATHS CONFIGURATION\n","# ============================================================================\n","\n","BASE_DIR = '/content/drive/MyDrive/FedComs'\n","DICT_DIR = f'{BASE_DIR}/Dictionaries'\n","VALIDATION_DIR = f'{BASE_DIR}/Validation_Sets'\n","\n","SOURCE_CONFIGS = {\n","    'transcripts': {\n","        'input_dir': f'{BASE_DIR}/Transcripts/final_transcripts',\n","        'output_dir': f'{BASE_DIR}/Transcripts',\n","        'output_file': 'transcripts_content.csv',\n","        'validation_file': 'transcripts_validate.csv',\n","        'text_column': 'Text',\n","        'id_column': 'id',\n","        'date_column': 'Date',\n","        'name_column': 'Name',\n","        'role_column': 'Role',\n","        'process_type': 'multiple_files',  # Multiple CSV files\n","        'group_by': 'row'  # Each row is a separate item\n","    },\n","    'statements': {\n","        'input_file': f'{BASE_DIR}/Statements/fomc_statements_cleaned.csv',\n","        'output_dir': f'{BASE_DIR}/Statements',\n","        'output_file': 'statement_content.csv',\n","        'validation_file': 'statement_validate.csv',\n","        'text_column': 'text',\n","        'id_column': 'id',\n","        'date_column': 'date',\n","        'process_type': 'single_file',\n","        'group_by': 'row'\n","    },\n","    'minutes': {\n","        'input_file': f'{BASE_DIR}/Minutes/fomc_minutes_cleaned.csv',\n","        'output_dir': f'{BASE_DIR}/Minutes',\n","        'output_file': 'minutes_content.csv',\n","        'validation_file': 'minutes_validate.csv',\n","        'text_column': 'text',\n","        'id_column': 'id',\n","        'date_column': 'date',\n","        'process_type': 'single_file',\n","        'group_by': 'row'\n","    },\n","    'speeches': {\n","        'input_dir': f'{BASE_DIR}/Speeches/fed_speeches_clean',\n","        'output_dir': f'{BASE_DIR}/Speeches',\n","        'output_file': 'speeches_content.csv',\n","        'validation_file': 'speeches_validate.csv',\n","        'text_column': 'text',  # Will search for columns containing 'text'\n","        'date_column': 'date',\n","        'name_column': 'official_name',  # Will search multiple options\n","        'process_type': 'multiple_files',\n","        'group_by': 'row'\n","    },\n","    'press_conferences': {\n","        'input_file': f'{BASE_DIR}/PressConf/fomc_press_conferences.csv',\n","        'output_dir': f'{BASE_DIR}/PressConf',\n","        'output_file': 'press_conferences_content.csv',\n","        'validation_file': 'press_conferences_validate.csv',\n","        'text_column': 'text',\n","        'id_column': 'id',\n","        'date_column': 'date',\n","        'speaker_column': 'speaker',\n","        'process_type': 'single_file',\n","        'group_by': 'row'\n","    }\n","}\n","\n","# ============================================================================\n","# LOAD DICTIONARIES\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"LOADING DICTIONARIES\")\n","print(\"=\"*70)\n","\n","with open(os.path.join(DICT_DIR, 'labor_indicators.json'), 'r') as f:\n","    LABOR_INDICATORS = json.load(f)\n","\n","with open(os.path.join(DICT_DIR, 'inflation_indicators.json'), 'r') as f:\n","    INFLATION_INDICATORS = json.load(f)\n","\n","with open(os.path.join(DICT_DIR, 'inflation_pattern_mapping.json'), 'r') as f:\n","    INFLATION_PATTERN_TO_INDICATOR = json.load(f)\n","\n","print(\"Dictionaries loaded successfully!\")\n","print(f\"Labor indicators: {list(LABOR_INDICATORS.keys())}\")\n","print(f\"Inflation categories: {list(INFLATION_INDICATORS.keys())}\")\n","\n","# ============================================================================\n","# TEXT PROCESSING FUNCTIONS\n","# ============================================================================\n","\n","def fix_text_encoding(text):\n","    \"\"\"Fix common text encoding issues.\"\"\"\n","    text = text.replace('Ã¢â‚¬\"', '–')\n","    text = text.replace('Ã¢â‚¬\"', '—')\n","    text = text.replace('Ã¢â‚¬Å\"', '\"')\n","    text = text.replace('Ã¢â‚¬', '\"')\n","    text = text.replace('\\u2013', '–')\n","    text = text.replace('\\u2014', '—')\n","    text = text.replace('\\u2018', \"'\")\n","    text = text.replace('\\u2019', \"'\")\n","    text = text.replace('\\u201c', '\"')\n","    text = text.replace('\\u201d', '\"')\n","    text = text.replace('\\u2026', '...')\n","    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n","    return text\n","\n","def split_into_sentences(text):\n","    \"\"\"Split text into sentences, preserving initials and abbreviations.\"\"\"\n","    text = fix_text_encoding(text)\n","\n","    abbreviations = [\n","        r'\\bU\\.S\\.A\\.', r'\\bU\\.S\\.', r'\\bU\\.K\\.', r'\\bE\\.U\\.',\n","        r'\\bSt\\.', r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.',\n","        r'\\bProf\\.', r'\\bSr\\.', r'\\bJr\\.', r'\\bvs\\.', r'\\betc\\.',\n","        r'\\bi\\.e\\.', r'\\be\\.g\\.', r'\\bVol\\.', r'\\bNo\\.', r'\\bpp\\.',\n","        r'\\bCo\\.', r'\\bInc\\.', r'\\bLtd\\.', r'\\bCorp\\.',\n","        r'\\bPh\\.D\\.', r'\\bM\\.A\\.', r'\\bM\\.S\\.', r'\\bB\\.A\\.',\n","        r'\\bD\\.C\\.', r'\\bA\\.M\\.', r'\\bP\\.M\\.'\n","    ]\n","\n","    for idx, abbr in enumerate(abbreviations):\n","        text = re.sub(abbr, f'<ABBR_{idx}>', text, flags=re.IGNORECASE)\n","\n","    text = re.sub(r'\\b([A-Z])\\.([\\s+[A-Z]\\.)*(?=\\s+[A-Z][a-z]+)', lambda m: m.group(0).replace('.', '<NAME>'), text)\n","    text = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: m.group(0).replace('.', '<DEC>'), text)\n","\n","    voting_pattern = r'((?:Voting for|Voting against)\\s+[^.!?]+?)([.!?]+\\s+|$)'\n","    voting_matches = []\n","    def store_voting_match(match):\n","        voting_matches.append(match.group(1))\n","        return f'<VOTE_{len(voting_matches) - 1}>'\n","    text = re.sub(voting_pattern, store_voting_match, text)\n","\n","    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z]|$)', text)\n","    sentences = [s.strip() for s in sentences if s.strip()]\n","\n","    restored_sentences = []\n","    for sentence in sentences:\n","        for idx in range(len(abbreviations)):\n","            sentence = sentence.replace(f'<ABBR_{idx}>', abbreviations[idx].replace(r'\\b', '').replace(r'\\.', '.'))\n","        sentence = sentence.replace('<NAME>', '.')\n","        sentence = sentence.replace('<DEC>', '.')\n","        for i, voting_list in enumerate(voting_matches):\n","            placeholder = f'<VOTE_{i}>'\n","            if placeholder in sentence:\n","                sentence = sentence.replace(placeholder, voting_list)\n","        restored_sentences.append(sentence)\n","\n","    return restored_sentences\n","\n","# ============================================================================\n","# CLASSIFICATION FUNCTIONS\n","# ============================================================================\n","\n","def check_keywords_in_sentence(sentence, keywords):\n","    \"\"\"Check if any keyword appears in the sentence.\"\"\"\n","    sentence_lower = sentence.lower()\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_employment_indicator(sentence, keywords):\n","    \"\"\"Check for Employment indicator, excluding maximum/full employment.\"\"\"\n","    sentence_lower = sentence.lower()\n","\n","    if re.search(r'\\b(?:maximum|full)\\s+employment\\b', sentence_lower):\n","        return False\n","    if re.search(r'\\bemployment\\s+goal\\b', sentence_lower):\n","        return False\n","\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_labor_term(sentence):\n","    \"\"\"Check if sentence contains general labor terms.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_labor_keywords = LABOR_INDICATORS.get(\"General Labor\", [])\n","    for keyword in general_labor_keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_inflation_terms(sentence):\n","    \"\"\"Check if sentence contains general inflation terms.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_inflation_patterns = INFLATION_INDICATORS.get(\"General Inflation\", {}).get(\"general_patterns\", [])\n","    for pattern in general_inflation_patterns:\n","        if re.search(pattern, sentence_lower, re.IGNORECASE):\n","            return True\n","    return False\n","\n","def check_inflation_sentence(sentence):\n","    \"\"\"Check if sentence mentions any inflation indicator.\"\"\"\n","    mentioned_indicators = set()\n","    sentence_lower = sentence.lower()\n","\n","    for category, subcategories in INFLATION_INDICATORS.items():\n","        for pattern_name, pattern_list in subcategories.items():\n","            for pattern in pattern_list:\n","                if re.search(pattern, sentence_lower, re.IGNORECASE):\n","                    indicator_name = INFLATION_PATTERN_TO_INDICATOR.get(pattern_name, \"Other\")\n","                    mentioned_indicators.add(indicator_name)\n","                    break\n","\n","    # Remove generic indicators when specific ones are present\n","    if \"Core_CPI\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","    if \"Core_PCE\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","    if \"Headline_CPI\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","    if \"Headline_PCE\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    return mentioned_indicators\n","\n","def classify_sentence(sentence):\n","    \"\"\"Classify a single sentence and return its indicators.\"\"\"\n","    labor_specific_found = False\n","    labor_indicators_in_sentence = set()\n","\n","    for indicator, keywords in LABOR_INDICATORS.items():\n","        if indicator == \"General Labor\":\n","            continue\n","\n","        if indicator == \"Employment\":\n","            if check_employment_indicator(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","        else:\n","            if check_keywords_in_sentence(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","\n","    labor_general_found = check_general_labor_term(sentence)\n","    labor_found = labor_specific_found or labor_general_found\n","\n","    inflation_indicators_in_sentence = check_inflation_sentence(sentence)\n","    inflation_specific_found = bool(inflation_indicators_in_sentence)\n","\n","    inflation_general_found = check_general_inflation_terms(sentence)\n","    inflation_found = inflation_specific_found or inflation_general_found\n","\n","    if labor_found and inflation_found:\n","        classification = \"Both\"\n","    elif labor_found:\n","        classification = \"Labor\"\n","    elif inflation_found:\n","        classification = \"Inflation\"\n","    else:\n","        classification = \"Neither\"\n","\n","    return {\n","        'classification': classification,\n","        'labor_indicators': list(labor_indicators_in_sentence),\n","        'inflation_indicators': list(inflation_indicators_in_sentence)\n","    }\n","\n","def analyze_text(text):\n","    \"\"\"Analyze a single text for labor and inflation content.\"\"\"\n","    sentences = split_into_sentences(text)\n","    total_sentences = len(sentences)\n","\n","    labor_sentences = 0\n","    inflation_sentences = 0\n","    both_sentences = 0\n","\n","    labor_indicator_counts = {indicator: 0 for indicator in LABOR_INDICATORS.keys() if indicator != \"General Labor\"}\n","    inflation_indicator_list = sorted(list(set(\n","        indicator for indicator in INFLATION_PATTERN_TO_INDICATOR.values()\n","        if indicator not in [\"General_Inflation\", \"Other\"]\n","    )))\n","    inflation_indicator_counts = {indicator: 0 for indicator in inflation_indicator_list}\n","\n","    sentence_data_list = []\n","\n","    for sent_idx, sentence in enumerate(sentences):\n","        classification_result = classify_sentence(sentence)\n","\n","        labor_indicators_filtered = [ind for ind in classification_result['labor_indicators']\n","                                      if ind != \"General Labor\"]\n","        inflation_indicators_filtered = [ind for ind in classification_result['inflation_indicators']\n","                                          if ind not in [\"General_Inflation\", \"Other\"]]\n","\n","        sentence_data = {\n","            'sentence_number': sent_idx + 1,\n","            'sentence_text': sentence,\n","            'classification': classification_result['classification'],\n","            'labor_indicators': ', '.join(sorted(labor_indicators_filtered)) if labor_indicators_filtered else '',\n","            'inflation_indicators': ', '.join(sorted(inflation_indicators_filtered)) if inflation_indicators_filtered else ''\n","        }\n","        sentence_data_list.append(sentence_data)\n","\n","        labor_specific_found = bool(classification_result['labor_indicators'])\n","        labor_general_found = check_general_labor_term(sentence)\n","        labor_found = labor_specific_found or labor_general_found\n","\n","        inflation_specific_found = bool(classification_result['inflation_indicators'])\n","        inflation_general_found = check_general_inflation_terms(sentence)\n","        inflation_found = inflation_specific_found or inflation_general_found\n","\n","        if labor_found and inflation_found:\n","            both_sentences += 1\n","            labor_sentences += 1\n","            inflation_sentences += 1\n","        elif labor_found:\n","            labor_sentences += 1\n","        elif inflation_found:\n","            inflation_sentences += 1\n","\n","        for indicator in classification_result['labor_indicators']:\n","            if indicator in labor_indicator_counts:\n","                labor_indicator_counts[indicator] += 1\n","\n","        for indicator in classification_result['inflation_indicators']:\n","            if indicator in inflation_indicator_counts:\n","                inflation_indicator_counts[indicator] += 1\n","\n","    total_labor_mentions = sum(labor_indicator_counts.values())\n","    total_inflation_mentions = sum(inflation_indicator_counts.values())\n","\n","    labor_emphasis = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_emphasis[f\"labor_emphasis_{indicator}\"] = count / total_labor_mentions if total_labor_mentions > 0 else 0\n","\n","    inflation_emphasis = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_emphasis[f\"inflation_emphasis_{indicator}\"] = count / total_inflation_mentions if total_inflation_mentions > 0 else 0\n","\n","    labor_sentence_share = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_sentence_share[f\"labor_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    inflation_sentence_share = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_sentence_share[f\"inflation_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    labor_inflation_total = labor_sentences + inflation_sentences - both_sentences\n","    labor_share_of_labor_inflation = labor_sentences / labor_inflation_total if labor_inflation_total > 0 else 0\n","\n","    summary_results = {\n","        'sentences_on_labor': labor_sentences,\n","        'sentences_on_inflation': inflation_sentences,\n","        'sentences_on_both': both_sentences,\n","        'total_sentences': total_sentences,\n","        'labor_share_of_labor_inflation_sentences': labor_share_of_labor_inflation\n","    }\n","\n","    for indicator, count in labor_indicator_counts.items():\n","        summary_results[f'labor_{indicator}_count'] = count\n","\n","    for indicator, count in inflation_indicator_counts.items():\n","        summary_results[f'inflation_{indicator}_count'] = count\n","\n","    summary_results.update(labor_emphasis)\n","    summary_results.update(inflation_emphasis)\n","    summary_results.update(labor_sentence_share)\n","    summary_results.update(inflation_sentence_share)\n","\n","    return summary_results, sentence_data_list\n","\n","# ============================================================================\n","# HELPER FUNCTIONS FOR DATA PROCESSING\n","# ============================================================================\n","\n","def find_column(df, possible_names):\n","    \"\"\"Find a column by checking multiple possible names.\"\"\"\n","    for name in possible_names:\n","        if name in df.columns:\n","            return name\n","        # Case-insensitive search\n","        for col in df.columns:\n","            if col.lower() == name.lower():\n","                return col\n","    return None\n","\n","def get_text_column(df, config):\n","    \"\"\"Get the text column from dataframe.\"\"\"\n","    text_col = config.get('text_column')\n","\n","    # Direct match\n","    if text_col and text_col in df.columns:\n","        return text_col\n","\n","    # Search for columns containing 'text'\n","    for col in df.columns:\n","        if 'text' in col.lower():\n","            return col\n","\n","    return None\n","\n","def create_validation_set(sentences_df, source_name):\n","    \"\"\"Create validation set from sentence dataframe.\"\"\"\n","    n_labor = 15\n","    n_inflation = 15\n","    n_both = 5\n","    n_neither = 10\n","\n","    validation_samples = []\n","\n","    labor_sentences = sentences_df[sentences_df['classification'] == 'Labor']\n","    if len(labor_sentences) >= n_labor:\n","        validation_samples.append(labor_sentences.sample(n=n_labor, random_state=seed))\n","    elif len(labor_sentences) > 0:\n","        validation_samples.append(labor_sentences)\n","\n","    inflation_sentences = sentences_df[sentences_df['classification'] == 'Inflation']\n","    if len(inflation_sentences) >= n_inflation:\n","        validation_samples.append(inflation_sentences.sample(n=n_inflation, random_state=seed))\n","    elif len(inflation_sentences) > 0:\n","        validation_samples.append(inflation_sentences)\n","\n","    both_sentences = sentences_df[sentences_df['classification'] == 'Both']\n","    if len(both_sentences) >= n_both:\n","        validation_samples.append(both_sentences.sample(n=n_both, random_state=seed))\n","    elif len(both_sentences) > 0:\n","        validation_samples.append(both_sentences)\n","\n","    neither_sentences = sentences_df[sentences_df['classification'] == 'Neither']\n","    if len(neither_sentences) >= n_neither:\n","        validation_samples.append(neither_sentences.sample(n=n_neither, random_state=seed))\n","    elif len(neither_sentences) > 0:\n","        validation_samples.append(neither_sentences)\n","\n","    if validation_samples:\n","        validation_df = pd.concat(validation_samples, ignore_index=True)\n","        validation_df = validation_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","        return validation_df\n","\n","    return None\n","\n","# ============================================================================\n","# MAIN PROCESSING FUNCTION\n","# ============================================================================\n","\n","def process_source(source_name, config):\n","    \"\"\"Process a single source.\"\"\"\n","    print(\"\\n\" + \"=\"*70)\n","    print(f\"PROCESSING: {source_name.upper()}\")\n","    print(\"=\"*70)\n","\n","    results_list = []\n","    all_sentences = []\n","\n","    # Handle multiple files vs single file\n","    if config['process_type'] == 'multiple_files':\n","        input_dir = config['input_dir']\n","        csv_files = glob(os.path.join(input_dir, '*.csv'))\n","        print(f\"Found {len(csv_files)} files in {input_dir}\")\n","\n","        for idx, csv_file in enumerate(csv_files):\n","            filename = os.path.basename(csv_file)\n","            if idx % 5 == 0:\n","                print(f\"Processing file {idx+1}/{len(csv_files)}: {filename}\")\n","\n","            try:\n","                df = pd.read_csv(csv_file, encoding='utf-8', encoding_errors='replace')\n","                text_col = get_text_column(df, config)\n","\n","                if text_col is None:\n","                    print(f\"  Warning: No text column found in {filename}\")\n","                    continue\n","\n","                for row_idx, row in df.iterrows():\n","                    text = str(row[text_col]) if pd.notna(row[text_col]) else ''\n","                    if len(text.strip()) == 0:\n","                        continue\n","\n","                    summary_results, sentence_data_list = analyze_text(text)\n","\n","                    # Add metadata\n","                    for col in df.columns:\n","                        if col != text_col:\n","                            summary_results[col] = str(row[col]) if pd.notna(row[col]) else ''\n","\n","                    results_list.append(summary_results)\n","\n","                    # Add sentence-level data\n","                    for sentence_data in sentence_data_list:\n","                        for col in df.columns:\n","                            if col != text_col:\n","                                sentence_data[col] = str(row[col]) if pd.notna(row[col]) else ''\n","                        all_sentences.append(sentence_data)\n","\n","            except Exception as e:\n","                print(f\"  Error processing {filename}: {e}\")\n","                continue\n","\n","    else:  # single_file\n","        input_file = config['input_file']\n","        print(f\"Reading from: {input_file}\")\n","\n","        try:\n","            df = pd.read_csv(input_file, encoding='utf-8', encoding_errors='replace')\n","            print(f\"Loaded {len(df)} records\")\n","\n","            text_col = get_text_column(df, config)\n","            if text_col is None:\n","                print(f\"ERROR: No text column found\")\n","                return\n","\n","            for idx, row in df.iterrows():\n","                if idx % 10 == 0:\n","                    print(f\"Processing record {idx+1}/{len(df)}\")\n","\n","                text = str(row[text_col]) if pd.notna(row[text_col]) else ''\n","                if len(text.strip()) == 0:\n","                    continue\n","\n","                summary_results, sentence_data_list = analyze_text(text)\n","\n","                # Add metadata\n","                for col in df.columns:\n","                    if col != text_col:\n","                        summary_results[col] = str(row[col]) if pd.notna(row[col]) else ''\n","\n","                results_list.append(summary_results)\n","\n","                # Add sentence-level data\n","                for sentence_data in sentence_data_list:\n","                    for col in df.columns:\n","                        if col != text_col:\n","                            sentence_data[col] = str(row[col]) if pd.notna(row[col]) else ''\n","                    all_sentences.append(sentence_data)\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {e}\")\n","            return\n","\n","    # Save results\n","    results_df = pd.DataFrame(results_list)\n","\n","    if len(results_df) > 0:\n","        # Try to sort by date\n","        date_col = config.get('date_column')\n","        if date_col and date_col in results_df.columns:\n","            try:\n","                results_df = results_df.sort_values(date_col)\n","            except:\n","                pass\n","\n","        output_file = os.path.join(config['output_dir'], config['output_file'])\n","        results_df.to_csv(output_file, index=False)\n","        print(f\"\\n✓ Summary saved to: {output_file}\")\n","        print(f\"  Shape: {results_df.shape}\")\n","\n","    # Create validation set\n","    sentences_df = pd.DataFrame(all_sentences)\n","\n","    if len(sentences_df) > 0:\n","        print(f\"\\n✓ Total sentences: {len(sentences_df)}\")\n","        print(f\"  Classification distribution:\")\n","        print(sentences_df['classification'].value_counts().to_string())\n","\n","        validation_df = create_validation_set(sentences_df, source_name)\n","\n","        if validation_df is not None:\n","            validation_file = os.path.join(VALIDATION_DIR, config['validation_file'])\n","            validation_df.to_csv(validation_file, index=False)\n","            print(f\"\\n✓ Validation set saved to: {validation_file}\")\n","            print(f\"  Size: {len(validation_df)}\")\n","\n","# ============================================================================\n","# RUN PROCESSING\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STARTING UNIFIED CLASSIFICATION\")\n","print(\"=\"*70)\n","\n","for source_name, should_process in SOURCES_TO_PROCESS.items():\n","    if should_process:\n","        config = SOURCE_CONFIGS[source_name]\n","        process_source(source_name, config)\n","    else:\n","        print(f\"\\nSkipping {source_name} (disabled in config)\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ALL PROCESSING COMPLETE!\")\n","print(\"=\"*70)"]}]}