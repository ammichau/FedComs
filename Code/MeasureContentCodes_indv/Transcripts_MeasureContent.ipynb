{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnS0JfTsu/pmPzODK2xj0F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import random\n","import time\n","from glob import glob\n","\n","# Set random seed for reproducibility\n","seed = int(time.time())\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","# Directory paths\n","dict_dir = '/content/drive/MyDrive/FedComs/Dictionaries'\n","input_dir = '/content/drive/MyDrive/FedComs/Transcripts/cleaned_transcripts'\n","cleaned_output_dir = '/content/drive/MyDrive/FedComs/Transcripts/final_transcripts'\n","validation_output_dir = '/content/drive/MyDrive/FedComs/Validation_Sets'\n","summary_output_dir = '/content/drive/MyDrive/FedComs/Transcripts'\n","\n","# Create output directories if they don't exist\n","for directory in [cleaned_output_dir, validation_output_dir, summary_output_dir]:\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","os.chdir(summary_output_dir)\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# ============================================================================\n","# STEP 1: CLEAN TRANSCRIPTS\n","# ============================================================================\n","\n","def clean_page_numbers(text):\n","    \"\"\"Remove page number patterns from transcript text.\"\"\"\n","    # Pattern 1: \"April 26–27, 2011 52 of 244\" or \"April 26-27, 2011 52 of 244\"\n","    # Handles both en-dash (–), em-dash (—), and regular dash (-)\n","    text = re.sub(\n","        r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+'\n","        r'\\d{1,2}(?:[-–—]\\d{1,2})?,\\s+\\d{4}\\s+\\d+\\s+of\\s+\\d+',\n","        '',\n","        text\n","    )\n","\n","    # Pattern 2: Date ranges that may be incomplete/cut off\n","    # \"April 30–May 1\" or \"April 30–\" or similar with various dash types\n","    text = re.sub(\n","        r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+'\n","        r'\\d{1,2}[-–—]+'\n","        r'(?:(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2})?',\n","        '',\n","        text\n","    )\n","\n","    # Pattern 3: Standalone page numbers like \"52 of 244\" that might be on their own line\n","    text = re.sub(r'\\n\\s*\\d+\\s+of\\s+\\d+\\s*\\n', '\\n', text)\n","\n","    # Pattern 4: Date formats with forward slashes: \"6/26-27/01 87\"\n","    text = re.sub(\n","        r'\\d{1,2}/\\d{1,2}(?:[-–—]\\d{1,2})?/\\d{2,4}\\s+\\d+',\n","        '',\n","        text\n","    )\n","\n","    return text\n","\n","def fix_text_encoding(text):\n","    \"\"\"Fix common text encoding issues.\"\"\"\n","    text = text.replace('â€\"', '—')\n","    text = text.replace('â€\"', '—')\n","    text = text.replace('â€œ', '\"')\n","    text = text.replace('â€', '\"')\n","    text = text.replace('\\u2013', '–')\n","    text = text.replace('\\u2014', '—')\n","    text = text.replace('\\u2018', \"'\")\n","    text = text.replace('\\u2019', \"'\")\n","    text = text.replace('\\u201c', '\"')\n","    text = text.replace('\\u201d', '\"')\n","    text = text.replace('\\u2026', '...')\n","    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n","    return text\n","\n","def clean_transcript_text(text):\n","    \"\"\"Clean transcript text by fixing encoding and removing page numbers.\"\"\"\n","    text = fix_text_encoding(text)\n","    text = clean_page_numbers(text)\n","\n","    # Clean up excessive whitespace\n","    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines to double\n","    text = re.sub(r' +', ' ', text)  # Multiple spaces to single\n","\n","    return text.strip()\n","\n","print(\"\\nCleaning transcript files...\")\n","print(f\"Reading from: {input_dir}\")\n","\n","# Get all CSV files in the input directory\n","csv_files = glob(os.path.join(input_dir, '*.csv'))\n","print(f\"Found {len(csv_files)} transcript files\")\n","\n","if len(csv_files) == 0:\n","    print(\"ERROR: No CSV files found in input directory!\")\n","    print(f\"Please check that files exist in: {input_dir}\")\n","else:\n","    # Process each file\n","    cleaned_count = 0\n","    for csv_file in csv_files:\n","        filename = os.path.basename(csv_file)\n","        print(f\"Processing {filename}...\")\n","\n","        try:\n","            # Read the transcript file\n","            df = pd.read_csv(csv_file)\n","\n","            if 'Text' not in df.columns:\n","                print(f\"  Warning: No 'Text' column found in {filename}, skipping...\")\n","                continue\n","\n","            # Clean the text\n","            df['Text'] = df['Text'].apply(lambda x: clean_transcript_text(str(x)) if pd.notna(x) else '')\n","\n","            # Save cleaned version\n","            output_file = os.path.join(cleaned_output_dir, filename.replace('cleaned_', ''))\n","            df.to_csv(output_file, index=False)\n","            cleaned_count += 1\n","\n","        except Exception as e:\n","            print(f\"  Error processing {filename}: {e}\")\n","            continue\n","\n","    print(f\"\\nCleaned {cleaned_count} transcript files\")\n","    print(f\"Cleaned files saved to: {cleaned_output_dir}\")\n","\n","# ============================================================================\n","# STEP 2: CLASSIFY CONTENT\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"LOADING DICTIONARIES\")\n","print(\"=\"*70)\n","\n","with open(os.path.join(dict_dir, 'labor_indicators.json'), 'r') as f:\n","    LABOR_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_indicators.json'), 'r') as f:\n","    INFLATION_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_pattern_mapping.json'), 'r') as f:\n","    INFLATION_PATTERN_TO_INDICATOR = json.load(f)\n","\n","print(\"Dictionaries loaded successfully!\")\n","print(f\"Labor indicators: {list(LABOR_INDICATORS.keys())}\")\n","print(f\"Inflation categories: {list(INFLATION_INDICATORS.keys())}\")\n","\n","def split_into_sentences(text):\n","    \"\"\"Split text into sentences, preserving initials and abbreviations.\"\"\"\n","    text = fix_text_encoding(text)\n","\n","    abbreviations = [\n","        r'\\bU\\.S\\.A\\.', r'\\bU\\.S\\.', r'\\bU\\.K\\.', r'\\bE\\.U\\.',\n","        r'\\bSt\\.', r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.',\n","        r'\\bProf\\.', r'\\bSr\\.', r'\\bJr\\.', r'\\bvs\\.', r'\\betc\\.',\n","        r'\\bi\\.e\\.', r'\\be\\.g\\.', r'\\bVol\\.', r'\\bNo\\.', r'\\bpp\\.',\n","        r'\\bCo\\.', r'\\bInc\\.', r'\\bLtd\\.', r'\\bCorp\\.',\n","        r'\\bPh\\.D\\.', r'\\bM\\.A\\.', r'\\bM\\.S\\.', r'\\bB\\.A\\.',\n","        r'\\bD\\.C\\.', r'\\bA\\.M\\.', r'\\bP\\.M\\.'\n","    ]\n","\n","    for idx, abbr in enumerate(abbreviations):\n","        text = re.sub(abbr, f'<ABBR_{idx}>', text, flags=re.IGNORECASE)\n","\n","    text = re.sub(r'\\b([A-Z])\\.(\\s+[A-Z]\\.)*(?=\\s+[A-Z][a-z]+)', lambda m: m.group(0).replace('.', f'<NAME>'), text)\n","    text = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: m.group(0).replace('.', '<DEC>'), text)\n","\n","    voting_pattern = r'((?:Voting for|Voting against)\\s+[^.!?]+?)([.!?]+\\s+|$)'\n","    voting_matches = []\n","    def store_voting_match(match):\n","        voting_matches.append(match.group(1))\n","        return f'<VOTE_{len(voting_matches) - 1}>'\n","    text = re.sub(voting_pattern, store_voting_match, text)\n","\n","    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z]|$)', text)\n","    sentences = [s.strip() for s in sentences if s.strip()]\n","\n","    restored_sentences = []\n","    for sentence in sentences:\n","        for idx in range(len(abbreviations)):\n","            sentence = sentence.replace(f'<ABBR_{idx}>', abbreviations[idx].replace(r'\\b', '').replace(r'\\.', '.'))\n","        sentence = sentence.replace('<NAME>', '.')\n","        sentence = sentence.replace('<DEC>', '.')\n","        for i, voting_list in enumerate(voting_matches):\n","            placeholder = f'<VOTE_{i}>'\n","            if placeholder in sentence:\n","                sentence = sentence.replace(placeholder, voting_list)\n","        restored_sentences.append(sentence)\n","\n","    return restored_sentences\n","\n","def check_keywords_in_sentence(sentence, keywords):\n","    \"\"\"Check if any keyword appears in the sentence.\"\"\"\n","    sentence_lower = sentence.lower()\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_employment_indicator(sentence, keywords):\n","    \"\"\"Check for Employment indicator, excluding maximum/full employment.\"\"\"\n","    sentence_lower = sentence.lower()\n","\n","    # Check if sentence contains maximum employment, full employment, or employment goal\n","    if re.search(r'\\b(?:maximum|full)\\s+employment\\b', sentence_lower):\n","        return False\n","    if re.search(r'\\bemployment\\s+goal\\b', sentence_lower):\n","        return False\n","\n","    # Otherwise check for employment keywords normally\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_labor_term(sentence):\n","    \"\"\"Check if sentence contains general labor terms (from General Labor category).\"\"\"\n","    sentence_lower = sentence.lower()\n","    # Use the General Labor keywords from the dictionary\n","    general_labor_keywords = LABOR_INDICATORS.get(\"General Labor\", [])\n","    for keyword in general_labor_keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_inflation_terms(sentence):\n","    \"\"\"Check if sentence contains general inflation terms (from General Inflation category).\"\"\"\n","    sentence_lower = sentence.lower()\n","    # Use the General Inflation patterns from the dictionary\n","    general_inflation_patterns = INFLATION_INDICATORS.get(\"General Inflation\", {}).get(\"general_patterns\", [])\n","    for pattern in general_inflation_patterns:\n","        if re.search(pattern, sentence_lower, re.IGNORECASE):\n","            return True\n","    return False\n","\n","def check_inflation_sentence(sentence):\n","    \"\"\"Check if sentence mentions any inflation indicator.\"\"\"\n","    mentioned_indicators = set()\n","    sentence_lower = sentence.lower()\n","\n","    for category, subcategories in INFLATION_INDICATORS.items():\n","        for pattern_name, pattern_list in subcategories.items():\n","            for pattern in pattern_list:\n","                if re.search(pattern, sentence_lower, re.IGNORECASE):\n","                    indicator_name = INFLATION_PATTERN_TO_INDICATOR.get(pattern_name, \"Other\")\n","                    mentioned_indicators.add(indicator_name)\n","                    break\n","\n","    # If sentence has both Core_CPI and Core, remove the generic Core\n","    if \"Core_CPI\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Core_PCE and Core, remove the generic Core\n","    if \"Core_PCE\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Headline_CPI and Headline, remove the generic Headline\n","    if \"Headline_CPI\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    # If sentence has both Headline_PCE and Headline, remove the generic Headline\n","    if \"Headline_PCE\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    return mentioned_indicators\n","\n","def classify_sentence(sentence):\n","    \"\"\"Classify a single sentence and return its indicators.\"\"\"\n","    labor_specific_found = False\n","    labor_indicators_in_sentence = set()\n","\n","    # Check all labor indicators EXCEPT \"General Labor\"\n","    for indicator, keywords in LABOR_INDICATORS.items():\n","        if indicator == \"General Labor\":\n","            continue  # Skip general labor for indicator counts\n","\n","        # Use special handling for Employment indicator\n","        if indicator == \"Employment\":\n","            if check_employment_indicator(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","        else:\n","            if check_keywords_in_sentence(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","\n","    labor_general_found = check_general_labor_term(sentence)\n","    labor_found = labor_specific_found or labor_general_found\n","\n","    inflation_indicators_in_sentence = check_inflation_sentence(sentence)\n","    inflation_specific_found = bool(inflation_indicators_in_sentence)\n","\n","    inflation_general_found = check_general_inflation_terms(sentence)\n","    inflation_found = inflation_specific_found or inflation_general_found\n","\n","    if labor_found and inflation_found:\n","        classification = \"Both\"\n","    elif labor_found:\n","        classification = \"Labor\"\n","    elif inflation_found:\n","        classification = \"Inflation\"\n","    else:\n","        classification = \"Neither\"\n","\n","    return {\n","        'classification': classification,\n","        'labor_indicators': list(labor_indicators_in_sentence),\n","        'inflation_indicators': list(inflation_indicators_in_sentence)\n","    }\n","\n","def analyze_transcript(text):\n","    \"\"\"Analyze a single transcript for labor and inflation content.\"\"\"\n","    sentences = split_into_sentences(text)\n","    total_sentences = len(sentences)\n","\n","    labor_sentences = 0\n","    inflation_sentences = 0\n","    both_sentences = 0\n","\n","    # Only create indicator counts for non-general categories\n","    labor_indicator_counts = {indicator: 0 for indicator in LABOR_INDICATORS.keys() if indicator != \"General Labor\"}\n","    inflation_indicator_list = sorted(list(set(\n","        indicator for indicator in INFLATION_PATTERN_TO_INDICATOR.values()\n","        if indicator not in [\"General_Inflation\", \"Other\"]\n","    )))\n","    inflation_indicator_counts = {indicator: 0 for indicator in inflation_indicator_list}\n","\n","    sentence_data_list = []\n","\n","    for sent_idx, sentence in enumerate(sentences):\n","        classification_result = classify_sentence(sentence)\n","\n","        # Filter out general categories from the indicator lists\n","        labor_indicators_filtered = [ind for ind in classification_result['labor_indicators']\n","                                      if ind != \"General Labor\"]\n","        inflation_indicators_filtered = [ind for ind in classification_result['inflation_indicators']\n","                                          if ind not in [\"General_Inflation\", \"Other\"]]\n","\n","        sentence_data = {\n","            'sentence_number': sent_idx + 1,\n","            'sentence_text': sentence,\n","            'classification': classification_result['classification'],\n","            'labor_indicators': ', '.join(sorted(labor_indicators_filtered)) if labor_indicators_filtered else '',\n","            'inflation_indicators': ', '.join(sorted(inflation_indicators_filtered)) if inflation_indicators_filtered else ''\n","        }\n","        sentence_data_list.append(sentence_data)\n","\n","        labor_specific_found = bool(classification_result['labor_indicators'])\n","        labor_general_found = check_general_labor_term(sentence)\n","        labor_found = labor_specific_found or labor_general_found\n","\n","        inflation_specific_found = bool(classification_result['inflation_indicators'])\n","        inflation_general_found = check_general_inflation_terms(sentence)\n","        inflation_found = inflation_specific_found or inflation_general_found\n","\n","        if labor_found and inflation_found:\n","            both_sentences += 1\n","            labor_sentences += 1\n","            inflation_sentences += 1\n","        elif labor_found:\n","            labor_sentences += 1\n","        elif inflation_found:\n","            inflation_sentences += 1\n","\n","        # Only count specific indicators (not general terms or \"Other\") for emphasis vectors\n","        for indicator in classification_result['labor_indicators']:\n","            if indicator in labor_indicator_counts:\n","                labor_indicator_counts[indicator] += 1\n","\n","        for indicator in classification_result['inflation_indicators']:\n","            if indicator in inflation_indicator_counts:\n","                inflation_indicator_counts[indicator] += 1\n","\n","    total_labor_mentions = sum(labor_indicator_counts.values())\n","    total_inflation_mentions = sum(inflation_indicator_counts.values())\n","\n","    labor_emphasis = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_emphasis[f\"labor_emphasis_{indicator}\"] = count / total_labor_mentions if total_labor_mentions > 0 else 0\n","\n","    inflation_emphasis = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_emphasis[f\"inflation_emphasis_{indicator}\"] = count / total_inflation_mentions if total_inflation_mentions > 0 else 0\n","\n","    labor_sentence_share = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_sentence_share[f\"labor_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    inflation_sentence_share = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_sentence_share[f\"inflation_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    labor_inflation_total = labor_sentences + inflation_sentences - both_sentences\n","    labor_share_of_labor_inflation = labor_sentences / labor_inflation_total if labor_inflation_total > 0 else 0\n","\n","    summary_results = {\n","        'sentences_on_labor': labor_sentences,\n","        'sentences_on_inflation': inflation_sentences,\n","        'sentences_on_both': both_sentences,\n","        'total_sentences': total_sentences,\n","        'labor_share_of_labor_inflation_sentences': labor_share_of_labor_inflation\n","    }\n","\n","    for indicator, count in labor_indicator_counts.items():\n","        summary_results[f'labor_{indicator}_count'] = count\n","\n","    for indicator, count in inflation_indicator_counts.items():\n","        summary_results[f'inflation_{indicator}_count'] = count\n","\n","    summary_results.update(labor_emphasis)\n","    summary_results.update(inflation_emphasis)\n","    summary_results.update(labor_sentence_share)\n","    summary_results.update(inflation_sentence_share)\n","\n","    return summary_results, sentence_data_list\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"CLASSIFYING TRANSCRIPT CONTENT\")\n","print(\"=\"*70)\n","\n","# Get all cleaned transcript files\n","cleaned_files = glob(os.path.join(cleaned_output_dir, '*.csv'))\n","print(f\"Found {len(cleaned_files)} cleaned transcript files to classify\")\n","\n","results_list = []\n","all_sentences = []\n","\n","for idx, csv_file in enumerate(cleaned_files):\n","    filename = os.path.basename(csv_file)\n","    official_name = filename.replace('.csv', '').replace('_', ' ')\n","\n","    if idx % 5 == 0:\n","        print(f\"Processing file {idx+1}/{len(cleaned_files)}: {filename}\")\n","\n","    try:\n","        # Read CSV without any string processing that could trigger regex\n","        df = pd.read_csv(csv_file, encoding='utf-8', encoding_errors='replace')\n","\n","        if 'Text' not in df.columns:\n","            print(f\"  Warning: No 'Text' column in {filename}, skipping...\")\n","            continue\n","\n","        # Process each row (each speech/statement)\n","        for row_idx, row in df.iterrows():\n","            # Convert to string safely without regex\n","            if pd.isna(row['Text']):\n","                text = ''\n","            else:\n","                text = str(row['Text'])\n","\n","            if len(text.strip()) == 0:\n","                continue\n","\n","            summary_results, sentence_data_list = analyze_transcript(text)\n","\n","            # Add metadata from the transcript - safely get values\n","            summary_results['transcript_id'] = str(row.get('id', f\"{official_name}_{row_idx}\")) if 'id' in row else f\"{official_name}_{row_idx}\"\n","            summary_results['date'] = str(row.get('Date', '')) if 'Date' in row else ''\n","            summary_results['official_name'] = str(row.get('Name', official_name)) if 'Name' in row else official_name\n","            summary_results['role'] = str(row.get('Role', '')) if 'Role' in row else ''\n","\n","            results_list.append(summary_results)\n","\n","            # Add sentence-level data\n","            for sentence_data in sentence_data_list:\n","                sentence_data['transcript_id'] = summary_results['transcript_id']\n","                sentence_data['date'] = summary_results['date']\n","                sentence_data['official_name'] = summary_results['official_name']\n","                all_sentences.append(sentence_data)\n","\n","    except Exception as e:\n","        print(f\"  Error processing {filename}: {e}\")\n","        import traceback\n","        print(f\"  Full traceback: {traceback.format_exc()}\")\n","        continue\n","\n","# Create summary dataframe\n","results_df = pd.DataFrame(results_list)\n","\n","if len(results_df) > 0:\n","    cols = ['transcript_id', 'date', 'official_name', 'role'] + [col for col in results_df.columns if col not in ['transcript_id', 'date', 'official_name', 'role']]\n","    results_df = results_df[[col for col in cols if col in results_df.columns]]\n","    results_df = results_df.sort_values('date')\n","\n","    summary_output_file = os.path.join(summary_output_dir, 'transcripts_content.csv')\n","    results_df.to_csv(summary_output_file, index=False)\n","    print(f\"\\nSummary dataset saved to: {summary_output_file}\")\n","    print(f\"Shape: {results_df.shape}\")\n","else:\n","    print(\"\\nWarning: No results to save!\")\n","\n","# Create sentence-level dataframe\n","sentences_df = pd.DataFrame(all_sentences)\n","\n","if len(sentences_df) > 0:\n","    print(f\"\\nTotal sentences extracted: {len(sentences_df)}\")\n","    print(\"\\nClassification distribution:\")\n","    print(sentences_df['classification'].value_counts())\n","\n","    # Sample sentences for validation\n","    n_labor = 15\n","    n_inflation = 15\n","    n_both = 5\n","    n_neither = 10\n","\n","    print(f\"\\nSampling sentences for validation...\")\n","    validation_samples = []\n","\n","    labor_sentences = sentences_df[sentences_df['classification'] == 'Labor']\n","    if len(labor_sentences) >= n_labor:\n","        validation_samples.append(labor_sentences.sample(n=n_labor, random_state=seed))\n","    else:\n","        print(f\"Warning: Only {len(labor_sentences)} labor sentences available\")\n","        validation_samples.append(labor_sentences)\n","\n","    inflation_sentences = sentences_df[sentences_df['classification'] == 'Inflation']\n","    if len(inflation_sentences) >= n_inflation:\n","        validation_samples.append(inflation_sentences.sample(n=n_inflation, random_state=seed))\n","    else:\n","        print(f\"Warning: Only {len(inflation_sentences)} inflation sentences available\")\n","        validation_samples.append(inflation_sentences)\n","\n","    both_sentences = sentences_df[sentences_df['classification'] == 'Both']\n","    if len(both_sentences) >= n_both:\n","        validation_samples.append(both_sentences.sample(n=n_both, random_state=seed))\n","    else:\n","        print(f\"Warning: Only {len(both_sentences)} both sentences available\")\n","        validation_samples.append(both_sentences)\n","\n","    neither_sentences = sentences_df[sentences_df['classification'] == 'Neither']\n","    if len(neither_sentences) >= n_neither:\n","        validation_samples.append(neither_sentences.sample(n=n_neither, random_state=seed))\n","    else:\n","        print(f\"Warning: Only {len(neither_sentences)} neither sentences available\")\n","        validation_samples.append(neither_sentences)\n","\n","    validation_df = pd.concat(validation_samples, ignore_index=True)\n","    validation_df = validation_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","    validation_output_file = os.path.join(validation_output_dir, 'transcripts_validate.csv')\n","    validation_df.to_csv(validation_output_file, index=False)\n","\n","    print(f\"\\nValidation set created: {validation_output_file}\")\n","    print(f\"Total sentences in validation set: {len(validation_df)}\")\n","    print(f\"\\nValidation set distribution:\")\n","    print(validation_df['classification'].value_counts())\n","\n","# Print summary statistics\n","if len(results_df) > 0:\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"SUMMARY STATISTICS\")\n","    print(\"=\"*70)\n","    print(f\"\\nNumber of transcripts analyzed: {len(results_df)}\")\n","    print(f\"\\nAverage sentences per transcript: {results_df['total_sentences'].mean():.1f}\")\n","    print(f\"Average labor sentences: {results_df['sentences_on_labor'].mean():.1f}\")\n","    print(f\"Average inflation sentences: {results_df['sentences_on_inflation'].mean():.1f}\")\n","    print(f\"Average sentences on both: {results_df['sentences_on_both'].mean():.1f}\")\n","    print(f\"Average labor share of labor/inflation: {results_df['labor_share_of_labor_inflation_sentences'].mean():.2%}\")\n","\n","    # Labor emphasis breakdown\n","    labor_emphasis_cols = [col for col in results_df.columns if col.startswith('labor_emphasis_')]\n","    print(\"\\n\" + \"-\"*70)\n","    print(\"AVERAGE LABOR EMPHASIS VECTORS\")\n","    print(\"-\"*70)\n","    for col in sorted(labor_emphasis_cols):\n","        indicator_name = col.replace('labor_emphasis_', '')\n","        avg_emphasis = results_df[col].mean()\n","        print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","    total_labor_emphasis = results_df[labor_emphasis_cols].mean().sum()\n","    print(f\"\\n{'Total':20s}: {total_labor_emphasis:.4f}\")\n","\n","    # Inflation emphasis breakdown\n","    inflation_emphasis_cols = [col for col in results_df.columns if col.startswith('inflation_emphasis_')]\n","    print(\"\\n\" + \"-\"*70)\n","    print(\"AVERAGE INFLATION EMPHASIS VECTORS\")\n","    print(\"-\"*70)\n","    for col in sorted(inflation_emphasis_cols):\n","        indicator_name = col.replace('inflation_emphasis_', '')\n","        avg_emphasis = results_df[col].mean()\n","        print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","    total_inflation_emphasis = results_df[inflation_emphasis_cols].mean().sum()\n","    print(f\"\\n{'Total':20s}: {total_inflation_emphasis:.4f}\")\n","\n","    # Officials with highest and lowest labor share\n","    print(\"\\n\" + \"-\"*70)\n","    print(\"OFFICIALS BY LABOR SHARE OF LABOR/INFLATION\")\n","    print(\"-\"*70)\n","\n","    # Calculate average per official\n","    official_labor_share = results_df.groupby('official_name')['labor_share_of_labor_inflation_sentences'].agg(['mean', 'count'])\n","    official_labor_share = official_labor_share[official_labor_share['count'] >= 3]  # At least 3 transcripts\n","    official_labor_share = official_labor_share.sort_values('mean', ascending=False)\n","\n","    print(\"\\nTop 10 Officials (Highest Labor Share):\")\n","    for idx, (official, row) in enumerate(official_labor_share.head(10).iterrows(), 1):\n","        print(f\"{idx:2d}. {official:30s}: {row['mean']:.2%} (n={int(row['count'])})\")\n","\n","    print(\"\\nBottom 10 Officials (Lowest Labor Share):\")\n","    for idx, (official, row) in enumerate(official_labor_share.tail(10).iterrows(), 1):\n","        print(f\"{idx:2d}. {official:30s}: {row['mean']:.2%} (n={int(row['count'])})\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"PROCESSING COMPLETE!\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J59J2gxPfMre","executionInfo":{"status":"ok","timestamp":1759955425780,"user_tz":300,"elapsed":509087,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"6b444376-6482-45d3-8fdb-f49bd5f51c0d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/FedComs/Transcripts\n","\n","Cleaning transcript files...\n","Reading from: /content/drive/MyDrive/FedComs/Transcripts/cleaned_transcripts\n","Found 53 transcript files\n","Processing Alan_Greenspan.csv...\n","Processing Roger_W_Ferguson.csv...\n","Processing William_McDonough.csv...\n","Processing Alfred_Broaddus.csv...\n","Processing Edward_G_Boehne.csv...\n","Processing Jerry_Jordan.csv...\n","Processing Robert_T_Parry.csv...\n","Processing Gary_H_Stern.csv...\n","Processing William_Poole.csv...\n","Processing Thomas_M_Hoenig.csv...\n","Processing Robert_D_McTeer.csv...\n","Processing Michael_Moskow.csv...\n","Processing Jack_Guynn.csv...\n","Processing Edward_M_Gramlich.csv...\n","Processing Anthony_M_Santomero.csv...\n","Processing Susan_S_Bies.csv...\n","Processing Mark_W_Olson.csv...\n","Processing Donald_Kohn.csv...\n","Processing Sandra_Pianalto.csv...\n","Processing Timothy_Geithner.csv...\n","Processing Jeffrey_M_Lacker.csv...\n","Processing Richard_W_Fisher.csv...\n","Processing Ben_Bernanke.csv...\n","Processing Kevin_Warsh.csv...\n","Processing Randall_Kroszner.csv...\n","Processing Charles_I_Plosser.csv...\n","Processing Frederic_S_Mishkin.csv...\n","Processing Dennis_P_Lockhart.csv...\n","Processing Eric_S_Rosengren.csv...\n","Processing Charles_L_Evans.csv...\n","Processing James_Bullard.csv...\n","Processing Elizabeth_Duke.csv...\n","Processing William_C_Dudley.csv...\n","Processing Daniel_Tarullo.csv...\n","Processing Narayana_Kocherlakota.csv...\n","Processing Janet_Yellen.csv...\n","Processing Sarah_Bloom_Raskin.csv...\n","Processing John_C_Williams.csv...\n","Processing Esther_L_George.csv...\n","Processing Jeremy_Stein.csv...\n","Processing Jerome_Powell.csv...\n","Processing Stanley_Fischer.csv...\n","Processing Loretta_Mester.csv...\n","Processing Lael_Brainard.csv...\n","Processing Patrick_T_Harker.csv...\n","Processing Robert_Steven_Kaplan.csv...\n","Processing Neel_Kashkari.csv...\n","Processing Raphael_Bostic.csv...\n","Processing Randal_Quarles.csv...\n","Processing Thomas_I_Barkin.csv...\n","Processing Richard_Clarida.csv...\n","Processing Mary_C_Daly.csv...\n","Processing Michelle_Bowman.csv...\n","\n","Cleaned 53 transcript files\n","Cleaned files saved to: /content/drive/MyDrive/FedComs/Transcripts/final_transcripts\n","\n","======================================================================\n","LOADING DICTIONARIES\n","======================================================================\n","Dictionaries loaded successfully!\n","Labor indicators: ['General Labor', 'Employment', 'Unemployment', 'Participation', 'Wages', 'Vacancies', 'Quits', 'Layoffs', 'Hiring']\n","Inflation categories: ['General Inflation', 'Core Measures', 'Headline Measures', 'Sectoral Measures', 'Producer Price Index', 'Wage Inflation', 'Inflation Expectations', 'Commodity Prices']\n","\n","======================================================================\n","CLASSIFYING TRANSCRIPT CONTENT\n","======================================================================\n","Found 53 cleaned transcript files to classify\n","Processing file 1/53: Alan_Greenspan.csv\n","Processing file 6/53: Jerry_Jordan.csv\n","Processing file 11/53: Robert_D_McTeer.csv\n","Processing file 16/53: Susan_S_Bies.csv\n","Processing file 21/53: Jeffrey_M_Lacker.csv\n","Processing file 26/53: Charles_I_Plosser.csv\n","Processing file 31/53: James_Bullard.csv\n","Processing file 36/53: Janet_Yellen.csv\n","Processing file 41/53: Jerome_Powell.csv\n","Processing file 46/53: Robert_Steven_Kaplan.csv\n","Processing file 51/53: Richard_Clarida.csv\n","\n","Summary dataset saved to: /content/drive/MyDrive/FedComs/Transcripts/transcripts_content.csv\n","Shape: (2589, 78)\n","\n","Total sentences extracted: 329108\n","\n","Classification distribution:\n","classification\n","Neither      284344\n","Labor         22256\n","Inflation     18898\n","Both           3610\n","Name: count, dtype: int64\n","\n","Sampling sentences for validation...\n","\n","Validation set created: /content/drive/MyDrive/FedComs/Validation_Sets/transcripts_validate.csv\n","Total sentences in validation set: 45\n","\n","Validation set distribution:\n","classification\n","Labor        15\n","Inflation    15\n","Neither      10\n","Both          5\n","Name: count, dtype: int64\n","\n","======================================================================\n","SUMMARY STATISTICS\n","======================================================================\n","\n","Number of transcripts analyzed: 2589\n","\n","Average sentences per transcript: 127.1\n","Average labor sentences: 10.0\n","Average inflation sentences: 8.7\n","Average sentences on both: 1.4\n","Average labor share of labor/inflation: 57.18%\n","\n","----------------------------------------------------------------------\n","AVERAGE LABOR EMPHASIS VECTORS\n","----------------------------------------------------------------------\n","Employment          : 0.1874 (18.74%)\n","Hiring              : 0.1446 (14.46%)\n","Layoffs             : 0.0232 (2.32%)\n","Participation       : 0.0587 (5.87%)\n","Quits               : 0.0092 (0.92%)\n","Unemployment        : 0.2705 (27.05%)\n","Vacancies           : 0.0217 (2.17%)\n","Wages               : 0.2170 (21.70%)\n","\n","Total               : 0.9324\n","\n","----------------------------------------------------------------------\n","AVERAGE INFLATION EMPHASIS VECTORS\n","----------------------------------------------------------------------\n","Commodity_Prices    : 0.1245 (12.45%)\n","Core                : 0.0725 (7.25%)\n","Core_CPI            : 0.0228 (2.28%)\n","Core_PCE            : 0.0425 (4.25%)\n","Energy              : 0.0604 (6.04%)\n","Food                : 0.0038 (0.38%)\n","Goods               : 0.0066 (0.66%)\n","Headline            : 0.0203 (2.03%)\n","Headline_CPI        : 0.0286 (2.86%)\n","Headline_PCE        : 0.0910 (9.10%)\n","Housing             : 0.0215 (2.15%)\n","Inflation_Expectations: 0.2454 (24.54%)\n","PPI                 : 0.0033 (0.33%)\n","Services            : 0.0000 (0.00%)\n","Wage_Inflation      : 0.1132 (11.32%)\n","\n","Total               : 0.8563\n","\n","----------------------------------------------------------------------\n","OFFICIALS BY LABOR SHARE OF LABOR/INFLATION\n","----------------------------------------------------------------------\n","\n","Top 10 Officials (Highest Labor Share):\n"," 1. Daniel Tarullo                : 83.87% (n=65)\n"," 2. Jeremy Stein                  : 80.11% (n=16)\n"," 3. Sarah Bloom Raskin            : 77.96% (n=23)\n"," 4. Jerry Jordan                  : 77.94% (n=23)\n"," 5. Patrick T. Harker             : 75.53% (n=36)\n"," 6. Stanley Fischer               : 73.10% (n=27)\n"," 7. Jerome Powell                 : 71.67% (n=61)\n"," 8. Narayana Kocherlakota         : 71.62% (n=49)\n"," 9. Eric S. Rosengren             : 71.21% (n=100)\n","10. Michelle Bowman               : 70.87% (n=9)\n","\n","Bottom 10 Officials (Lowest Labor Share):\n"," 1. Roger W. Ferguson             : 47.30% (n=50)\n"," 2. Richard Clarida               : 44.06% (n=11)\n"," 3. Alan Greenspan                : 41.64% (n=50)\n"," 4. Sandra Pianalto               : 40.85% (n=91)\n"," 5. Kevin Warsh                   : 38.70% (n=40)\n"," 6. Randall Kroszner              : 30.93% (n=23)\n"," 7. Donald Kohn                   : 30.88% (n=70)\n"," 8. Mark W. Olson                 : 28.83% (n=37)\n"," 9. Timothy Geithner              : 22.26% (n=39)\n","10. Frederic S. Mishkin           : 12.93% (n=16)\n","\n","======================================================================\n","PROCESSING COMPLETE!\n","======================================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xRiZP5M5UvZE"},"execution_count":null,"outputs":[]}]}