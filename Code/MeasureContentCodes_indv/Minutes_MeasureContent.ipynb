{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kBRk_zj4e9dQt5s2py8S5KQiU5gSu3Gb","timestamp":1759433333106}],"gpuType":"T4","mount_file_id":"1sdrBaQCwRh59KkEiso3VYjOyFtRmf7QP","authorship_tag":"ABX9TyMoOlLn6iDRidqdML3bwJIV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Rot40QDe4DD","executionInfo":{"status":"ok","timestamp":1760017265593,"user_tz":300,"elapsed":11784,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"8593071d-28c9-4907-ac4f-151c8d6a89e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/Minutes\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import os\n","dict_dir = '/content/drive/MyDrive/FedComs/Dictionaries'\n","input_file = '/content/drive/MyDrive/FedComs/Minutes/fomc_minutes.csv'\n","cleaned_file = '/content/drive/MyDrive/FedComs/Minutes/fomc_minutes_cleaned.csv'\n","output_dir = '/content/drive/MyDrive/Minutes'\n","validation_dir = '/content/drive/MyDrive/FedComs/Validation_Sets'\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","os.chdir(output_dir)\n","\n","# Verify the current working directory\n","print(f\"Current working directory: {os.getcwd()}\")"]},{"cell_type":"code","source":["#@title Additional Clean\n","import pandas as pd\n","import re\n","\n","input_file = '/content/drive/MyDrive/FedComs/Minutes/fomc_minutes.csv'\n","output_dir = '/content/drive/MyDrive/FedComs/Minutes'\n","\n","print(\"Reading FOMC minutes...\")\n","df = pd.read_csv(input_file)\n","print(f\"Loaded {len(df)} minutes\")\n","\n","df['date'] = pd.to_datetime(df['date'])\n","\n","def fix_text_encoding(text):\n","    \"\"\"Fix common text encoding issues from web scraping.\"\"\"\n","    text = text.replace('â', '—')\n","    text = text.replace('â', '—')\n","    text = text.replace('â', '\"')\n","    text = text.replace('â', '\"')\n","    text = text.replace('\\u2013', '–')\n","    text = text.replace('\\u2014', '—')\n","    text = text.replace('\\u2018', \"'\")\n","    text = text.replace('\\u2019', \"'\")\n","    text = text.replace('\\u201c', '\"')\n","    text = text.replace('\\u201d', '\"')\n","    text = text.replace('\\u2026', '...')\n","    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n","    return text\n","\n","def is_in_footnote(text, match_position):\n","    \"\"\"Check if a match position is within a footnote.\"\"\"\n","    start_check = max(0, match_position - 500)\n","    text_before = text[start_check:match_position]\n","    text_after = text[match_position:match_position + 200]\n","\n","    if re.search(r'Return to text', text_before, re.IGNORECASE):\n","        if not re.search(r'Return to text', text_after, re.IGNORECASE):\n","            return True\n","\n","    if re.search(r'\\[\\d+\\]', text_before):\n","        return True\n","\n","    if re.search(r'\\d+\\.\\s+[A-Z]', text_before[-100:]):\n","        return True\n","\n","    return False\n","\n","def clean_text_minutes(text, date):\n","    \"\"\"Clean text for FOMC minutes based on date.\"\"\"\n","    text = fix_text_encoding(text)\n","\n","    if date == pd.Timestamp('2021-11-03'):\n","        start_pattern = r'The manager turned first to a discussion'\n","        match = re.search(start_pattern, text, re.IGNORECASE)\n","        if match:\n","            sentence_start = text.rfind('.', 0, match.start())\n","            if sentence_start == -1:\n","                sentence_start = 0\n","            else:\n","                sentence_start += 1\n","            text = text[sentence_start:].strip()\n","    elif date < pd.Timestamp('2019-01-01'):\n","        pattern = r'Manager of the System Open'\n","        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n","        if matches:\n","            match = matches[0]\n","            sentence_end = text.find('.', match.end())\n","            if sentence_end != -1:\n","                text = text[sentence_end + 1:].strip()\n","    else:\n","        pattern = r'Developments in Financial Markets and Open Market Operations'\n","        matches = list(re.finditer(pattern, text, re.IGNORECASE))\n","        if matches:\n","            selected_match = None\n","            for match in matches:\n","                if not is_in_footnote(text, match.start()):\n","                    selected_match = match\n","                    break\n","            if selected_match is None:\n","                selected_match = matches[0]\n","            sentence_end = text.find('.', selected_match.end())\n","            if sentence_end != -1:\n","                text = text[sentence_end + 1:].strip()\n","\n","    end_pattern = r'meeting adjourned'\n","    match = re.search(end_pattern, text, re.IGNORECASE)\n","    if match:\n","        sentence_start = text.rfind('.', 0, match.start())\n","        if sentence_start == -1:\n","            sentence_start = 0\n","        text = text[:sentence_start].strip()\n","\n","    return text.strip()\n","\n","print(\"\\nCleaning minutes...\")\n","cleaned_texts = []\n","\n","for idx, row in df.iterrows():\n","    if idx % 20 == 0:\n","        print(f\"Processing minutes {idx+1}/{len(df)}...\")\n","    date = row['date']\n","    text = row['text']\n","    cleaned_text = clean_text_minutes(text, date)\n","    cleaned_texts.append(cleaned_text)\n","\n","df_cleaned = df.copy()\n","df_cleaned['text'] = cleaned_texts\n","df_cleaned['date'] = df_cleaned['date'].dt.strftime('%Y-%m-%d')\n","\n","output_file = os.path.join(output_dir, 'fomc_minutes_cleaned.csv')\n","df_cleaned.to_csv(output_file, index=False)\n","\n","print(f\"\\nCleaned minutes saved to: {output_file}\")\n","print(f\"Total minutes: {len(df_cleaned)}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"CLEANING STATISTICS\")\n","print(\"=\"*70)\n","\n","original_lengths = df['text'].str.len()\n","cleaned_lengths = df_cleaned['text'].str.len()\n","\n","print(f\"\\nAverage original text length: {original_lengths.mean():.0f} characters\")\n","print(f\"Average cleaned text length: {cleaned_lengths.mean():.0f} characters\")\n","print(f\"Average reduction: {(original_lengths.mean() - cleaned_lengths.mean()):.0f} characters ({((1 - cleaned_lengths.mean()/original_lengths.mean())*100):.1f}%)\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"EXAMPLES OF CLEANED TEXT\")\n","print(\"=\"*70)\n","\n","pre_2019_example = df_cleaned[df_cleaned['date'] < '2019-01-01'].iloc[-1] if len(df_cleaned[df_cleaned['date'] < '2019-01-01']) > 0 else None\n","if pre_2019_example is not None:\n","    print(f\"\\nPRE-2019 PERIOD\")\n","    print(f\"Date: {pre_2019_example['date']}\")\n","    print(f\"First 200 characters: {pre_2019_example['text'][:200]}...\")\n","\n","post_2019_example = df_cleaned[df_cleaned['date'] >= '2019-01-01'].iloc[0] if len(df_cleaned[df_cleaned['date'] >= '2019-01-01']) > 0 else None\n","if post_2019_example is not None:\n","    print(f\"\\n2019 ONWARD PERIOD\")\n","    print(f\"Date: {post_2019_example['date']}\")\n","    print(f\"First 200 characters: {post_2019_example['text'][:200]}...\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"SHORTEST 5 ENTRIES (FOR INSPECTION)\")\n","print(\"=\"*70)\n","\n","df_cleaned['text_length'] = df_cleaned['text'].str.len()\n","shortest_5 = df_cleaned.nsmallest(5, 'text_length')[['id', 'date', 'text_length', 'text']]\n","\n","for idx, row in shortest_5.iterrows():\n","    print(f\"\\n{'-'*70}\")\n","    print(f\"ID: {row['id']}\")\n","    print(f\"Date: {row['date']}\")\n","    print(f\"Length: {row['text_length']} characters\")\n","    print(f\"\\nFull text:\")\n","    print(row['text'][:1000])\n","    if row['text_length'] > 1000:\n","        print(f\"\\n... (truncated, {row['text_length'] - 1000} more characters)\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Cleaning complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3MzWyRh-xtXX","executionInfo":{"status":"ok","timestamp":1759524475931,"user_tz":300,"elapsed":863,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"45c75e38-3e88-4e0f-c650-3ed3f07d3e0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading FOMC minutes...\n","Loaded 199 minutes\n","\n","Cleaning minutes...\n","Processing minutes 1/199...\n","Processing minutes 21/199...\n","Processing minutes 41/199...\n","Processing minutes 61/199...\n","Processing minutes 81/199...\n","Processing minutes 101/199...\n","Processing minutes 121/199...\n","Processing minutes 141/199...\n","Processing minutes 161/199...\n","Processing minutes 181/199...\n","\n","Cleaned minutes saved to: /content/drive/MyDrive/FedComs/Minutes/fomc_minutes_cleaned.csv\n","Total minutes: 199\n","\n","======================================================================\n","CLEANING STATISTICS\n","======================================================================\n","\n","Average original text length: 47193 characters\n","Average cleaned text length: 39751 characters\n","Average reduction: 7442 characters (15.8%)\n","\n","======================================================================\n","EXAMPLES OF CLEANED TEXT\n","======================================================================\n","\n","PRE-2019 PERIOD\n","Date: 2018-12-19\n","First 200 characters: Minutes of the Federal Open Market Committee\n","December 18-19, 2018\n","A joint meeting of the Federal Open Market Committee and the Board of Governors was held in the offices of the Board of Governors of t...\n","\n","2019 ONWARD PERIOD\n","Date: 2019-01-30\n","First 200 characters: S. and global financial markets. Financial markets were quite volatile over the intermeeting period. Market participants pointed to a number of factors as contributing to the heightened volatility and...\n","\n","======================================================================\n","SHORTEST 5 ENTRIES (FOR INSPECTION)\n","======================================================================\n","\n","----------------------------------------------------------------------\n","ID: minutes_20050809\n","Date: 2005-08-09\n","Length: 17993 characters\n","\n","Full text:\n","There were no open market operations in foreign currencies for the System's account in the period since the previous meeting. The Manager also reported on developments in domestic financial markets and on System open market transactions in government securities and federal agency obligations during the period since the previous meeting. By unanimous vote, the Committee ratified these transactions. The information received at this meeting suggested that final demand had expanded at a solid pace in the second quarter, led by a surge in net exports and another robust gain in residential investment, while business investment and consumer spending rose at moderate rates. The labor market continued to improve gradually in June and July. Core CPI and PCE prices decelerated in recent months, after notable increases earlier in the year. Crude oil prices continued to rise, reaching record levels in nominal terms over the intermeeting period. Payroll employment grew in June and July at a pace tha\n","\n","... (truncated, 16993 more characters)\n","\n","----------------------------------------------------------------------\n","ID: minutes_20021210\n","Date: 2002-12-10\n","Length: 19392 characters\n","\n","Full text:\n","There were no open market operations in foreign currencies for the System's account in the period since the previous meeting. The Manager also reported on developments in domestic financial markets and on System open market transactions in government securities and securities issued or fully guaranteed by federal agencies during the period November 6, 2002, through December 9, 2002. By unanimous vote, the Committee ratified these transactions. The Committee then turned to a discussion of the economic and financial outlook and the conduct of monetary policy over the intermeeting period ahead. The information reviewed at this meeting suggested that economic growth had been sluggish on balance since midsummer. Housing demand remained strong, but business fixed investment was still in the doldrums and consumer spending had flagged in late summer before apparently picking up somewhat in the autumn. Payroll employment had changed little since midyear, and industrial production still seemed t\n","\n","... (truncated, 18392 more characters)\n","\n","----------------------------------------------------------------------\n","ID: minutes_20050920\n","Date: 2005-09-20\n","Length: 19409 characters\n","\n","Full text:\n","There were no open market operations in foreign currencies for the System's account in the period since the previous meeting. The Manager also reported on developments in domestic financial markets and on System open market transactions in government securities and federal agency obligations during the period since the previous meeting. By unanimous vote, the Committee ratified these transactions. The information reviewed at this meeting suggested that, before the landfall of Hurricane Katrina on the Gulf Coast, expansion of economic activity had been solid, led by robust gains in housing and buoyant consumer spending. While business investment appeared to be losing some momentum, labor markets continued to improve, and increases in core CPI and PCE prices were modest after notable increases earlier in the year. Only limited data bearing on the likely economic effects of the hurricane were available. Oil and gasoline prices, however, were on the rise, spiking to record levels in the da\n","\n","... (truncated, 18409 more characters)\n","\n","----------------------------------------------------------------------\n","ID: minutes_20060131\n","Date: 2006-01-31\n","Length: 19480 characters\n","\n","Full text:\n","There were no open market operations in foreign currencies for the System's account in the period since the previous meeting. The Manager also reported on developments in domestic financial markets and on System open market transactions in government securities and federal agency obligations during the period since the previous meeting. By unanimous vote, the Committee ratified these transactions. The information reviewed at this meeting suggested that underlying growth in aggregate demand remained solid, even though the expansion of real GDP was estimated to have slowed in the fourth quarter. Household spending rose smartly, outside of autos, and orders and shipments of nondefense capital goods in the business sector were generally quite strong. Housing markets showed some signs of cooling, but starts and sales remained at high levels. Industrial production posted moderate gains, even after excluding hurricane-related rebounds in some production categories, and private payrolls expand\n","\n","... (truncated, 18480 more characters)\n","\n","----------------------------------------------------------------------\n","ID: minutes_20051213\n","Date: 2005-12-13\n","Length: 19999 characters\n","\n","Full text:\n","There were no open\n","market operations in foreign currencies for the System's account in the\n","period since the previous meeting. The Manager also reported on developments\n","in domestic financial markets and on System open market transactions in\n","government securities and federal agency obligations during the period\n","since the previous meeting. By unanimous vote, the Committee ratified\n","these transactions. The information reviewed at this meeting suggested that the economy\n","continued to expand at a solid rate in the fourth quarter. Industrial\n","production rebounded, and employment growth appeared to have recovered\n","smartly from the depressing effects of recent hurricanes. Although some\n","scattered signs of cooling of the housing sector had emerged, the pace\n","of construction activity and sales remained brisk. More broadly, spending\n","by consumers and businesses was well maintained. Core consumer price inflation\n","remained subdued, even though some of the increase in energy costs had\n","apparently passed throu\n","\n","... (truncated, 18999 more characters)\n","\n","======================================================================\n","Cleaning complete!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5WYFGwm5OMgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@ Classify content\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import random\n","import time\n","import os\n","\n","seed = int(time.time())\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","dict_dir = '/content/drive/MyDrive/FedComs/Dictionaries'\n","input_file = '/content/drive/MyDrive/FedComs/Minutes/fomc_minutes_cleaned.csv'\n","output_dir = '/content/drive/MyDrive/FedComs/Minutes'\n","validation_dir = '/content/drive/MyDrive/FedComs/Validation_Sets'\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"\\nLoading dictionaries...\")\n","with open(os.path.join(dict_dir, 'labor_indicators.json'), 'r') as f:\n","    LABOR_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_indicators.json'), 'r') as f:\n","    INFLATION_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_pattern_mapping.json'), 'r') as f:\n","    INFLATION_PATTERN_TO_INDICATOR = json.load(f)\n","\n","print(\"Dictionaries loaded successfully!\")\n","print(f\"Labor indicators: {list(LABOR_INDICATORS.keys())}\")\n","print(f\"Inflation categories: {list(INFLATION_INDICATORS.keys())}\")\n","\n","def fix_text_encoding(text):\n","    \"\"\"Fix text encoding issues (if any).\"\"\"\n","    return text.encode('utf-8', errors='ignore').decode('utf-8')\n","\n","def split_into_sentences(text):\n","    \"\"\"Split text into sentences, preserving initials and abbreviations.\"\"\"\n","    text = fix_text_encoding(text)\n","\n","    abbreviations = [\n","        r'\\bU\\.S\\.A\\.', r'\\bU\\.S\\.', r'\\bU\\.K\\.', r'\\bE\\.U\\.',\n","        r'\\bSt\\.', r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.',\n","        r'\\bProf\\.', r'\\bSr\\.', r'\\bJr\\.', r'\\bvs\\.', r'\\betc\\.',\n","        r'\\bi\\.e\\.', r'\\be\\.g\\.', r'\\bVol\\.', r'\\bNo\\.', r'\\bpp\\.',\n","        r'\\bCo\\.', r'\\bInc\\.', r'\\bLtd\\.', r'\\bCorp\\.',\n","        r'\\bPh\\.D\\.', r'\\bM\\.A\\.', r'\\bM\\.S\\.', r'\\bB\\.A\\.',\n","        r'\\bD\\.C\\.', r'\\bA\\.M\\.', r'\\bP\\.M\\.'\n","    ]\n","\n","    for idx, abbr in enumerate(abbreviations):\n","        text = re.sub(abbr, f'<ABBR_{idx}>', text, flags=re.IGNORECASE)\n","\n","    text = re.sub(r'\\b([A-Z])\\.(\\s+[A-Z]\\.)*(?=\\s+[A-Z][a-z]+)', lambda m: m.group(0).replace('.', f'<NAME>'), text)\n","    text = re.sub(r'\\b\\d+\\.\\d+\\b', lambda m: m.group(0).replace('.', '<DEC>'), text)\n","\n","    voting_pattern = r'((?:Voting for|Voting against)\\s+[^.!?]+?)([.!?]+\\s+|$)'\n","    voting_matches = []\n","    def store_voting_match(match):\n","        voting_matches.append(match.group(1))\n","        return f'<VOTE_{len(voting_matches) - 1}>'\n","    text = re.sub(voting_pattern, store_voting_match, text)\n","\n","    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z]|$)', text)\n","    sentences = [s.strip() for s in sentences if s.strip()]\n","\n","    restored_sentences = []\n","    for sentence in sentences:\n","        for idx in range(len(abbreviations)):\n","            sentence = sentence.replace(f'<ABBR_{idx}>', abbreviations[idx].replace(r'\\b', '').replace(r'\\.', '.'))\n","        sentence = sentence.replace('<NAME>', '.')\n","        sentence = sentence.replace('<DEC>', '.')\n","        for i, voting_list in enumerate(voting_matches):\n","            placeholder = f'<VOTE_{i}>'\n","            if placeholder in sentence:\n","                sentence = sentence.replace(placeholder, voting_list)\n","        restored_sentences.append(sentence)\n","\n","    return restored_sentences\n","\n","def check_keywords_in_sentence(sentence, keywords):\n","    \"\"\"Check if any keyword appears in the sentence.\"\"\"\n","    sentence_lower = sentence.lower()\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_employment_indicator(sentence, keywords):\n","    \"\"\"Check for Employment indicator, excluding maximum/full employment.\"\"\"\n","    sentence_lower = sentence.lower()\n","\n","    # Check if sentence contains maximum employment, full employment, or employment goal\n","    if re.search(r'\\b(?:maximum|full)\\s+employment\\b', sentence_lower):\n","        return False\n","    if re.search(r'\\bemployment\\s+goal\\b', sentence_lower):\n","        return False\n","\n","    # Otherwise check for employment keywords normally\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_labor_term(sentence):\n","    \"\"\"Check if sentence contains general labor terms.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_labor_keywords = LABOR_INDICATORS.get(\"General Labor\", [])\n","    for keyword in general_labor_keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_inflation_terms(sentence):\n","    \"\"\"Check if sentence contains general inflation terms.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_inflation_patterns = INFLATION_INDICATORS.get(\"General Inflation\", {}).get(\"general_patterns\", [])\n","    for pattern in general_inflation_patterns:\n","        if re.search(pattern, sentence_lower, re.IGNORECASE):\n","            return True\n","    return False\n","\n","def check_inflation_sentence(sentence):\n","    \"\"\"Check if sentence mentions any inflation indicator.\"\"\"\n","    mentioned_indicators = set()\n","    sentence_lower = sentence.lower()\n","\n","    for category, subcategories in INFLATION_INDICATORS.items():\n","        for pattern_name, pattern_list in subcategories.items():\n","            for pattern in pattern_list:\n","                if re.search(pattern, sentence_lower, re.IGNORECASE):\n","                    indicator_name = INFLATION_PATTERN_TO_INDICATOR.get(pattern_name, \"Other\")\n","                    mentioned_indicators.add(indicator_name)\n","                    break\n","\n","    # If sentence has both Core_CPI and Core, remove the generic Core\n","    if \"Core_CPI\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Core_PCE and Core, remove the generic Core\n","    if \"Core_PCE\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Headline_CPI and Headline, remove the generic Headline\n","    if \"Headline_CPI\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    # If sentence has both Headline_PCE and Headline, remove the generic Headline\n","    if \"Headline_PCE\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    return mentioned_indicators\n","\n","def classify_sentence(sentence):\n","    \"\"\"Classify a single sentence and return its indicators.\"\"\"\n","    labor_specific_found = False\n","    labor_indicators_in_sentence = set()\n","\n","    # Check all labor indicators EXCEPT \"General Labor\"\n","    for indicator, keywords in LABOR_INDICATORS.items():\n","        if indicator == \"General Labor\":\n","            continue  # Skip general labor for indicator counts\n","\n","        # Use special handling for Employment indicator\n","        if indicator == \"Employment\":\n","            if check_employment_indicator(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","        else:\n","            if check_keywords_in_sentence(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","\n","    labor_general_found = check_general_labor_term(sentence)\n","    labor_found = labor_specific_found or labor_general_found\n","\n","    inflation_indicators_in_sentence = check_inflation_sentence(sentence)\n","    inflation_specific_found = bool(inflation_indicators_in_sentence)\n","\n","    inflation_general_found = check_general_inflation_terms(sentence)\n","    inflation_found = inflation_specific_found or inflation_general_found\n","\n","    if labor_found and inflation_found:\n","        classification = \"Both\"\n","    elif labor_found:\n","        classification = \"Labor\"\n","    elif inflation_found:\n","        classification = \"Inflation\"\n","    else:\n","        classification = \"Neither\"\n","\n","    return {\n","        'classification': classification,\n","        'labor_indicators': list(labor_indicators_in_sentence),\n","        'inflation_indicators': list(inflation_indicators_in_sentence)\n","    }\n","\n","def analyze_minutes(text):\n","    \"\"\"Analyze a single minutes document for labor and inflation content.\"\"\"\n","    sentences = split_into_sentences(text)\n","    total_sentences = len(sentences)\n","\n","    labor_sentences = 0\n","    inflation_sentences = 0\n","    both_sentences = 0\n","\n","    # Only create indicator counts for non-general categories\n","    labor_indicator_counts = {indicator: 0 for indicator in LABOR_INDICATORS.keys() if indicator != \"General Labor\"}\n","    inflation_indicator_list = sorted(list(set(\n","        indicator for indicator in INFLATION_PATTERN_TO_INDICATOR.values()\n","        if indicator not in [\"General_Inflation\", \"Other\"]\n","    )))\n","    inflation_indicator_counts = {indicator: 0 for indicator in inflation_indicator_list}\n","\n","    sentence_data_list = []\n","\n","    for sent_idx, sentence in enumerate(sentences):\n","        classification_result = classify_sentence(sentence)\n","\n","        # Filter out general categories from the indicator lists\n","        labor_indicators_filtered = [ind for ind in classification_result['labor_indicators']\n","                                      if ind != \"General Labor\"]\n","        inflation_indicators_filtered = [ind for ind in classification_result['inflation_indicators']\n","                                          if ind not in [\"General_Inflation\", \"Other\"]]\n","\n","        sentence_data = {\n","            'sentence_number': sent_idx + 1,\n","            'sentence_text': sentence,\n","            'classification': classification_result['classification'],\n","            'labor_indicators': ', '.join(sorted(labor_indicators_filtered)) if labor_indicators_filtered else '',\n","            'inflation_indicators': ', '.join(sorted(inflation_indicators_filtered)) if inflation_indicators_filtered else ''\n","        }\n","        sentence_data_list.append(sentence_data)\n","\n","        labor_specific_found = bool(classification_result['labor_indicators'])\n","        labor_general_found = check_general_labor_term(sentence)\n","        labor_found = labor_specific_found or labor_general_found\n","\n","        inflation_specific_found = bool(classification_result['inflation_indicators'])\n","        inflation_general_found = check_general_inflation_terms(sentence)\n","        inflation_found = inflation_specific_found or inflation_general_found\n","\n","        if labor_found and inflation_found:\n","            both_sentences += 1\n","            labor_sentences += 1\n","            inflation_sentences += 1\n","        elif labor_found:\n","            labor_sentences += 1\n","        elif inflation_found:\n","            inflation_sentences += 1\n","\n","        # Only count specific indicators (not general terms or \"Other\") for emphasis vectors\n","        for indicator in classification_result['labor_indicators']:\n","            if indicator in labor_indicator_counts:\n","                labor_indicator_counts[indicator] += 1\n","\n","        for indicator in classification_result['inflation_indicators']:\n","            if indicator in inflation_indicator_counts:\n","                inflation_indicator_counts[indicator] += 1\n","\n","    total_labor_mentions = sum(labor_indicator_counts.values())\n","    total_inflation_mentions = sum(inflation_indicator_counts.values())\n","\n","    labor_emphasis = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_emphasis[f\"labor_emphasis_{indicator}\"] = count / total_labor_mentions if total_labor_mentions > 0 else 0\n","\n","    inflation_emphasis = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_emphasis[f\"inflation_emphasis_{indicator}\"] = count / total_inflation_mentions if total_inflation_mentions > 0 else 0\n","\n","    labor_sentence_share = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_sentence_share[f\"labor_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    inflation_sentence_share = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_sentence_share[f\"inflation_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    labor_inflation_total = labor_sentences + inflation_sentences - both_sentences\n","    labor_share_of_labor_inflation = labor_sentences / labor_inflation_total if labor_inflation_total > 0 else 0\n","\n","    summary_results = {\n","        'sentences_on_labor': labor_sentences,\n","        'sentences_on_inflation': inflation_sentences,\n","        'sentences_on_both': both_sentences,\n","        'total_sentences': total_sentences,\n","        'labor_share_of_labor_inflation_sentences': labor_share_of_labor_inflation\n","    }\n","\n","    for indicator, count in labor_indicator_counts.items():\n","        summary_results[f'labor_{indicator}_count'] = count\n","\n","    for indicator, count in inflation_indicator_counts.items():\n","        summary_results[f'inflation_{indicator}_count'] = count\n","\n","    summary_results.update(labor_emphasis)\n","    summary_results.update(inflation_emphasis)\n","    summary_results.update(labor_sentence_share)\n","    summary_results.update(inflation_sentence_share)\n","\n","    return summary_results, sentence_data_list\n","\n","print(\"\\nReading cleaned FOMC minutes...\")\n","df = pd.read_csv(input_file)\n","print(f\"Loaded {len(df)} minutes\")\n","\n","print(\"\\nAnalyzing minutes...\")\n","results_list = []\n","all_sentences = []\n","\n","for idx, row in df.iterrows():\n","    if idx % 10 == 0:\n","        print(f\"Processing minutes {idx+1}/{len(df)}...\")\n","\n","    summary_results, sentence_data_list = analyze_minutes(row['text'])\n","    summary_results['date'] = row['date']\n","    summary_results['id'] = row['id']\n","    results_list.append(summary_results)\n","\n","    for sentence_data in sentence_data_list:\n","        sentence_data['minutes_date'] = row['date']\n","        sentence_data['minutes_id'] = row['id']\n","        all_sentences.append(sentence_data)\n","\n","results_df = pd.DataFrame(results_list)\n","cols = ['id', 'date'] + [col for col in results_df.columns if col not in ['id', 'date']]\n","results_df = results_df[cols]\n","results_df = results_df.sort_values('date')\n","\n","summary_output_file = os.path.join(output_dir, 'minutes_content.csv')\n","results_df.to_csv(summary_output_file, index=False)\n","print(f\"\\nSummary dataset saved to: {summary_output_file}\")\n","print(f\"Shape: {results_df.shape}\")\n","\n","sentences_df = pd.DataFrame(all_sentences)\n","print(f\"\\nTotal sentences extracted: {len(sentences_df)}\")\n","\n","print(\"\\nClassification distribution:\")\n","print(sentences_df['classification'].value_counts())\n","\n","n_labor = 15\n","n_inflation = 15\n","n_both = 5\n","n_neither = 10\n","\n","print(f\"\\nSampling sentences for validation...\")\n","validation_samples = []\n","\n","labor_sentences = sentences_df[sentences_df['classification'] == 'Labor']\n","if len(labor_sentences) >= n_labor:\n","    validation_samples.append(labor_sentences.sample(n=n_labor, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(labor_sentences)} labor sentences available\")\n","    validation_samples.append(labor_sentences)\n","\n","inflation_sentences = sentences_df[sentences_df['classification'] == 'Inflation']\n","if len(inflation_sentences) >= n_inflation:\n","    validation_samples.append(inflation_sentences.sample(n=n_inflation, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(inflation_sentences)} inflation sentences available\")\n","    validation_samples.append(inflation_sentences)\n","\n","both_sentences = sentences_df[sentences_df['classification'] == 'Both']\n","if len(both_sentences) >= n_both:\n","    validation_samples.append(both_sentences.sample(n=n_both, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(both_sentences)} both sentences available\")\n","    validation_samples.append(both_sentences)\n","\n","neither_sentences = sentences_df[sentences_df['classification'] == 'Neither']\n","if len(neither_sentences) >= n_neither:\n","    validation_samples.append(neither_sentences.sample(n=n_neither, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(neither_sentences)} neither sentences available\")\n","    validation_samples.append(neither_sentences)\n","\n","validation_df = pd.concat(validation_samples, ignore_index=True)\n","validation_df = validation_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","validation_output_file = os.path.join(validation_dir, 'minutes_validate.csv')\n","validation_df.to_csv(validation_output_file, index=False)\n","\n","print(f\"\\nValidation set created: {validation_output_file}\")\n","print(f\"Total sentences in validation set: {len(validation_df)}\")\n","print(f\"\\nValidation set distribution:\")\n","print(validation_df['classification'].value_counts())\n","\n","results_df['date'] = pd.to_datetime(results_df['date'])\n","df_2010_plus = results_df[results_df['date'] >= '2010-01-01'].copy()\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"SUMMARY STATISTICS (2010-CURRENT)\")\n","print(\"=\"*70)\n","print(f\"\\nNumber of minutes: {len(df_2010_plus)}\")\n","print(f\"Date range: {df_2010_plus['date'].min().strftime('%Y-%m-%d')} to {df_2010_plus['date'].max().strftime('%Y-%m-%d')}\")\n","print(f\"\\nAverage sentences per minutes: {df_2010_plus['total_sentences'].mean():.1f}\")\n","print(f\"Average labor sentences: {df_2010_plus['sentences_on_labor'].mean():.1f}\")\n","print(f\"Average inflation sentences: {df_2010_plus['sentences_on_inflation'].mean():.1f}\")\n","print(f\"Average sentences on both: {df_2010_plus['sentences_on_both'].mean():.1f}\")\n","print(f\"Average labor share of labor/inflation: {df_2010_plus['labor_share_of_labor_inflation_sentences'].mean():.2%}\")\n","\n","labor_emphasis_cols = [col for col in results_df.columns if col.startswith('labor_emphasis_')]\n","print(\"\\n\" + \"-\"*70)\n","print(\"AVERAGE LABOR EMPHASIS VECTORS (2010-CURRENT)\")\n","print(\"-\"*70)\n","for col in sorted(labor_emphasis_cols):\n","    indicator_name = col.replace('labor_emphasis_', '')\n","    avg_emphasis = df_2010_plus[col].mean()\n","    print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","total_labor_emphasis = df_2010_plus[labor_emphasis_cols].mean().sum()\n","print(f\"\\n{'Total':20s}: {total_labor_emphasis:.4f}\")\n","\n","inflation_emphasis_cols = [col for col in results_df.columns if col.startswith('inflation_emphasis_')]\n","print(\"\\n\" + \"-\"*70)\n","print(\"AVERAGE INFLATION EMPHASIS VECTORS (2010-CURRENT)\")\n","print(\"-\"*70)\n","for col in sorted(inflation_emphasis_cols):\n","    indicator_name = col.replace('inflation_emphasis_', '')\n","    avg_emphasis = df_2010_plus[col].mean()\n","    print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","total_inflation_emphasis = df_2010_plus[inflation_emphasis_cols].mean().sum()\n","print(f\"\\n{'Total':20s}: {total_inflation_emphasis:.4f}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"VALIDATION SET SUMMARY\")\n","print(\"=\"*70)\n","summary_stats = {\n","    'total_sentences_in_corpus': len(sentences_df),\n","    'validation_set_size': len(validation_df),\n","    'labor_only_count': len(validation_df[validation_df['classification'] == 'Labor']),\n","    'inflation_only_count': len(validation_df[validation_df['classification'] == 'Inflation']),\n","    'both_count': len(validation_df[validation_df['classification'] == 'Both']),\n","    'neither_count': len(validation_df[validation_df['classification'] == 'Neither'])\n","}\n","for key, value in summary_stats.items():\n","    print(f\"{key}: {value}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"SAMPLE OF VALIDATION SET (First 10 rows)\")\n","print(\"=\"*70)\n","print(validation_df.head(10).to_string(index=True, max_colwidth=100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNKIuuOOfzB1","executionInfo":{"status":"ok","timestamp":1760017959931,"user_tz":300,"elapsed":60470,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"da8e8721-3169-4a24-ab71-91ad7a4be501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading dictionaries...\n","Dictionaries loaded successfully!\n","Labor indicators: ['General Labor', 'Employment', 'Unemployment', 'Participation', 'Wages', 'Vacancies', 'Quits', 'Layoffs', 'Hiring']\n","Inflation categories: ['General Inflation', 'Core Measures', 'Headline Measures', 'Sectoral Measures', 'Producer Price Index', 'Wage Inflation', 'Inflation Expectations', 'Commodity Prices']\n","\n","Reading cleaned FOMC minutes...\n","Loaded 199 minutes\n","\n","Analyzing minutes...\n","Processing minutes 1/199...\n","Processing minutes 11/199...\n","Processing minutes 21/199...\n","Processing minutes 31/199...\n","Processing minutes 41/199...\n","Processing minutes 51/199...\n","Processing minutes 61/199...\n","Processing minutes 71/199...\n","Processing minutes 81/199...\n","Processing minutes 91/199...\n","Processing minutes 101/199...\n","Processing minutes 111/199...\n","Processing minutes 121/199...\n","Processing minutes 131/199...\n","Processing minutes 141/199...\n","Processing minutes 151/199...\n","Processing minutes 161/199...\n","Processing minutes 171/199...\n","Processing minutes 181/199...\n","Processing minutes 191/199...\n","\n","Summary dataset saved to: /content/drive/MyDrive/FedComs/Minutes/minutes_content.csv\n","Shape: (199, 76)\n","\n","Total sentences extracted: 43578\n","\n","Classification distribution:\n","classification\n","Neither      30348\n","Inflation     5905\n","Labor         5568\n","Both          1757\n","Name: count, dtype: int64\n","\n","Sampling sentences for validation...\n","\n","Validation set created: /content/drive/MyDrive/FedComs/Validation_Sets/minutes_validate.csv\n","Total sentences in validation set: 45\n","\n","Validation set distribution:\n","classification\n","Inflation    15\n","Labor        15\n","Neither      10\n","Both          5\n","Name: count, dtype: int64\n","\n","======================================================================\n","SUMMARY STATISTICS (2010-CURRENT)\n","======================================================================\n","\n","Number of minutes: 119\n","Date range: 2010-01-27 to 2025-01-29\n","\n","Average sentences per minutes: 257.2\n","Average labor sentences: 48.7\n","Average inflation sentences: 44.5\n","Average sentences on both: 11.3\n","Average labor share of labor/inflation: 59.27%\n","\n","----------------------------------------------------------------------\n","AVERAGE LABOR EMPHASIS VECTORS (2010-CURRENT)\n","----------------------------------------------------------------------\n","Employment          : 0.2153 (21.53%)\n","Hiring              : 0.1587 (15.87%)\n","Layoffs             : 0.0092 (0.92%)\n","Participation       : 0.0842 (8.42%)\n","Quits               : 0.0229 (2.29%)\n","Unemployment        : 0.3052 (30.52%)\n","Vacancies           : 0.0507 (5.07%)\n","Wages               : 0.1539 (15.39%)\n","\n","Total               : 1.0000\n","\n","----------------------------------------------------------------------\n","AVERAGE INFLATION EMPHASIS VECTORS (2010-CURRENT)\n","----------------------------------------------------------------------\n","Commodity_Prices    : 0.1496 (14.96%)\n","Core                : 0.0360 (3.60%)\n","Core_CPI            : 0.0356 (3.56%)\n","Core_PCE            : 0.0400 (4.00%)\n","Energy              : 0.1145 (11.45%)\n","Food                : 0.0294 (2.94%)\n","Goods               : 0.0072 (0.72%)\n","Headline            : 0.0282 (2.82%)\n","Headline_CPI        : 0.0363 (3.63%)\n","Headline_PCE        : 0.1310 (13.10%)\n","Housing             : 0.0045 (0.45%)\n","Inflation_Expectations: 0.3019 (30.19%)\n","PPI                 : 0.0029 (0.29%)\n","Services            : 0.0000 (0.00%)\n","Wage_Inflation      : 0.0829 (8.29%)\n","\n","Total               : 1.0000\n","\n","======================================================================\n","VALIDATION SET SUMMARY\n","======================================================================\n","total_sentences_in_corpus: 43578\n","validation_set_size: 45\n","labor_only_count: 15\n","inflation_only_count: 15\n","both_count: 5\n","neither_count: 10\n","\n","======================================================================\n","SAMPLE OF VALIDATION SET (First 10 rows)\n","======================================================================\n","   sentence_number                                                                                        sentence_text classification      labor_indicators    inflation_indicators minutes_date        minutes_id\n","0              121  Residential real estate prices continued to rise, and the staff noted that although valuations h...      Inflation                                                 2022-07-27  minutes_20220727\n","1               82  Market analysts' earnings forecasts for speculative-grade companies, including those outside the...          Labor                 Wages                           2016-03-16  minutes_20160316\n","2              219  In view of realized and expected labor market conditions and inflation, the Committee decided to...          Labor                                                 2018-09-26  minutes_20180926\n","3              245  On balance, market-based measures of inflation compensation have remained low in recent months, ...      Inflation                        Inflation_Expectations   2019-05-01  minutes_20190501\n","4              256  Job gains have been strong, on average, in recent months, and the unemployment rate has remained...          Labor  Hiring, Unemployment                           2018-12-19  minutes_20181219\n","5              174  However, the recovery in consumer spending was not expected to be particularly rapid beyond this...          Labor            Employment                           2020-06-10  minutes_20200610\n","6               30  The Committee voted unanimously to renew the reciprocal currency arrangements with the Bank of C...          Labor         Participation                           2021-04-28  minutes_20210428\n","7               18              Labor market conditions continued to improve, but labor compensation gains were modest.           Both                 Wages          Wage_Inflation   2015-09-17  minutes_20150917\n","8              112  Nonetheless, the nation's current account deficit apparently continued to increase, a developmen...        Neither                                                 2000-08-22  minutes_20000822\n","9              238  With substantial resource slack continuing to restrain cost pressures and longer-term inflation ...      Inflation                        Inflation_Expectations   2010-06-23  minutes_20100623\n"]}]}]}