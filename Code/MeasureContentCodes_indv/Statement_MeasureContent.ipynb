{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kBRk_zj4e9dQt5s2py8S5KQiU5gSu3Gb","timestamp":1759433333106}],"authorship_tag":"ABX9TyPVzpBrO706aLK3cSbtwuh2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Rot40QDe4DD","executionInfo":{"status":"ok","timestamp":1760017791982,"user_tz":300,"elapsed":16282,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"30cf7624-2341-4e6b-d56d-85f8c114f85e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/Statements\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import os\n","dict_dir = '/content/drive/MyDrive/FedComs/Dictionaries'\n","input_file = '/content/drive/MyDrive/FedComs/Statements/fomc_statements.csv'\n","cleaned_file = '/content/drive/MyDrive/FedComs/Statements/fomc_statements_cleaned.csv'\n","output_dir = '/content/drive/MyDrive/Statements'\n","validation_dir = '/content/drive/MyDrive/FedComs/Validation_Sets'\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","os.chdir(output_dir)\n","\n","# Verify the current working directory\n","print(f\"Current working directory: {os.getcwd()}\")"]},{"cell_type":"code","source":["#@title Step 1: Clean\n","import pandas as pd\n","import re\n","import os\n","\n","# Define paths\n","input_file = '/content/drive/MyDrive/FedComs/Statements/fomc_statements.csv'\n","output_dir = '/content/drive/MyDrive/Statements'\n","\n","# Read the original statements\n","print(\"Reading FOMC statements...\")\n","df = pd.read_csv(input_file)\n","print(f\"Loaded {len(df)} statements\")\n","\n","# Delete statement_20081216\n","print(\"\\nDeleting statement_20081216...\")\n","df = df[df['id'] != 'statement_20081216'].copy()\n","print(f\"Statements remaining: {len(df)}\")\n","\n","# Convert date to datetime for easier filtering\n","df['date'] = pd.to_datetime(df['date'])\n","\n","def clean_text_old(text, date):\n","    \"\"\"Clean text for statements from 2005-12-13 and prior.\"\"\"\n","    # Delete text before and including \"For immediate release\"\n","    match = re.search(r'For immediate release', text, re.IGNORECASE)\n","    if match:\n","        text = text[match.end():]\n","\n","    # Delete text after and including \"YYYY Monetary policy\" where YYYY is the year\n","    year = date.year\n","    pattern = rf'{year}\\s+Monetary policy'\n","    match = re.search(pattern, text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","def clean_text_middle(text):\n","    \"\"\"Clean text for statements from 2006-01-31 to 2020-03-05.\"\"\"\n","    # Delete text before and including the first mention of \"share\"\n","    match = re.search(r'share', text, re.IGNORECASE)\n","    if match:\n","        text = text[match.end():]\n","\n","    # Delete text after and including \"Last Update\"\n","    match = re.search(r'Last Update', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","def clean_text_recent(text):\n","    \"\"\"Clean text for statements after 2020-03-05.\"\"\"\n","    # Find \"For release\" first\n","    for_release_match = re.search(r'For release', text, re.IGNORECASE)\n","\n","    if for_release_match:\n","        # Look for \"share\" after \"For release\"\n","        text_after_release = text[for_release_match.end():]\n","        share_match = re.search(r'share', text_after_release, re.IGNORECASE)\n","\n","        if share_match:\n","            # Calculate position in original text\n","            start_pos = for_release_match.end() + share_match.end()\n","            text = text[start_pos:]\n","    else:\n","        # If no \"For release\" found, just look for \"share\"\n","        match = re.search(r'share', text, re.IGNORECASE)\n","        if match:\n","            text = text[match.end():]\n","\n","    # Delete text after and including \"Last Update\"\n","    match = re.search(r'Last Update', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    # Delete text after and including \"For media inquiries\"\n","    match = re.search(r'For media inquiries', text, re.IGNORECASE)\n","    if match:\n","        text = text[:match.start()]\n","\n","    return text.strip()\n","\n","# Apply cleaning based on date\n","print(\"\\nCleaning statements...\")\n","cleaned_texts = []\n","\n","for idx, row in df.iterrows():\n","    if idx % 20 == 0:\n","        print(f\"Processing statement {idx+1}/{len(df)}...\")\n","\n","    date = row['date']\n","    text = row['text']\n","\n","    # Apply appropriate cleaning function based on date\n","    if date <= pd.Timestamp('2005-12-13'):\n","        cleaned_text = clean_text_old(text, date)\n","    elif date <= pd.Timestamp('2020-03-05'):\n","        cleaned_text = clean_text_middle(text)\n","    else:\n","        cleaned_text = clean_text_recent(text)\n","\n","    cleaned_texts.append(cleaned_text)\n","\n","# Create new dataframe with cleaned text\n","df_cleaned = df.copy()\n","df_cleaned['text'] = cleaned_texts\n","\n","# Convert date back to string format to match original\n","df_cleaned['date'] = df_cleaned['date'].dt.strftime('%Y-%m-%d')\n","\n","# Save cleaned statements\n","output_file = os.path.join(output_dir, 'fomc_statements_cleaned.csv')\n","df_cleaned.to_csv(output_file, index=False)\n","\n","print(f\"\\nCleaned statements saved to: {output_file}\")\n","print(f\"Total statements: {len(df_cleaned)}\")\n","\n","# Display some statistics about the cleaning\n","print(\"\\n\" + \"=\"*70)\n","print(\"CLEANING STATISTICS\")\n","print(\"=\"*70)\n","\n","# Calculate average text length before and after\n","original_lengths = df['text'].str.len()\n","cleaned_lengths = df_cleaned['text'].str.len()\n","\n","print(f\"\\nAverage original text length: {original_lengths.mean():.0f} characters\")\n","print(f\"Average cleaned text length: {cleaned_lengths.mean():.0f} characters\")\n","print(f\"Average reduction: {(original_lengths.mean() - cleaned_lengths.mean()):.0f} characters ({((1 - cleaned_lengths.mean()/original_lengths.mean())*100):.1f}%)\")\n","\n","# Show examples from each period\n","print(\"\\n\" + \"=\"*70)\n","print(\"EXAMPLES OF CLEANED TEXT\")\n","print(\"=\"*70)\n","\n","# Example from old period (2005 and prior)\n","old_example = df_cleaned[df_cleaned['date'] <= '2005-12-13'].iloc[-1]\n","print(f\"\\nOLD PERIOD (2005 and prior)\")\n","print(f\"Date: {old_example['date']}\")\n","print(f\"First 200 characters: {old_example['text'][:200]}...\")\n","\n","# Example from middle period (2006-2020-03-05)\n","middle_example = df_cleaned[(df_cleaned['date'] > '2005-12-13') & (df_cleaned['date'] <= '2020-03-05')].iloc[0]\n","print(f\"\\nMIDDLE PERIOD (2006 to March 2020)\")\n","print(f\"Date: {middle_example['date']}\")\n","print(f\"First 200 characters: {middle_example['text'][:200]}...\")\n","\n","# Example from recent period (after 2020-03-05)\n","recent_example = df_cleaned[df_cleaned['date'] > '2020-03-05'].iloc[0]\n","print(f\"\\nRECENT PERIOD (after March 2020)\")\n","print(f\"Date: {recent_example['date']}\")\n","print(f\"First 200 characters: {recent_example['text'][:200]}...\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Cleaning complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEHXo600lmqs","executionInfo":{"status":"ok","timestamp":1759440220310,"user_tz":300,"elapsed":52,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"65e1a8c1-92dc-4362-d7b1-ca0327dc91ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading FOMC statements...\n","Loaded 199 statements\n","\n","Deleting statement_20081216...\n","Statements remaining: 198\n","\n","Cleaning statements...\n","Processing statement 1/198...\n","Processing statement 21/198...\n","Processing statement 41/198...\n","Processing statement 61/198...\n","Processing statement 81/198...\n","Processing statement 101/198...\n","Processing statement 121/198...\n","Processing statement 141/198...\n","Processing statement 161/198...\n","Processing statement 181/198...\n","\n","Cleaned statements saved to: /content/drive/MyDrive/Statements/fomc_statements_cleaned.csv\n","Total statements: 198\n","\n","======================================================================\n","CLEANING STATISTICS\n","======================================================================\n","\n","Average original text length: 4247 characters\n","Average cleaned text length: 2621 characters\n","Average reduction: 1627 characters (38.3%)\n","\n","======================================================================\n","EXAMPLES OF CLEANED TEXT\n","======================================================================\n","\n","OLD PERIOD (2005 and prior)\n","Date: 2005-12-13\n","First 200 characters: The Federal Open Market Committee decided today to raise its target for the federal funds rate by 25 basis points to 4-1/4 percent.\n","Despite elevated energy prices and hurricane-related disruptions, th...\n","\n","MIDDLE PERIOD (2006 to March 2020)\n","Date: 2006-01-31\n","First 200 characters: The Federal Open Market Committee decided today to raise its target for the federal funds rate by 25 basis points to 4-1/2 percent.\n","Although recent economic data have been uneven, the expansion in eco...\n","\n","RECENT PERIOD (after March 2020)\n","Date: 2020-03-15\n","First 200 characters: The coronavirus outbreak has harmed communities and disrupted economic activity in many countries, including the United States. Global financial conditions have also been significantly affected. Avail...\n","\n","======================================================================\n","Cleaning complete!\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import random\n","import os\n","from collections import defaultdict\n","\n","import time\n","seed = int(time.time())  # Use current timestamp as seed\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","# Define paths\n","dict_dir = '/content/drive/MyDrive/FedComs/Dictionaries'\n","input_file = '/content/drive/MyDrive/Statements/fomc_statements_cleaned.csv'\n","output_dir = '/content/drive/MyDrive/Statements'\n","validation_dir = '/content/drive/MyDrive/FedComs/Validation_Sets'\n","\n","# Ensure output directory exists\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Load dictionaries\n","print(\"\\nLoading dictionaries...\")\n","with open(os.path.join(dict_dir, 'labor_indicators.json'), 'r') as f:\n","    LABOR_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_indicators.json'), 'r') as f:\n","    INFLATION_INDICATORS = json.load(f)\n","\n","with open(os.path.join(dict_dir, 'inflation_pattern_mapping.json'), 'r') as f:\n","    INFLATION_PATTERN_TO_INDICATOR = json.load(f)\n","\n","print(\"Dictionaries loaded successfully!\")\n","print(f\"Labor indicators: {list(LABOR_INDICATORS.keys())}\")\n","print(f\"Inflation categories: {list(INFLATION_INDICATORS.keys())}\")\n","\n","# Analysis functions\n","def split_into_sentences(text):\n","    \"\"\"Split text into sentences, preserving initials and abbreviations.\"\"\"\n","    # Step 1: Protect known abbreviations\n","    abbreviations = [\n","        r'\\bSt\\.', r'\\bU\\.S\\.', r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.',\n","        r'\\bProf\\.', r'\\bSr\\.', r'\\bJr\\.', r'\\bvs\\.', r'\\betc\\.', r'\\bi\\.e\\.', r'\\be\\.g\\.'\n","    ]\n","    for abbr in abbreviations:\n","        text = re.sub(abbr, lambda m: m.group(0).replace('.', '<ABBR_PERIOD>'), text)\n","\n","    # Step 2: Protect initials in names (e.g., \"Jerome H. Powell\")\n","    text = re.sub(\n","        r'\\b([A-Z]\\.)(\\s+[A-Z]\\.)?(?:\\s+[A-Z][a-z]+)+',\n","        lambda m: m.group(0).replace('.', '<NAME_PERIOD>'),\n","        text\n","    )\n","\n","    # Step 3: Protect lists of officials (e.g., \"Voting for ... Jerome H. Powell, ...\")\n","    voting_pattern = r'((?:Voting for|Voting against)\\s+[^.!?]+?)([.!?]+\\s+|$)'\n","    voting_matches = []\n","    def store_voting_match(match):\n","        voting_matches.append(match.group(1))\n","        return '<VOTING_LIST_{}>'.format(len(voting_matches) - 1)\n","    text = re.sub(voting_pattern, store_voting_match, text)\n","\n","    # Step 4: Split into sentences\n","    sentences = re.split(r'[.!?]+\\s+', text)\n","    sentences = [s.strip() for s in sentences if s.strip()]\n","\n","    # Step 5: Restore protected periods and voting lists\n","    restored_sentences = []\n","    for sentence in sentences:\n","        sentence = sentence.replace('<ABBR_PERIOD>', '.')\n","        sentence = sentence.replace('<NAME_PERIOD>', '.')\n","        for i, voting_list in enumerate(voting_matches):\n","            placeholder = f'<VOTING_LIST_{i}>'\n","            if placeholder in sentence:\n","                sentence = sentence.replace(placeholder, voting_list)\n","        restored_sentences.append(sentence)\n","\n","    return restored_sentences\n","\n","def check_keywords_in_sentence(sentence, keywords):\n","    \"\"\"Check if any keyword appears in the sentence.\"\"\"\n","    sentence_lower = sentence.lower()\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_employment_indicator(sentence, keywords):\n","    \"\"\"Check for Employment indicator, excluding maximum/full employment.\"\"\"\n","    sentence_lower = sentence.lower()\n","\n","    # Check if sentence contains maximum employment, full employment, or employment goal\n","    if re.search(r'\\b(?:maximum|full)\\s+employment\\b', sentence_lower):\n","        return False\n","    if re.search(r'\\bemployment\\s+goal\\b', sentence_lower):\n","        return False\n","\n","    # Otherwise check for employment keywords normally\n","    for keyword in keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_labor_term(sentence):\n","    \"\"\"Check if sentence contains general labor terms, including 'maximum employment'.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_labor_keywords = LABOR_INDICATORS.get(\"General Labor\", [])\n","    for keyword in general_labor_keywords:\n","        pattern = r'\\b' + re.escape(keyword.lower()) + r'\\b'\n","        if re.search(pattern, sentence_lower):\n","            return True\n","    return False\n","\n","def check_general_inflation_terms(sentence):\n","    \"\"\"Check if sentence contains general inflation terms.\"\"\"\n","    sentence_lower = sentence.lower()\n","    general_inflation_patterns = INFLATION_INDICATORS.get(\"General Inflation\", {}).get(\"general_patterns\", [])\n","    for pattern in general_inflation_patterns:\n","        if re.search(pattern, sentence_lower, re.IGNORECASE):\n","            return True\n","    return False\n","\n","def check_inflation_sentence(sentence):\n","    \"\"\"Check if sentence mentions any inflation indicator and which ones.\"\"\"\n","    mentioned_indicators = set()\n","    sentence_lower = sentence.lower()\n","\n","    for category, subcategories in INFLATION_INDICATORS.items():\n","        for pattern_name, pattern_list in subcategories.items():\n","            for pattern in pattern_list:\n","                if re.search(pattern, sentence_lower, re.IGNORECASE):\n","                    indicator_name = INFLATION_PATTERN_TO_INDICATOR.get(pattern_name, \"Other\")\n","                    mentioned_indicators.add(indicator_name)\n","                    break\n","\n","    # If sentence has both Core_CPI and Core, remove the generic Core\n","    if \"Core_CPI\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Core_PCE and Core, remove the generic Core\n","    if \"Core_PCE\" in mentioned_indicators and \"Core\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Core\")\n","\n","    # If sentence has both Headline_CPI and Headline, remove the generic Headline\n","    if \"Headline_CPI\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    # If sentence has both Headline_PCE and Headline, remove the generic Headline\n","    if \"Headline_PCE\" in mentioned_indicators and \"Headline\" in mentioned_indicators:\n","        mentioned_indicators.discard(\"Headline\")\n","\n","    return mentioned_indicators\n","\n","def classify_sentence(sentence):\n","    \"\"\"Classify a single sentence and return its indicators.\"\"\"\n","    labor_specific_found = False\n","    labor_indicators_in_sentence = set()\n","\n","    # Check all labor indicators EXCEPT \"General Labor\"\n","    for indicator, keywords in LABOR_INDICATORS.items():\n","        if indicator == \"General Labor\":\n","            continue  # Skip general labor for indicator counts\n","\n","        # Use special handling for Employment indicator\n","        if indicator == \"Employment\":\n","            if check_employment_indicator(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","        else:\n","            if check_keywords_in_sentence(sentence, keywords):\n","                labor_indicators_in_sentence.add(indicator)\n","                labor_specific_found = True\n","\n","    labor_general_found = check_general_labor_term(sentence)\n","    labor_found = labor_specific_found or labor_general_found\n","\n","    inflation_indicators_in_sentence = check_inflation_sentence(sentence)\n","    inflation_specific_found = bool(inflation_indicators_in_sentence)\n","\n","    inflation_general_found = check_general_inflation_terms(sentence)\n","    inflation_found = inflation_specific_found or inflation_general_found\n","\n","    if labor_found and inflation_found:\n","        classification = \"Both\"\n","    elif labor_found:\n","        classification = \"Labor\"\n","    elif inflation_found:\n","        classification = \"Inflation\"\n","    else:\n","        classification = \"Neither\"\n","\n","    return {\n","        'classification': classification,\n","        'labor_indicators': list(labor_indicators_in_sentence),\n","        'inflation_indicators': list(inflation_indicators_in_sentence)\n","    }\n","\n","def analyze_statement(text):\n","    \"\"\"Analyze a single statement for labor and inflation content.\"\"\"\n","    sentences = split_into_sentences(text)\n","    total_sentences = len(sentences)\n","\n","    labor_sentences = 0\n","    inflation_sentences = 0\n","    both_sentences = 0\n","\n","    # Only create indicator counts for non-general categories\n","    labor_indicator_counts = {indicator: 0 for indicator in LABOR_INDICATORS.keys() if indicator != \"General Labor\"}\n","    inflation_indicator_list = sorted(list(set(\n","        indicator for indicator in INFLATION_PATTERN_TO_INDICATOR.values()\n","        if indicator not in [\"General_Inflation\", \"Other\"]\n","    )))\n","    inflation_indicator_counts = {indicator: 0 for indicator in inflation_indicator_list}\n","\n","    sentence_data_list = []\n","\n","    for sent_idx, sentence in enumerate(sentences):\n","        classification_result = classify_sentence(sentence)\n","\n","        # Filter out general categories from the indicator lists\n","        labor_indicators_filtered = [ind for ind in classification_result['labor_indicators']\n","                                      if ind != \"General Labor\"]\n","        inflation_indicators_filtered = [ind for ind in classification_result['inflation_indicators']\n","                                          if ind not in [\"General_Inflation\", \"Other\"]]\n","\n","        sentence_data = {\n","            'sentence_number': sent_idx + 1,\n","            'sentence_text': sentence,\n","            'classification': classification_result['classification'],\n","            'labor_indicators': ', '.join(sorted(labor_indicators_filtered)) if labor_indicators_filtered else '',\n","            'inflation_indicators': ', '.join(sorted(inflation_indicators_filtered)) if inflation_indicators_filtered else ''\n","        }\n","        sentence_data_list.append(sentence_data)\n","\n","        labor_specific_found = bool(classification_result['labor_indicators'])\n","        labor_general_found = check_general_labor_term(sentence)\n","        labor_found = labor_specific_found or labor_general_found\n","\n","        inflation_specific_found = bool(classification_result['inflation_indicators'])\n","        inflation_general_found = check_general_inflation_terms(sentence)\n","        inflation_found = inflation_specific_found or inflation_general_found\n","\n","        if labor_found and inflation_found:\n","            both_sentences += 1\n","            labor_sentences += 1\n","            inflation_sentences += 1\n","        elif labor_found:\n","            labor_sentences += 1\n","        elif inflation_found:\n","            inflation_sentences += 1\n","\n","        # Only count specific indicators (not general terms or \"Other\") for emphasis vectors\n","        for indicator in classification_result['labor_indicators']:\n","            if indicator in labor_indicator_counts:\n","                labor_indicator_counts[indicator] += 1\n","\n","        for indicator in classification_result['inflation_indicators']:\n","            if indicator in inflation_indicator_counts:\n","                inflation_indicator_counts[indicator] += 1\n","\n","    total_labor_mentions = sum(labor_indicator_counts.values())\n","    total_inflation_mentions = sum(inflation_indicator_counts.values())\n","\n","    labor_emphasis = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_emphasis[f\"labor_emphasis_{indicator}\"] = count / total_labor_mentions if total_labor_mentions > 0 else 0\n","\n","    inflation_emphasis = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_emphasis[f\"inflation_emphasis_{indicator}\"] = count / total_inflation_mentions if total_inflation_mentions > 0 else 0\n","\n","    labor_sentence_share = {}\n","    for indicator, count in labor_indicator_counts.items():\n","        labor_sentence_share[f\"labor_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    inflation_sentence_share = {}\n","    for indicator, count in inflation_indicator_counts.items():\n","        inflation_sentence_share[f\"inflation_share_total_sentences_{indicator}\"] = count / total_sentences if total_sentences > 0 else 0\n","\n","    labor_inflation_total = labor_sentences + inflation_sentences - both_sentences\n","    labor_share_of_labor_inflation = labor_sentences / labor_inflation_total if labor_inflation_total > 0 else 0\n","\n","    summary_results = {\n","        'sentences_on_labor': labor_sentences,\n","        'sentences_on_inflation': inflation_sentences,\n","        'sentences_on_both': both_sentences,\n","        'total_sentences': total_sentences,\n","        'labor_share_of_labor_inflation_sentences': labor_share_of_labor_inflation\n","    }\n","\n","    for indicator, count in labor_indicator_counts.items():\n","        summary_results[f'labor_{indicator}_count'] = count\n","\n","    for indicator, count in inflation_indicator_counts.items():\n","        summary_results[f'inflation_{indicator}_count'] = count\n","\n","    summary_results.update(labor_emphasis)\n","    summary_results.update(inflation_emphasis)\n","    summary_results.update(labor_sentence_share)\n","    summary_results.update(inflation_sentence_share)\n","\n","    return summary_results, sentence_data_list\n","\n","# Read the cleaned statements\n","print(\"\\nReading cleaned FOMC statements...\")\n","df = pd.read_csv(input_file)\n","print(f\"Loaded {len(df)} statements\")\n","\n","# Process each statement\n","print(\"\\nAnalyzing statements...\")\n","results_list = []\n","all_sentences = []\n","\n","for idx, row in df.iterrows():\n","    if idx % 10 == 0:\n","        print(f\"Processing statement {idx+1}/{len(df)}...\")\n","\n","    summary_results, sentence_data_list = analyze_statement(row['text'])\n","    summary_results['date'] = row['date']\n","    summary_results['id'] = row['id']\n","    results_list.append(summary_results)\n","\n","    for sentence_data in sentence_data_list:\n","        sentence_data['statement_date'] = row['date']\n","        sentence_data['statement_id'] = row['id']\n","        all_sentences.append(sentence_data)\n","\n","# Create summary dataframe (training set)\n","results_df = pd.DataFrame(results_list)\n","cols = ['id', 'date'] + [col for col in results_df.columns if col not in ['id', 'date']]\n","results_df = results_df[cols]\n","results_df = results_df.sort_values('date')\n","\n","# Save summary dataset\n","summary_output_file = os.path.join(output_dir, 'statement_content.csv')\n","results_df.to_csv(summary_output_file, index=False)\n","print(f\"\\nSummary dataset saved to: {summary_output_file}\")\n","print(f\"Shape: {results_df.shape}\")\n","\n","# Create sentence-level dataframe (all sentences)\n","sentences_df = pd.DataFrame(all_sentences)\n","print(f\"\\nTotal sentences extracted: {len(sentences_df)}\")\n","\n","# Print classification distribution\n","print(\"\\nClassification distribution:\")\n","print(sentences_df['classification'].value_counts())\n","\n","# Sample sentences for validation\n","n_labor = 15\n","n_inflation = 15\n","n_both = 5\n","n_neither = 10\n","\n","print(f\"\\nSampling sentences for validation...\")\n","print(f\"  - {n_labor} Labor sentences\")\n","print(f\"  - {n_inflation} Inflation sentences\")\n","print(f\"  - {n_both} Both sentences\")\n","print(f\"  - {n_neither} Neither sentences\")\n","\n","validation_samples = []\n","\n","labor_sentences = sentences_df[sentences_df['classification'] == 'Labor']\n","if len(labor_sentences) >= n_labor:\n","    validation_samples.append(labor_sentences.sample(n=n_labor, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(labor_sentences)} labor sentences available\")\n","    validation_samples.append(labor_sentences)\n","\n","inflation_sentences = sentences_df[sentences_df['classification'] == 'Inflation']\n","if len(inflation_sentences) >= n_inflation:\n","    validation_samples.append(inflation_sentences.sample(n=n_inflation, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(inflation_sentences)} inflation sentences available\")\n","    validation_samples.append(inflation_sentences)\n","\n","both_sentences = sentences_df[sentences_df['classification'] == 'Both']\n","if len(both_sentences) >= n_both:\n","    validation_samples.append(both_sentences.sample(n=n_both, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(both_sentences)} both sentences available\")\n","    validation_samples.append(both_sentences)\n","\n","neither_sentences = sentences_df[sentences_df['classification'] == 'Neither']\n","if len(neither_sentences) >= n_neither:\n","    validation_samples.append(neither_sentences.sample(n=n_neither, random_state=seed))\n","else:\n","    print(f\"Warning: Only {len(neither_sentences)} neither sentences available\")\n","    validation_samples.append(neither_sentences)\n","\n","validation_df = pd.concat(validation_samples, ignore_index=True)\n","validation_df = validation_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","\n","# Save validation dataset\n","validation_output_file = os.path.join(validation_dir, 'statement_validate.csv')\n","validation_df.to_csv(validation_output_file, index=False)\n","\n","print(f\"\\nValidation set created: {validation_output_file}\")\n","print(f\"Total sentences in validation set: {len(validation_df)}\")\n","print(f\"\\nValidation set distribution:\")\n","print(validation_df['classification'].value_counts())\n","\n","# Summary statistics\n","results_df['date'] = pd.to_datetime(results_df['date'])\n","df_2010_plus = results_df[results_df['date'] >= '2010-01-01'].copy()\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"SUMMARY STATISTICS (2010-CURRENT)\")\n","print(\"=\"*70)\n","print(f\"\\nNumber of statements: {len(df_2010_plus)}\")\n","print(f\"Date range: {df_2010_plus['date'].min().strftime('%Y-%m-%d')} to {df_2010_plus['date'].max().strftime('%Y-%m-%d')}\")\n","print(f\"\\nAverage sentences per statement: {df_2010_plus['total_sentences'].mean():.1f}\")\n","print(f\"Average labor sentences: {df_2010_plus['sentences_on_labor'].mean():.1f}\")\n","print(f\"Average inflation sentences: {df_2010_plus['sentences_on_inflation'].mean():.1f}\")\n","print(f\"Average sentences on both: {df_2010_plus['sentences_on_both'].mean():.1f}\")\n","print(f\"Average labor share of labor/inflation: {df_2010_plus['labor_share_of_labor_inflation_sentences'].mean():.2%}\")\n","\n","labor_emphasis_cols = [col for col in results_df.columns if col.startswith('labor_emphasis_')]\n","print(\"\\n\" + \"-\"*70)\n","print(\"AVERAGE LABOR EMPHASIS VECTORS (2010-CURRENT)\")\n","print(\"-\"*70)\n","print(\"(Share of specific labor indicator mentions for each indicator)\")\n","for col in sorted(labor_emphasis_cols):\n","    indicator_name = col.replace('labor_emphasis_', '')\n","    avg_emphasis = df_2010_plus[col].mean()\n","    print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","total_labor_emphasis = df_2010_plus[labor_emphasis_cols].mean().sum()\n","print(f\"\\n{'Total':20s}: {total_labor_emphasis:.4f} (should be ~1.00)\")\n","\n","inflation_emphasis_cols = [col for col in results_df.columns if col.startswith('inflation_emphasis_')]\n","print(\"\\n\" + \"-\"*70)\n","print(\"AVERAGE INFLATION EMPHASIS VECTORS (2010-CURRENT)\")\n","print(\"-\"*70)\n","print(\"(Share of specific inflation indicator mentions for each indicator)\")\n","for col in sorted(inflation_emphasis_cols):\n","    indicator_name = col.replace('inflation_emphasis_', '')\n","    avg_emphasis = df_2010_plus[col].mean()\n","    print(f\"{indicator_name:20s}: {avg_emphasis:.4f} ({avg_emphasis*100:.2f}%)\")\n","\n","total_inflation_emphasis = df_2010_plus[inflation_emphasis_cols].mean().sum()\n","print(f\"\\n{'Total':20s}: {total_inflation_emphasis:.4f} (should be ~1.00)\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"VALIDATION SET SUMMARY\")\n","print(\"=\"*70)\n","summary_stats = {\n","    'total_sentences_in_corpus': len(sentences_df),\n","    'validation_set_size': len(validation_df),\n","    'labor_only_count': len(validation_df[validation_df['classification'] == 'Labor']),\n","    'inflation_only_count': len(validation_df[validation_df['classification'] == 'Inflation']),\n","    'both_count': len(validation_df[validation_df['classification'] == 'Both']),\n","    'neither_count': len(validation_df[validation_df['classification'] == 'Neither'])\n","}\n","for key, value in summary_stats.items():\n","    print(f\"{key}: {value}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"SAMPLE OF VALIDATION SET (First 10 rows)\")\n","print(\"=\"*70)\n","print(validation_df.head(10).to_string(index=True, max_colwidth=100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_EpXe4qgF1a","executionInfo":{"status":"ok","timestamp":1760017871356,"user_tz":300,"elapsed":9875,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"99fe7ba3-edd4-4cdc-f24b-cbbf02dc0518"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading dictionaries...\n","Dictionaries loaded successfully!\n","Labor indicators: ['General Labor', 'Employment', 'Unemployment', 'Participation', 'Wages', 'Vacancies', 'Quits', 'Layoffs', 'Hiring']\n","Inflation categories: ['General Inflation', 'Core Measures', 'Headline Measures', 'Sectoral Measures', 'Producer Price Index', 'Wage Inflation', 'Inflation Expectations', 'Commodity Prices']\n","\n","Reading cleaned FOMC statements...\n","Loaded 198 statements\n","\n","Analyzing statements...\n","Processing statement 1/198...\n","Processing statement 11/198...\n","Processing statement 21/198...\n","Processing statement 31/198...\n","Processing statement 41/198...\n","Processing statement 51/198...\n","Processing statement 61/198...\n","Processing statement 71/198...\n","Processing statement 81/198...\n","Processing statement 91/198...\n","Processing statement 101/198...\n","Processing statement 111/198...\n","Processing statement 121/198...\n","Processing statement 131/198...\n","Processing statement 141/198...\n","Processing statement 151/198...\n","Processing statement 161/198...\n","Processing statement 171/198...\n","Processing statement 181/198...\n","Processing statement 191/198...\n","\n","Summary dataset saved to: /content/drive/MyDrive/Statements/statement_content.csv\n","Shape: (198, 76)\n","\n","Total sentences extracted: 2819\n","\n","Classification distribution:\n","classification\n","Neither      1536\n","Labor         572\n","Inflation     411\n","Both          300\n","Name: count, dtype: int64\n","\n","Sampling sentences for validation...\n","  - 15 Labor sentences\n","  - 15 Inflation sentences\n","  - 5 Both sentences\n","  - 10 Neither sentences\n","\n","Validation set created: /content/drive/MyDrive/FedComs/Validation_Sets/statement_validate.csv\n","Total sentences in validation set: 45\n","\n","Validation set distribution:\n","classification\n","Labor        15\n","Inflation    15\n","Neither      10\n","Both          5\n","Name: count, dtype: int64\n","\n","======================================================================\n","SUMMARY STATISTICS (2010-CURRENT)\n","======================================================================\n","\n","Number of statements: 119\n","Date range: 2010-01-27 to 2025-01-29\n","\n","Average sentences per statement: 17.7\n","Average labor sentences: 7.0\n","Average inflation sentences: 4.4\n","Average sentences on both: 2.4\n","Average labor share of labor/inflation: 78.49%\n","\n","----------------------------------------------------------------------\n","AVERAGE LABOR EMPHASIS VECTORS (2010-CURRENT)\n","----------------------------------------------------------------------\n","(Share of specific labor indicator mentions for each indicator)\n","Employment          : 0.2346 (23.46%)\n","Hiring              : 0.3032 (30.32%)\n","Layoffs             : 0.0000 (0.00%)\n","Participation       : 0.0000 (0.00%)\n","Quits               : 0.0000 (0.00%)\n","Unemployment        : 0.4384 (43.84%)\n","Vacancies           : 0.0000 (0.00%)\n","Wages               : 0.0238 (2.38%)\n","\n","Total               : 1.0000 (should be ~1.00)\n","\n","----------------------------------------------------------------------\n","AVERAGE INFLATION EMPHASIS VECTORS (2010-CURRENT)\n","----------------------------------------------------------------------\n","(Share of specific inflation indicator mentions for each indicator)\n","Commodity_Prices    : 0.1147 (11.47%)\n","Core                : 0.0247 (2.47%)\n","Core_CPI            : 0.0038 (0.38%)\n","Core_PCE            : 0.0000 (0.00%)\n","Energy              : 0.1004 (10.04%)\n","Food                : 0.0017 (0.17%)\n","Goods               : 0.0000 (0.00%)\n","Headline            : 0.0538 (5.38%)\n","Headline_CPI        : 0.0000 (0.00%)\n","Headline_PCE        : 0.0000 (0.00%)\n","Housing             : 0.0000 (0.00%)\n","Inflation_Expectations: 0.7009 (70.09%)\n","PPI                 : 0.0000 (0.00%)\n","Services            : 0.0000 (0.00%)\n","Wage_Inflation      : 0.0000 (0.00%)\n","\n","Total               : 1.0000 (should be ~1.00)\n","\n","======================================================================\n","VALIDATION SET SUMMARY\n","======================================================================\n","total_sentences_in_corpus: 2819\n","validation_set_size: 45\n","labor_only_count: 15\n","inflation_only_count: 15\n","both_count: 5\n","neither_count: 10\n","\n","======================================================================\n","SAMPLE OF VALIDATION SET (First 10 rows)\n","======================================================================\n","   sentence_number                                                                                        sentence_text classification labor_indicators      inflation_indicators statement_date        statement_id\n","0               18  If incoming information broadly supports the Committee's expectation of ongoing improvement in l...          Labor                                                2014-04-30  statement_20140430\n","1                1  The Federal Open Market Committee decided today to raise its target for the federal funds rate b...        Neither                                                2004-09-21  statement_20040921\n","2                4  Inflation has been running somewhat below the Committee's longer-run objective, apart from tempo...      Inflation                   Commodity_Prices, Energy     2013-05-01  statement_20130501\n","3                9  The Committee expects that, with appropriate policy accommodation, economic activity will expand...          Labor                                                2014-07-30  statement_20140730\n","4               14  The Committee will carefully monitor actual and expected inflation developments relative to its ...      Inflation                     Inflation_Expectations     2017-03-15  statement_20170315\n","5                4  On a 12-month basis, both overall inflation and inflation for items other than food and energy r...      Inflation                                   Headline     2018-08-01  statement_20180801\n","6                4  Partly reflecting transitory influences, inflation has been running below the Committee's longer...      Inflation                     Inflation_Expectations     2013-06-19  statement_20130619\n","7               10  Given the economic outlook, the Committee decided to maintain the target range for the federal f...        Neither                                                2016-01-27  statement_20160127\n","8                3  A range of labor market indicators suggests that underutilization of labor resources continues t...          Labor                                                2015-03-18  statement_20150318\n","9               20  Voting for the FOMC monetary policy action were: Janet L<NAME_PERIOD> Yellen, Chair; William C<N...        Neither                                                2016-04-27  statement_20160427\n"]}]}]}