{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQP8ovoPjsbYH5mv4sZDRO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Fed Communications Analysis\n","Produces summary statistics, LaTeX tables, and time series graphs\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","BASE_DIR = '/content/drive/MyDrive/FedComs'\n","OUTPUT_DIR = f'{BASE_DIR}/SummaryStats'\n","DATA_FILE = f'{OUTPUT_DIR}/Combined_Dataset.csv'\n","\n","# Toggle for including variance in tables (set to False to hide variance)\n","INCLUDE_VARIANCE = True\n","\n","# Variables of interest\n","CORE_VARS = [\n","    'total_sentences',\n","    'sentences_on_labor',\n","    'sentences_on_inflation',\n","    'labor_share_of_labor_inflation_sentences'\n","]\n","\n","LABOR_EMPHASIS_VARS = [\n","    'labor_emphasis_Employment',\n","    'labor_emphasis_Unemployment',\n","    'labor_emphasis_Participation',\n","    'labor_emphasis_Wages',\n","    'labor_emphasis_Vacancies',\n","    'labor_emphasis_Quits',\n","    'labor_emphasis_Layoffs',\n","    'labor_emphasis_Hiring'\n","]\n","\n","INFLATION_EMPHASIS_VARS = [\n","    'inflation_emphasis_Commodity_Prices',\n","    'inflation_emphasis_Core',\n","    'inflation_emphasis_Core_CPI',\n","    'inflation_emphasis_Core_PCE',\n","    'inflation_emphasis_Energy',\n","    'inflation_emphasis_Food',\n","    'inflation_emphasis_Goods',\n","    'inflation_emphasis_Headline',\n","    'inflation_emphasis_Headline_CPI',\n","    'inflation_emphasis_Headline_PCE',\n","    'inflation_emphasis_Housing',\n","    'inflation_emphasis_Inflation_Expectations',\n","    'inflation_emphasis_PPI',\n","    'inflation_emphasis_Services',\n","    'inflation_emphasis_Wage_Inflation'\n","]\n","\n","ALL_EMPHASIS_VARS = LABOR_EMPHASIS_VARS + INFLATION_EMPHASIS_VARS\n","\n","# ============================================================================\n","# UTILITY FUNCTIONS\n","# ============================================================================\n","\n","def clean_numeric(df, cols):\n","    \"\"\"Convert columns to numeric, replacing '.' with 0\"\"\"\n","    for col in cols:\n","        if col in df.columns:\n","            df[col] = pd.to_numeric(df[col].replace('.', 0), errors='coerce').fillna(0)\n","    return df\n","\n","def compute_shares(df):\n","    \"\"\"Compute labor and inflation shares of total sentences\"\"\"\n","    df = df.copy()\n","\n","    # Avoid division by zero\n","    df['labor_share'] = np.where(\n","        df['total_sentences'] > 0,\n","        100 * df['sentences_on_labor'] / df['total_sentences'],\n","        0\n","    )\n","\n","    df['inflation_share'] = np.where(\n","        df['total_sentences'] > 0,\n","        100 * df['sentences_on_inflation'] / df['total_sentences'],\n","        0\n","    )\n","\n","    return df\n","\n","def clr_transform(composition):\n","    \"\"\"\n","    Apply Centered Log-Ratio (CLR) transformation to compositional data\n","\n","    Parameters:\n","    -----------\n","    composition : array-like, shape (n_samples, n_components)\n","        Compositional data where rows sum to 1 (or constant)\n","\n","    Returns:\n","    --------\n","    clr_data : array, shape (n_samples, n_components)\n","        CLR-transformed data\n","    \"\"\"\n","    composition = np.array(composition)\n","\n","    # Replace zeros with small value to avoid log(0)\n","    composition = np.where(composition == 0, 1e-10, composition)\n","\n","    # Compute geometric mean for each row\n","    geo_mean = np.exp(np.mean(np.log(composition), axis=1, keepdims=True))\n","\n","    # CLR transformation\n","    clr_data = np.log(composition / geo_mean)\n","\n","    return clr_data\n","\n","def compute_clr_variance(df, emphasis_vars):\n","    \"\"\"\n","    Compute variance of CLR-transformed emphasis vectors\n","\n","    Returns variance of each component after CLR transformation\n","    \"\"\"\n","    # Extract emphasis data\n","    emphasis_data = df[emphasis_vars].values\n","\n","    # Apply CLR transformation\n","    clr_data = clr_transform(emphasis_data)\n","\n","    # Compute variance for each component\n","    variances = np.var(clr_data, axis=0)\n","\n","    return variances, clr_data\n","\n","def compute_euclidean_distance_variance(df, emphasis_vars, mean_vector=None):\n","    \"\"\"\n","    Compute variance of Euclidean distances from mean emphasis vector\n","\n","    Parameters:\n","    -----------\n","    df : DataFrame\n","        Data containing emphasis variables\n","    emphasis_vars : list\n","        List of emphasis variable names\n","    mean_vector : array, optional\n","        Mean vector to compute distance from. If None, uses sample mean.\n","\n","    Returns:\n","    --------\n","    variance : float\n","        Variance of distances\n","    distances : array\n","        Individual distances from mean\n","    \"\"\"\n","    # Extract emphasis data\n","    emphasis_data = df[emphasis_vars].values\n","\n","    # Apply CLR transformation\n","    clr_data = clr_transform(emphasis_data)\n","\n","    # Compute mean vector if not provided\n","    if mean_vector is None:\n","        mean_vector = np.mean(clr_data, axis=0)\n","\n","    # Compute Euclidean distance for each observation\n","    distances = np.sqrt(np.sum((clr_data - mean_vector)**2, axis=1))\n","\n","    # Compute variance of distances\n","    variance = np.var(distances)\n","\n","    return variance, distances, mean_vector\n","\n","def format_mean_var(mean_val, var_val, is_percentage=True, decimals_mean=1, decimals_var=2):\n","    \"\"\"Format mean and variance for LaTeX table\"\"\"\n","    if is_percentage:\n","        if INCLUDE_VARIANCE:\n","            return f\"{mean_val:.{decimals_mean}f} ({var_val:.{decimals_var}f})\"\n","        else:\n","            return f\"{mean_val:.{decimals_mean}f}\"\n","    else:\n","        if INCLUDE_VARIANCE:\n","            return f\"{mean_val:.{decimals_mean}f} ({var_val:.{decimals_var}f})\"\n","        else:\n","            return f\"{mean_val:.{decimals_mean}f}\"\n","\n","def clean_variable_name(var):\n","    \"\"\"Clean up variable name for display in tables\"\"\"\n","    clean_name = var.replace('_', ' ').replace('labor emphasis', 'L:').replace('inflation emphasis', 'I:')\n","    clean_name = clean_name.replace('labor share', 'Lab. share').replace('inflation share', 'Infl. share')\n","    # Special case for labor share of labor inflation sentences\n","    if var == 'labor_share_of_labor_inflation_sentences':\n","        clean_name = 'Lab. share, lab + infl'\n","    return clean_name\n","\n","def compute_year_weighted_mean(data, var):\n","    \"\"\"\n","    Compute mean by first averaging within each year, then averaging across years.\n","    This gives equal weight to each year regardless of number of observations.\n","\n","    Parameters:\n","    -----------\n","    data : DataFrame\n","        Data to compute mean from\n","    var : str\n","        Variable name to compute mean for\n","\n","    Returns:\n","    --------\n","    float : Year-weighted mean\n","    \"\"\"\n","    if len(data) == 0:\n","        return np.nan\n","\n","    # Remove rows with missing year\n","    data_with_year = data[data['year'].notna()].copy()\n","\n","    if len(data_with_year) == 0:\n","        return np.nan\n","\n","    # Compute mean within each year\n","    yearly_means = data_with_year.groupby('year')[var].mean()\n","\n","    # Compute mean across years (equal weight to each year)\n","    year_weighted_mean = yearly_means.mean()\n","\n","    return year_weighted_mean\n","\n","def compute_year_weighted_var(data, var):\n","    \"\"\"\n","    Compute variance using year-weighted approach.\n","    First compute within-year means, then compute variance of those yearly means.\n","\n","    Parameters:\n","    -----------\n","    data : DataFrame\n","        Data to compute variance from\n","    var : str\n","        Variable name to compute variance for\n","\n","    Returns:\n","    --------\n","    float : Year-weighted variance\n","    \"\"\"\n","    if len(data) == 0:\n","        return np.nan\n","\n","    # Remove rows with missing year\n","    data_with_year = data[data['year'].notna()].copy()\n","\n","    if len(data_with_year) == 0:\n","        return np.nan\n","\n","    # Compute mean within each year\n","    yearly_means = data_with_year.groupby('year')[var].mean()\n","\n","    if len(yearly_means) < 2:\n","        return np.nan\n","\n","    # Compute variance across years\n","    year_weighted_var = yearly_means.var()\n","\n","    return year_weighted_var\n","\n","# ============================================================================\n","# LOAD AND PREPARE DATA\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"LOADING DATA\")\n","print(\"=\"*70)\n","\n","df = pd.read_csv(DATA_FILE)\n","print(f\"Loaded {len(df)} records\")\n","\n","# Convert dates\n","df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","df['year'] = pd.to_numeric(df['year'], errors='coerce')\n","df['month'] = pd.to_numeric(df['month'], errors='coerce')\n","\n","# Create quarter variable\n","df['quarter'] = df['date'].dt.to_period('Q')\n","\n","# Clean numeric columns\n","all_numeric_cols = CORE_VARS + ALL_EMPHASIS_VARS\n","df = clean_numeric(df, all_numeric_cols)\n","\n","# Compute shares\n","df = compute_shares(df)\n","\n","# Convert labor_share_of_labor_inflation_sentences to percentage\n","df['labor_share_of_labor_inflation_sentences'] = pd.to_numeric(\n","    df['labor_share_of_labor_inflation_sentences'].replace('.', 0),\n","    errors='coerce'\n",").fillna(0) * 100\n","\n","print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n","print(f\"Records by communication type:\")\n","print(df['com_type'].value_counts())\n","\n","# ============================================================================\n","# DROP ROWS WITH NO LABOR OR INFLATION MENTIONS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"FILTERING DATA: DROPPING ROWS WITH NO LABOR/INFLATION MENTIONS\")\n","print(\"=\"*70)\n","\n","# Calculate sums to identify zero vectors\n","df['labor_emphasis_sum'] = df[LABOR_EMPHASIS_VARS].sum(axis=1)\n","df['inflation_emphasis_sum'] = df[INFLATION_EMPHASIS_VARS].sum(axis=1)\n","\n","# Keep rows that have EITHER labor OR inflation mentions (or both)\n","df_original_len = len(df)\n","df = df[(df['labor_emphasis_sum'] > 0) | (df['inflation_emphasis_sum'] > 0)].copy()\n","\n","print(f\"\\nOriginal rows: {df_original_len}\")\n","print(f\"Rows dropped: {df_original_len - len(df)}\")\n","print(f\"Remaining rows: {len(df)}\")\n","print(f\"\\nRemaining records by communication type:\")\n","print(df['com_type'].value_counts())\n","\n","# Drop the sum columns (no longer needed)\n","df = df.drop(['labor_emphasis_sum', 'inflation_emphasis_sum'], axis=1)\n","\n","# ============================================================================\n","# TABLE 1: AVERAGES ACROSS COMMUNICATION TYPES\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 1: AVERAGES ACROSS COMMUNICATION TYPES (YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","# Variables to summarize\n","summary_vars = ['total_sentences', 'labor_share', 'inflation_share',\n","                'labor_share_of_labor_inflation_sentences'] + ALL_EMPHASIS_VARS\n","\n","results_1 = []\n","\n","# Process each communication type\n","for com_type in ['statements', 'minutes', 'transcripts', 'speeches', 'presscon']:\n","    subset = df[df['com_type'] == com_type]\n","\n","    if len(subset) == 0:\n","        continue\n","\n","    # Overall mean for this type\n","    display_name = com_type.capitalize()\n","    if com_type == 'presscon':\n","        display_name = 'Press'\n","\n","    row = {'Group': display_name}\n","    for var in summary_vars:\n","        row[f'{var}_mean'] = compute_year_weighted_mean(subset, var)\n","        row[f'{var}_var'] = compute_year_weighted_var(subset, var)\n","    results_1.append(row)\n","\n","    # For press conferences, break down by chair vs other\n","    if com_type == 'presscon':\n","        # Chair\n","        chair_subset = subset[subset['role'] == 'Chair']\n","        if len(chair_subset) > 0:\n","            row = {'Group': 'Press: Chair'}\n","            for var in summary_vars:\n","                row[f'{var}_mean'] = compute_year_weighted_mean(chair_subset, var)\n","                row[f'{var}_var'] = compute_year_weighted_var(chair_subset, var)\n","            results_1.append(row)\n","\n","        # Other (non-chair)\n","        other_subset = subset[subset['role'] != 'Chair']\n","        if len(other_subset) > 0:\n","            row = {'Group': 'Press: Other'}\n","            for var in summary_vars:\n","                row[f'{var}_mean'] = compute_year_weighted_mean(other_subset, var)\n","                row[f'{var}_var'] = compute_year_weighted_var(other_subset, var)\n","            results_1.append(row)\n","\n","results_1_df = pd.DataFrame(results_1)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_1_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_1_means += \"\\\\caption{Means Across Communication Types (Year-Weighted)}\\n\"\n","latex_1_means += \"\\\\label{tab:comm_types_means}\\n\"\n","latex_1_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_1_df) + \"}\\n\"\n","latex_1_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header row with group names\n","header = \"Variable\"\n","for group in results_1_df['Group']:\n","    header += f\" & {group}\"\n","header += \" \\\\\\\\\\n\"\n","latex_1_means += header\n","latex_1_means += \"\\\\hline\\n\"\n","\n","# Data rows - one per variable\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_1_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_1_means += line\n","\n","latex_1_means += \"\\\\hline\\\\hline\\n\"\n","latex_1_means += \"\\\\end{tabular}\\n\"\n","latex_1_means += \"\\\\begin{minipage}{\\\\textwidth}\\n\"\n","latex_1_means += \"\\\\small\\n\"\n","latex_1_means += \"Note: Means computed by averaging within years, then across years.\\n\"\n","latex_1_means += \"\\\\end{minipage}\\n\"\n","latex_1_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_1_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_1_var += \"\\\\caption{Variances Across Communication Types (Year-Weighted)}\\n\"\n","    latex_1_var += \"\\\\label{tab:comm_types_var}\\n\"\n","    latex_1_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_1_df) + \"}\\n\"\n","    latex_1_var += \"\\\\hline\\\\hline\\n\"\n","    latex_1_var += header\n","    latex_1_var += \"\\\\hline\\n\"\n","\n","    # Data rows - one per variable\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_1_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_1_var += line\n","\n","    latex_1_var += \"\\\\hline\\\\hline\\n\"\n","    latex_1_var += \"\\\\end{tabular}\\n\"\n","    latex_1_var += \"\\\\begin{minipage}{\\\\textwidth}\\n\"\n","    latex_1_var += \"\\\\small\\n\"\n","    latex_1_var += \"Note: Variance of yearly means.\\n\"\n","    latex_1_var += \"\\\\end{minipage}\\n\"\n","    latex_1_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table1_comm_types_means.tex', 'w') as f:\n","    f.write(latex_1_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table1_comm_types_var.tex', 'w') as f:\n","        f.write(latex_1_var)\n","\n","print(\"✓ Table 1 saved (year-weighted means and variances)\")\n","\n","# ============================================================================\n","# TABLE 1A: AVERAGES 2006-2019 (EXCLUDING PRESS CONFERENCES, YEAR-WEIGHTED)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 1A: AVERAGES 2006-2019 (EXCLUDING PRESS CONFERENCES, YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","# Filter data\n","df_1a = df[(df['year'] >= 2006) & (df['year'] <= 2019) & (df['com_type'] != 'presscon')]\n","\n","results_1a = []\n","\n","for com_type in ['statements', 'minutes', 'transcripts', 'speeches']:\n","    subset = df_1a[df_1a['com_type'] == com_type]\n","\n","    if len(subset) == 0:\n","        continue\n","\n","    row = {'Group': com_type.capitalize()}\n","    for var in summary_vars:\n","        row[f'{var}_mean'] = compute_year_weighted_mean(subset, var)\n","        row[f'{var}_var'] = compute_year_weighted_var(subset, var)\n","    results_1a.append(row)\n","\n","results_1a_df = pd.DataFrame(results_1a)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_1a_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_1a_means += \"\\\\caption{Means Across Communication Types (2006-2019, Excluding Press Conferences, Year-Weighted)}\\n\"\n","latex_1a_means += \"\\\\label{tab:comm_types_2006_2019_means}\\n\"\n","latex_1a_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_1a_df) + \"}\\n\"\n","latex_1a_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header\n","header_1a = \"Variable\"\n","for group in results_1a_df['Group']:\n","    header_1a += f\" & {group}\"\n","header_1a += \" \\\\\\\\\\n\"\n","latex_1a_means += header_1a\n","latex_1a_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_1a_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_1a_means += line\n","\n","latex_1a_means += \"\\\\hline\\\\hline\\n\"\n","latex_1a_means += \"\\\\end{tabular}\\n\"\n","latex_1a_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_1a_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_1a_var += \"\\\\caption{Variances Across Communication Types (2006-2019, Excluding Press Conferences, Year-Weighted)}\\n\"\n","    latex_1a_var += \"\\\\label{tab:comm_types_2006_2019_var}\\n\"\n","    latex_1a_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_1a_df) + \"}\\n\"\n","    latex_1a_var += \"\\\\hline\\\\hline\\n\"\n","    latex_1a_var += header_1a\n","    latex_1a_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_1a_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_1a_var += line\n","\n","    latex_1a_var += \"\\\\hline\\\\hline\\n\"\n","    latex_1a_var += \"\\\\end{tabular}\\n\"\n","    latex_1a_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table1a_comm_types_2006_2019_means.tex', 'w') as f:\n","    f.write(latex_1a_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table1a_comm_types_2006_2019_var.tex', 'w') as f:\n","        f.write(latex_1a_var)\n","\n","print(\"✓ Table 1a saved (means and variances)\")\n","\n","# ============================================================================\n","# TABLE 2: PUBLIC STATEMENTS VS PRIVATE (YEAR-WEIGHTED)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 2: PUBLIC STATEMENTS VS PRIVATE (YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","results_2 = []\n","\n","# Official public: statements + minutes\n","official_public = df[df['com_type'].isin(['statements', 'minutes'])]\n","row = {'Group': 'Official Public'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(official_public, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(official_public, var)\n","results_2.append(row)\n","\n","# Individual public: press conferences + speeches\n","individual_public = df[df['com_type'].isin(['presscon', 'speeches'])]\n","row = {'Group': 'Individual Public'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(individual_public, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(individual_public, var)\n","results_2.append(row)\n","\n","# Private: transcripts\n","private = df[df['com_type'] == 'transcripts']\n","row = {'Group': 'Private'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(private, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(private, var)\n","results_2.append(row)\n","\n","results_2_df = pd.DataFrame(results_2)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_2_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_2_means += \"\\\\caption{Means: Public Statements vs Private (Year-Weighted)}\\n\"\n","latex_2_means += \"\\\\label{tab:public_private_means}\\n\"\n","latex_2_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_2_df) + \"}\\n\"\n","latex_2_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header\n","header_2 = \"Variable\"\n","for group in results_2_df['Group']:\n","    header_2 += f\" & {group}\"\n","header_2 += \" \\\\\\\\\\n\"\n","latex_2_means += header_2\n","latex_2_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_2_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_2_means += line\n","\n","latex_2_means += \"\\\\hline\\\\hline\\n\"\n","latex_2_means += \"\\\\end{tabular}\\n\"\n","latex_2_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_2_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_2_var += \"\\\\caption{Variances: Public Statements vs Private (Year-Weighted)}\\n\"\n","    latex_2_var += \"\\\\label{tab:public_private_var}\\n\"\n","    latex_2_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_2_df) + \"}\\n\"\n","    latex_2_var += \"\\\\hline\\\\hline\\n\"\n","    latex_2_var += header_2\n","    latex_2_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_2_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_2_var += line\n","\n","    latex_2_var += \"\\\\hline\\\\hline\\n\"\n","    latex_2_var += \"\\\\end{tabular}\\n\"\n","    latex_2_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table2_public_private_means.tex', 'w') as f:\n","    f.write(latex_2_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table2_public_private_var.tex', 'w') as f:\n","        f.write(latex_2_var)\n","\n","print(\"✓ Table 2 saved (means and variances)\")\n","\n","# ============================================================================\n","# TABLE 3: CHAIR PUBLIC VS PRIVATE (YEAR-WEIGHTED)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 3: CHAIR PUBLIC VS PRIVATE (YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","results_3 = []\n","\n","# Chair in transcripts\n","chair_transcripts = df[(df['role'] == 'Chair') & (df['com_type'] == 'transcripts')]\n","row = {'Group': 'Chair Private'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(chair_transcripts, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(chair_transcripts, var)\n","results_3.append(row)\n","\n","# Chair in speeches and press conferences\n","chair_public = df[(df['role'] == 'Chair') & (df['com_type'].isin(['speeches', 'presscon']))]\n","row = {'Group': 'Chair Public'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(chair_public, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(chair_public, var)\n","results_3.append(row)\n","\n","results_3_df = pd.DataFrame(results_3)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_3_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_3_means += \"\\\\caption{Means: Chair Public vs Private Communications (Year-Weighted)}\\n\"\n","latex_3_means += \"\\\\label{tab:chair_public_private_means}\\n\"\n","latex_3_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_3_df) + \"}\\n\"\n","latex_3_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header\n","header_3 = \"Variable\"\n","for group in results_3_df['Group']:\n","    header_3 += f\" & {group}\"\n","header_3 += \" \\\\\\\\\\n\"\n","latex_3_means += header_3\n","latex_3_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_3_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_3_means += line\n","\n","latex_3_means += \"\\\\hline\\\\hline\\n\"\n","latex_3_means += \"\\\\end{tabular}\\n\"\n","latex_3_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_3_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_3_var += \"\\\\caption{Variances: Chair Public vs Private Communications (Year-Weighted)}\\n\"\n","    latex_3_var += \"\\\\label{tab:chair_public_private_var}\\n\"\n","    latex_3_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_3_df) + \"}\\n\"\n","    latex_3_var += \"\\\\hline\\\\hline\\n\"\n","    latex_3_var += header_3\n","    latex_3_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_3_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_3_var += line\n","\n","    latex_3_var += \"\\\\hline\\\\hline\\n\"\n","    latex_3_var += \"\\\\end{tabular}\\n\"\n","    latex_3_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table3_chair_public_private_means.tex', 'w') as f:\n","    f.write(latex_3_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table3_chair_public_private_var.tex', 'w') as f:\n","        f.write(latex_3_var)\n","\n","print(\"✓ Table 3 saved (means and variances)\")\n","\n","# ============================================================================\n","# TABLE 4: REGIONAL BANKS VS GOVERNORS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 4: REGIONAL BANKS VS GOVERNORS (TRANSCRIPTS ONLY, YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","transcripts = df[df['com_type'] == 'transcripts']\n","\n","results_4 = []\n","\n","# Governors\n","governors = transcripts[transcripts['role'] == 'Governor']\n","row = {'Group': 'Governors'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(governors, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(governors, var)\n","results_4.append(row)\n","\n","# Regional Presidents\n","regional = transcripts[transcripts['role'] == 'Regional President']\n","row = {'Group': 'Regional Presidents'}\n","for var in summary_vars:\n","    row[f'{var}_mean'] = compute_year_weighted_mean(regional, var)\n","    row[f'{var}_var'] = compute_year_weighted_var(regional, var)\n","results_4.append(row)\n","\n","results_4_df = pd.DataFrame(results_4)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_4_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_4_means += \"\\\\caption{Means: Regional Banks vs Governors (Transcripts Only, Year-Weighted)}\\n\"\n","latex_4_means += \"\\\\label{tab:regional_governors_means}\\n\"\n","latex_4_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_4_df) + \"}\\n\"\n","latex_4_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header\n","header_4 = \"Variable\"\n","for group in results_4_df['Group']:\n","    header_4 += f\" & {group}\"\n","header_4 += \" \\\\\\\\\\n\"\n","latex_4_means += header_4\n","latex_4_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_4_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_4_means += line\n","\n","latex_4_means += \"\\\\hline\\\\hline\\n\"\n","latex_4_means += \"\\\\end{tabular}\\n\"\n","latex_4_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES (includes within-bank variance)\n","if INCLUDE_VARIANCE:\n","    # Compute variance within regional banks\n","    regional_banks = regional['institution'].unique()\n","    regional_banks = [rb for rb in regional_banks if rb != '.']\n","\n","    results_4_var = []\n","\n","    # Add variance for governors\n","    row_var = {'Group': 'Governors'}\n","    for var in summary_vars:\n","        row_var[f'{var}_var'] = compute_year_weighted_var(governors, var)\n","    results_4_var.append(row_var)\n","\n","    # Add variance for regional presidents\n","    row_var = {'Group': 'Regional Presidents'}\n","    for var in summary_vars:\n","        row_var[f'{var}_var'] = compute_year_weighted_var(regional, var)\n","    results_4_var.append(row_var)\n","\n","    # Add variance of bank-level means\n","    row_var = {'Group': 'Var of Bank Means'}\n","    for var in summary_vars:\n","        bank_means = []\n","        for bank in regional_banks:\n","            bank_subset = regional[regional['institution'] == bank]\n","            if len(bank_subset) > 0:\n","                bank_means.append(compute_year_weighted_mean(bank_subset, var))\n","\n","        if len(bank_means) > 0:\n","            row_var[f'{var}_var'] = np.var(bank_means)\n","        else:\n","            row_var[f'{var}_var'] = np.nan\n","    results_4_var.append(row_var)\n","\n","    results_4_var_df = pd.DataFrame(results_4_var)\n","\n","    latex_4_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_4_var += \"\\\\caption{Variances: Regional Banks vs Governors (Transcripts Only, Year-Weighted)}\\n\"\n","    latex_4_var += \"\\\\label{tab:regional_governors_var}\\n\"\n","    latex_4_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_4_var_df) + \"}\\n\"\n","    latex_4_var += \"\\\\hline\\\\hline\\n\"\n","\n","    # Header\n","    header_4_var = \"Variable\"\n","    for group in results_4_var_df['Group']:\n","        header_4_var += f\" & {group}\"\n","    header_4_var += \" \\\\\\\\\\n\"\n","    latex_4_var += header_4_var\n","    latex_4_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_4_var_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            if pd.isna(var_val):\n","                line += \" & ---\"\n","            else:\n","                line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_4_var += line\n","\n","    latex_4_var += \"\\\\hline\\\\hline\\n\"\n","    latex_4_var += \"\\\\end{tabular}\\n\"\n","    latex_4_var += \"\\\\begin{minipage}{\\\\textwidth}\\n\"\n","    latex_4_var += \"\\\\small\\n\"\n","    latex_4_var += \"Note: Last column shows variance of bank-level means.\\n\"\n","    latex_4_var += \"\\\\end{minipage}\\n\"\n","    latex_4_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table4_regional_governors_means.tex', 'w') as f:\n","    f.write(latex_4_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table4_regional_governors_var.tex', 'w') as f:\n","        f.write(latex_4_var)\n","\n","print(\"✓ Table 4 saved (means and variances)\")\n","\n","# ============================================================================\n","# TABLE 5: NOMINATING PRESIDENT\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 5: GOVERNORS BY NOMINATING PRESIDENT (YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","# Governors only, speeches and transcripts\n","gov_data = df[(df['role'] == 'Governor') &\n","              (df['com_type'].isin(['speeches', 'transcripts'])) &\n","              (df['president'] != '.')]\n","\n","results_5 = []\n","\n","presidents = sorted(gov_data['president'].unique())\n","\n","for president in presidents:\n","    subset = gov_data[gov_data['president'] == president]\n","\n","    row = {'Group': president}\n","    for var in summary_vars:\n","        row[f'{var}_mean'] = compute_year_weighted_mean(subset, var)\n","        row[f'{var}_var'] = compute_year_weighted_var(subset, var)\n","    results_5.append(row)\n","\n","results_5_df = pd.DataFrame(results_5)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_5_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","latex_5_means += \"\\\\caption{Means: Governors by Nominating President (Speeches and Transcripts, Year-Weighted)}\\n\"\n","latex_5_means += \"\\\\label{tab:nominating_president_means}\\n\"\n","latex_5_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_5_df) + \"}\\n\"\n","latex_5_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header\n","header_5 = \"Variable\"\n","for group in results_5_df['Group']:\n","    header_5 += f\" & {group}\"\n","header_5 += \" \\\\\\\\\\n\"\n","latex_5_means += header_5\n","latex_5_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_5_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_5_means += line\n","\n","latex_5_means += \"\\\\hline\\\\hline\\n\"\n","latex_5_means += \"\\\\end{tabular}\\n\"\n","latex_5_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_5_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\small\\n\"\n","    latex_5_var += \"\\\\caption{Variances: Governors by Nominating President (Speeches and Transcripts, Year-Weighted)}\\n\"\n","    latex_5_var += \"\\\\label{tab:nominating_president_var}\\n\"\n","    latex_5_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_5_df) + \"}\\n\"\n","    latex_5_var += \"\\\\hline\\\\hline\\n\"\n","    latex_5_var += header_5\n","    latex_5_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_5_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_5_var += line\n","\n","    latex_5_var += \"\\\\hline\\\\hline\\n\"\n","    latex_5_var += \"\\\\end{tabular}\\n\"\n","    latex_5_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table5_nominating_president_means.tex', 'w') as f:\n","    f.write(latex_5_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table5_nominating_president_var.tex', 'w') as f:\n","        f.write(latex_5_var)\n","\n","print(\"✓ Table 5 saved (means and variances)\")\n","\n","# ============================================================================\n","# TABLE 6: REGIONAL BANKS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TABLE 6: REGIONAL BANK PRESIDENTS (TRANSCRIPTS, YEAR-WEIGHTED)\")\n","print(\"=\"*70)\n","\n","# Regional presidents in transcripts\n","regional_transcripts = df[(df['com_type'] == 'transcripts') &\n","                          (df['role'] == 'Regional President') &\n","                          (df['institution'] != '.')]\n","\n","results_6 = []\n","\n","banks = sorted(regional_transcripts['institution'].unique())\n","\n","for bank in banks:\n","    subset = regional_transcripts[regional_transcripts['institution'] == bank]\n","\n","    row = {'Group': bank}\n","    for var in summary_vars:\n","        row[f'{var}_mean'] = compute_year_weighted_mean(subset, var)\n","        row[f'{var}_var'] = compute_year_weighted_var(subset, var)\n","    results_6.append(row)\n","\n","results_6_df = pd.DataFrame(results_6)\n","\n","# Create transposed LaTeX table for MEANS\n","latex_6_means = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\tiny\\n\"\n","latex_6_means += \"\\\\caption{Means: Regional Bank Presidents (Transcripts Only, Year-Weighted)}\\n\"\n","latex_6_means += \"\\\\label{tab:regional_banks_means}\\n\"\n","latex_6_means += \"\\\\begin{tabular}{l\" + \"r\" * len(results_6_df) + \"}\\n\"\n","latex_6_means += \"\\\\hline\\\\hline\\n\"\n","\n","# Header (abbreviated bank names to fit)\n","header_6 = \"Variable\"\n","for group in results_6_df['Group']:\n","    # Abbreviate bank names for table width\n","    abbrev = group.replace('St. Louis', 'STL').replace('Kansas City', 'KC').replace('San Francisco', 'SF')\n","    header_6 += f\" & {abbrev}\"\n","header_6 += \" \\\\\\\\\\n\"\n","latex_6_means += header_6\n","latex_6_means += \"\\\\hline\\n\"\n","\n","# Data rows\n","for var in summary_vars:\n","    clean_name = clean_variable_name(var)\n","\n","    line = clean_name\n","    for _, row in results_6_df.iterrows():\n","        mean_val = row[f'{var}_mean']\n","        # Multiply emphasis variables by 100 for display\n","        if 'emphasis' in var:\n","            mean_val = mean_val * 100\n","        is_pct = var not in ['total_sentences']\n","        decimals = 1 if is_pct else 0\n","        line += f\" & {mean_val:.{decimals}f}\"\n","    line += \" \\\\\\\\\\n\"\n","    latex_6_means += line\n","\n","latex_6_means += \"\\\\hline\\\\hline\\n\"\n","latex_6_means += \"\\\\end{tabular}\\n\"\n","latex_6_means += \"\\\\end{table}\\n\"\n","\n","# Create transposed LaTeX table for VARIANCES\n","if INCLUDE_VARIANCE:\n","    latex_6_var = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\tiny\\n\"\n","    latex_6_var += \"\\\\caption{Variances: Regional Bank Presidents (Transcripts Only, Year-Weighted)}\\n\"\n","    latex_6_var += \"\\\\label{tab:regional_banks_var}\\n\"\n","    latex_6_var += \"\\\\begin{tabular}{l\" + \"r\" * len(results_6_df) + \"}\\n\"\n","    latex_6_var += \"\\\\hline\\\\hline\\n\"\n","    latex_6_var += header_6\n","    latex_6_var += \"\\\\hline\\n\"\n","\n","    for var in summary_vars:\n","        clean_name = clean_variable_name(var)\n","\n","        line = clean_name\n","        for _, row in results_6_df.iterrows():\n","            var_val = row[f'{var}_var']\n","            line += f\" & {var_val:.2f}\"\n","        line += \" \\\\\\\\\\n\"\n","        latex_6_var += line\n","\n","    latex_6_var += \"\\\\hline\\\\hline\\n\"\n","    latex_6_var += \"\\\\end{tabular}\\n\"\n","    latex_6_var += \"\\\\end{table}\\n\"\n","\n","# Save tables\n","with open(f'{OUTPUT_DIR}/table6_regional_banks_means.tex', 'w') as f:\n","    f.write(latex_6_means)\n","\n","if INCLUDE_VARIANCE:\n","    with open(f'{OUTPUT_DIR}/table6_regional_banks_var.tex', 'w') as f:\n","        f.write(latex_6_var)\n","\n","print(\"✓ Table 6 saved (means and variances)\")\n","\n","# ============================================================================\n","# FIGURE 7: TIME SERIES BY YEAR - TOTAL SENTENCES\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"FIGURE 7: TIME SERIES GRAPHS\")\n","print(\"=\"*70)\n","\n","# Prepare yearly aggregates\n","yearly_data = df.groupby(['year', 'com_type']).agg({\n","    'total_sentences': 'sum',\n","    'sentences_on_labor': 'sum',\n","    'sentences_on_inflation': 'sum'\n","}).reset_index()\n","\n","# Compute shares\n","yearly_data['labor_share'] = np.where(\n","    yearly_data['total_sentences'] > 0,\n","    100 * yearly_data['sentences_on_labor'] / yearly_data['total_sentences'],\n","    0\n",")\n","\n","yearly_data['inflation_share'] = np.where(\n","    yearly_data['total_sentences'] > 0,\n","    100 * yearly_data['sentences_on_inflation'] / yearly_data['total_sentences'],\n","    0\n",")\n","\n","# Set up plotting style\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette(\"husl\")\n","\n","# Plot 7a: Total sentences\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","for com_type in ['statements', 'minutes', 'transcripts', 'speeches', 'presscon']:\n","    subset = yearly_data[yearly_data['com_type'] == com_type]\n","    if len(subset) > 0:\n","        ax.plot(subset['year'], subset['total_sentences'],\n","                marker='o', linewidth=2, label=com_type.capitalize())\n","\n","ax.set_xlabel('Year', fontsize=12)\n","ax.set_ylabel('Total Sentences', fontsize=12)\n","ax.set_title('Total Sentences by Communication Type (Annual)', fontsize=14, fontweight='bold')\n","ax.legend(loc='best', frameon=True)\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f'{OUTPUT_DIR}/figure7a_total_sentences.png', dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(\"✓ Figure 7a: Total sentences saved\")\n","\n","# Plot 7b: Labor share\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","for com_type in ['statements', 'minutes', 'transcripts', 'speeches', 'presscon']:\n","    subset = yearly_data[yearly_data['com_type'] == com_type]\n","    if len(subset) > 0:\n","        ax.plot(subset['year'], subset['labor_share'],\n","                marker='o', linewidth=2, label=com_type.capitalize())\n","\n","ax.set_xlabel('Year', fontsize=12)\n","ax.set_ylabel('Labor Share (%)', fontsize=12)\n","ax.set_title('Labor Share of Sentences by Communication Type (Annual)', fontsize=14, fontweight='bold')\n","ax.legend(loc='best', frameon=True)\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f'{OUTPUT_DIR}/figure7b_labor_share.png', dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(\"✓ Figure 7b: Labor share saved\")\n","\n","# Plot 7c: Inflation share\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","for com_type in ['statements', 'minutes', 'transcripts', 'speeches', 'presscon']:\n","    subset = yearly_data[yearly_data['com_type'] == com_type]\n","    if len(subset) > 0:\n","        ax.plot(subset['year'], subset['inflation_share'],\n","                marker='o', linewidth=2, label=com_type.capitalize())\n","\n","ax.set_xlabel('Year', fontsize=12)\n","ax.set_ylabel('Inflation Share (%)', fontsize=12)\n","ax.set_title('Inflation Share of Sentences by Communication Type (Annual)', fontsize=14, fontweight='bold')\n","ax.legend(loc='best', frameon=True)\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f'{OUTPUT_DIR}/figure7c_inflation_share.png', dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(\"✓ Figure 7c: Inflation share saved\")\n","\n","# ============================================================================\n","# FIGURE 8: QUARTERLY VARIANCE OF EUCLIDEAN DISTANCES\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"FIGURE 8: QUARTERLY VARIANCE OF EUCLIDEAN DISTANCES\")\n","print(\"=\"*70)\n","\n","# Function to compute quarterly variance for a specific dataset\n","def compute_quarterly_variance(data, emphasis_vars, var_type='labor'):\n","    \"\"\"Compute variance of Euclidean distances by quarter\"\"\"\n","\n","    quarterly_variance = []\n","\n","    for quarter in sorted(data['quarter'].dropna().unique()):\n","        quarter_data = data[data['quarter'] == quarter]\n","\n","        if len(quarter_data) > 1:  # Need at least 2 observations\n","            variance, _, _ = compute_euclidean_distance_variance(\n","                quarter_data, emphasis_vars\n","            )\n","            quarterly_variance.append({\n","                'quarter': quarter,\n","                'variance': variance,\n","                'n_obs': len(quarter_data)\n","            })\n","\n","    return pd.DataFrame(quarterly_variance)\n","\n","def apply_rolling_average(df, window=3):\n","    \"\"\"Apply rolling average to variance column\"\"\"\n","    df = df.copy()\n","    df['variance_smoothed'] = df['variance'].rolling(window=window, center=False, min_periods=1).mean()\n","    return df\n","\n","# Transcripts\n","transcripts_df = df[df['com_type'] == 'transcripts'].copy()\n","transcripts_df = transcripts_df.dropna(subset=['quarter'])\n","\n","# Compute for labor emphasis\n","labor_var_transcripts = compute_quarterly_variance(\n","    transcripts_df, LABOR_EMPHASIS_VARS, 'labor'\n",")\n","labor_var_transcripts = apply_rolling_average(labor_var_transcripts, window=3)\n","\n","# Compute for inflation emphasis\n","inflation_var_transcripts = compute_quarterly_variance(\n","    transcripts_df, INFLATION_EMPHASIS_VARS, 'inflation'\n",")\n","inflation_var_transcripts = apply_rolling_average(inflation_var_transcripts, window=3)\n","\n","# Speeches\n","speeches_df = df[df['com_type'] == 'speeches'].copy()\n","speeches_df = speeches_df.dropna(subset=['quarter'])\n","\n","labor_var_speeches = compute_quarterly_variance(\n","    speeches_df, LABOR_EMPHASIS_VARS, 'labor'\n",")\n","labor_var_speeches = apply_rolling_average(labor_var_speeches, window=3)\n","\n","inflation_var_speeches = compute_quarterly_variance(\n","    speeches_df, INFLATION_EMPHASIS_VARS, 'inflation'\n",")\n","inflation_var_speeches = apply_rolling_average(inflation_var_speeches, window=3)\n","\n","# Plot 8a: Labor emphasis variance (3-quarter rolling average)\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","if len(labor_var_transcripts) > 0:\n","    quarters_t = pd.PeriodIndex(labor_var_transcripts['quarter']).to_timestamp()\n","    ax.plot(quarters_t, labor_var_transcripts['variance_smoothed'],\n","            marker='o', linewidth=2, label='Transcripts', alpha=0.8)\n","\n","if len(labor_var_speeches) > 0:\n","    quarters_s = pd.PeriodIndex(labor_var_speeches['quarter']).to_timestamp()\n","    ax.plot(quarters_s, labor_var_speeches['variance_smoothed'],\n","            marker='s', linewidth=2, label='Speeches', alpha=0.8)\n","\n","ax.set_xlabel('Quarter', fontsize=12)\n","ax.set_ylabel('Variance of Euclidean Distances (3-Quarter MA)', fontsize=12)\n","ax.set_title('Variance in Labor Emphasis Vectors (3-Quarter Rolling Average)', fontsize=14, fontweight='bold')\n","ax.legend(loc='best', frameon=True)\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f'{OUTPUT_DIR}/figure8a_labor_variance.png', dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(\"✓ Figure 8a: Labor emphasis variance saved\")\n","\n","# Plot 8b: Inflation emphasis variance (3-quarter rolling average)\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","if len(inflation_var_transcripts) > 0:\n","    quarters_t = pd.PeriodIndex(inflation_var_transcripts['quarter']).to_timestamp()\n","    ax.plot(quarters_t, inflation_var_transcripts['variance_smoothed'],\n","            marker='o', linewidth=2, label='Transcripts', alpha=0.8)\n","\n","if len(inflation_var_speeches) > 0:\n","    quarters_s = pd.PeriodIndex(inflation_var_speeches['quarter']).to_timestamp()\n","    ax.plot(quarters_s, inflation_var_speeches['variance_smoothed'],\n","            marker='s', linewidth=2, label='Speeches', alpha=0.8)\n","\n","ax.set_xlabel('Quarter', fontsize=12)\n","ax.set_ylabel('Variance of Euclidean Distances (3-Quarter MA)', fontsize=12)\n","ax.set_title('Variance in Inflation Emphasis Vectors (3-Quarter Rolling Average)', fontsize=14, fontweight='bold')\n","ax.legend(loc='best', frameon=True)\n","ax.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.savefig(f'{OUTPUT_DIR}/figure8b_inflation_variance.png', dpi=300, bbox_inches='tight')\n","plt.close()\n","\n","print(\"✓ Figure 8b: Inflation emphasis variance saved\")\n","\n","# ============================================================================\n","# SUMMARY\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ANALYSIS COMPLETE!\")\n","print(\"=\"*70)\n","print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n","print(\"\\nFiles created:\")\n","print(\"  Tables (LaTeX) - Means:\")\n","print(\"    - table1_comm_types_means.tex\")\n","print(\"    - table1a_comm_types_2006_2019_means.tex\")\n","print(\"    - table2_public_private_means.tex\")\n","print(\"    - table3_chair_public_private_means.tex\")\n","print(\"    - table4_regional_governors_means.tex\")\n","print(\"    - table5_nominating_president_means.tex\")\n","print(\"    - table6_regional_banks_means.tex\")\n","if INCLUDE_VARIANCE:\n","    print(\"\\n  Tables (LaTeX) - Variances:\")\n","    print(\"    - table1_comm_types_var.tex\")\n","    print(\"    - table1a_comm_types_2006_2019_var.tex\")\n","    print(\"    - table2_public_private_var.tex\")\n","    print(\"    - table3_chair_public_private_var.tex\")\n","    print(\"    - table4_regional_governors_var.tex\")\n","    print(\"    - table5_nominating_president_var.tex\")\n","    print(\"    - table6_regional_banks_var.tex\")\n","print(\"\\n  Figures (PNG):\")\n","print(\"    - figure7a_total_sentences.png\")\n","print(\"    - figure7b_labor_share.png\")\n","print(\"    - figure7c_inflation_share.png\")\n","print(\"    - figure8a_labor_variance.png (3-quarter rolling average)\")\n","print(\"    - figure8b_inflation_variance.png (3-quarter rolling average)\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InAacyJF6-eb","executionInfo":{"status":"ok","timestamp":1761757373247,"user_tz":300,"elapsed":12435,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"2379f3e3-0ab5-4861-e1f4-f5a99679929d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","======================================================================\n","LOADING DATA\n","======================================================================\n","Loaded 4277 records\n","\n","Date range: 2000-02-02 00:00:00 to 2025-09-23 00:00:00\n","Records by communication type:\n","com_type\n","transcripts    2589\n","speeches       1121\n","minutes         199\n","statements      198\n","presscon        170\n","Name: count, dtype: int64\n","\n","======================================================================\n","FILTERING DATA: DROPPING ROWS WITH NO LABOR/INFLATION MENTIONS\n","======================================================================\n","\n","Original rows: 4277\n","Rows dropped: 310\n","Remaining rows: 3967\n","\n","Remaining records by communication type:\n","com_type\n","transcripts    2514\n","speeches        909\n","minutes         199\n","statements      175\n","presscon        170\n","Name: count, dtype: int64\n","\n","======================================================================\n","TABLE 1: AVERAGES ACROSS COMMUNICATION TYPES (YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 1 saved (year-weighted means and variances)\n","\n","======================================================================\n","TABLE 1A: AVERAGES 2006-2019 (EXCLUDING PRESS CONFERENCES, YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 1a saved (means and variances)\n","\n","======================================================================\n","TABLE 2: PUBLIC STATEMENTS VS PRIVATE (YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 2 saved (means and variances)\n","\n","======================================================================\n","TABLE 3: CHAIR PUBLIC VS PRIVATE (YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 3 saved (means and variances)\n","\n","======================================================================\n","TABLE 4: REGIONAL BANKS VS GOVERNORS (TRANSCRIPTS ONLY, YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 4 saved (means and variances)\n","\n","======================================================================\n","TABLE 5: GOVERNORS BY NOMINATING PRESIDENT (YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 5 saved (means and variances)\n","\n","======================================================================\n","TABLE 6: REGIONAL BANK PRESIDENTS (TRANSCRIPTS, YEAR-WEIGHTED)\n","======================================================================\n","✓ Table 6 saved (means and variances)\n","\n","======================================================================\n","FIGURE 7: TIME SERIES GRAPHS\n","======================================================================\n","✓ Figure 7a: Total sentences saved\n","✓ Figure 7b: Labor share saved\n","✓ Figure 7c: Inflation share saved\n","\n","======================================================================\n","FIGURE 8: QUARTERLY VARIANCE OF EUCLIDEAN DISTANCES\n","======================================================================\n","✓ Figure 8a: Labor emphasis variance saved\n","✓ Figure 8b: Inflation emphasis variance saved\n","\n","======================================================================\n","ANALYSIS COMPLETE!\n","======================================================================\n","\n","All outputs saved to: /content/drive/MyDrive/FedComs/SummaryStats\n","\n","Files created:\n","  Tables (LaTeX) - Means:\n","    - table1_comm_types_means.tex\n","    - table1a_comm_types_2006_2019_means.tex\n","    - table2_public_private_means.tex\n","    - table3_chair_public_private_means.tex\n","    - table4_regional_governors_means.tex\n","    - table5_nominating_president_means.tex\n","    - table6_regional_banks_means.tex\n","\n","  Tables (LaTeX) - Variances:\n","    - table1_comm_types_var.tex\n","    - table1a_comm_types_2006_2019_var.tex\n","    - table2_public_private_var.tex\n","    - table3_chair_public_private_var.tex\n","    - table4_regional_governors_var.tex\n","    - table5_nominating_president_var.tex\n","    - table6_regional_banks_var.tex\n","\n","  Figures (PNG):\n","    - figure7a_total_sentences.png\n","    - figure7b_labor_share.png\n","    - figure7c_inflation_share.png\n","    - figure8a_labor_variance.png (3-quarter rolling average)\n","    - figure8b_inflation_variance.png (3-quarter rolling average)\n","======================================================================\n"]}]},{"cell_type":"code","source":["\n","# ============================================================================\n","# VIZ BONUS: PCA SCATTER BY OFFICIAL\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"VIZ BONUS: PCA SCATTER BY OFFICIAL (TRANSCRIPTS ONLY)\")\n","print(\"=\"*70)\n","\n","def create_pca_by_official(df, emphasis_vars, labels, title_base, filename_base):\n","    \"\"\"\n","    Create two PCA scatters with one dot per official:\n","    1. Colored by institution\n","    2. Colored by time period\n","    \"\"\"\n","\n","    # Determine which sum to use\n","    if 'labor' in emphasis_vars[0]:\n","        sum_col = 'labor_emphasis_sum'\n","    else:\n","        sum_col = 'inflation_emphasis_sum'\n","\n","    # Data is already filtered to transcripts at the top level\n","    df_filtered = df[\n","        (df[sum_col] > 0) &\n","        (df['speaker'] != 'fomc') &\n","        (df['speaker'] != '.') &\n","        (df['speaker'].notna())\n","    ].copy()\n","\n","    print(f\"  Using transcripts data: {len(df_filtered)} communications\")\n","    if len(df_filtered) > 0:\n","        print(f\"  Date range: {df_filtered['date'].min()} to {df_filtered['date'].max()}\")\n","\n","    # Consolidate speaker codes for same person\n","    SPEAKER_CONSOLIDATION = {\n","        'bbernanke': 'Bernanke',\n","        'bsbernanke': 'Bernanke',\n","        'jpowell': 'Powell',\n","        'jhpowell': 'Powell',\n","        'agreenspan': 'Greenspan',\n","        'jyellen': 'Yellen',\n","    }\n","\n","    df_filtered['speaker_consolidated'] = df_filtered['speaker'].apply(\n","        lambda x: SPEAKER_CONSOLIDATION.get(str(x).lower(), str(x))\n","    )\n","\n","    print(f\"  Original unique speakers: {df_filtered['speaker'].nunique()}\")\n","    print(f\"  Consolidated unique speakers: {df_filtered['speaker_consolidated'].nunique()}\")\n","\n","    if len(df_filtered) < 10:\n","        print(f\"⚠ Not enough data for PCA by official ({len(df_filtered)} rows)\")\n","        return\n","\n","    # Aggregate by consolidated speaker\n","    official_profiles = df_filtered.groupby('speaker_consolidated')[emphasis_vars].mean()\n","\n","    # Also get institution, role, and average date\n","    official_info = df_filtered.groupby('speaker_consolidated').agg({\n","        'institution': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n","        'role': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n","        'date': 'mean'\n","    })\n","\n","    official_profiles = official_profiles.join(official_info)\n","\n","    # Remove officials with too few observations\n","    official_counts = df_filtered.groupby('speaker_consolidated').size()\n","    valid_officials = official_counts[official_counts >= 3].index\n","    official_profiles = official_profiles[official_profiles.index.isin(valid_officials)]\n","\n","    if len(official_profiles) < 10:\n","        print(f\"⚠ Not enough officials for PCA ({len(official_profiles)} officials)\")\n","        return\n","\n","    print(f\"✓ Aggregated {len(official_profiles)} unique officials\")\n","\n","    # Run PCA\n","    X = official_profiles[emphasis_vars].values\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","    pca = PCA(n_components=2)\n","    X_pca = pca.fit_transform(X_scaled)\n","\n","    institutions = official_profiles['institution'].values\n","    roles = official_profiles['role'].values\n","    speakers = official_profiles.index.values\n","    dates = official_profiles['date'].values\n","    years = pd.to_datetime(dates).year\n","\n","    # ========================================================================\n","    # PLOT 1: COLORED BY INSTITUTION\n","    # ========================================================================\n","\n","    fig, ax = plt.subplots(figsize=(14, 10))\n","\n","    institution_colors = {\n","        'Board': '#1f77b4',\n","        'Boston': '#ff7f0e',\n","        'New York': '#2ca02c',\n","        'Philadelphia': '#d62728',\n","        'Cleveland': '#9467bd',\n","        'Richmond': '#8c564b',\n","        'Atlanta': '#e377c2',\n","        'Chicago': '#7f7f7f',\n","        'St. Louis': '#bcbd22',\n","        'Minneapolis': '#17becf',\n","        'Kansas City': '#ff9896',\n","        'Dallas': '#c5b0d5',\n","        'San Francisco': '#c49c94'\n","    }\n","\n","    role_markers = {\n","        'Chair': '*',\n","        'Governor': 'o',\n","        'Regional President': 's',\n","        'Vice Chair': 'D'\n","    }\n","\n","    institutions_plotted = set()\n","\n","    for i, (institution, role, speaker) in enumerate(zip(institutions, roles, speakers)):\n","        color = institution_colors.get(institution, 'gray')\n","        marker = role_markers.get(role, 'o')\n","        size = 400 if role == 'Chair' else 150 if role == 'Governor' else 120\n","\n","        label = institution if institution not in institutions_plotted else \"\"\n","        if label:\n","            institutions_plotted.add(institution)\n","\n","        ax.scatter(\n","            X_pca[i, 0],\n","            X_pca[i, 1],\n","            color=color,\n","            marker=marker,\n","            s=size,\n","            alpha=0.7,\n","            edgecolors='black',\n","            linewidth=2.0 if role == 'Chair' else 1.0,\n","            label=label,\n","            zorder=10 if role == 'Chair' else 5\n","        )\n","\n","    # Add labels for Chairs\n","    for i, (institution, role, speaker) in enumerate(zip(institutions, roles, speakers)):\n","        if role == 'Chair':\n","            ax.annotate(\n","                speaker,\n","                (X_pca[i, 0], X_pca[i, 1]),\n","                xytext=(8, 8),\n","                textcoords='offset points',\n","                fontsize=11,\n","                fontweight='bold',\n","                bbox=dict(boxstyle='round,pad=0.4', facecolor='yellow', alpha=0.8,\n","                         edgecolor='black', linewidth=1.5)\n","            )\n","\n","    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',\n","                  fontsize=13, fontweight='bold')\n","    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',\n","                  fontsize=13, fontweight='bold')\n","    ax.set_title(f'{title_base} - Colored by Institution\\n(Transcripts Only)',\n","                 fontsize=15, fontweight='bold', pad=20)\n","\n","    # Institution legend\n","    handles, legend_labels = ax.get_legend_handles_labels()\n","    institution_order = ['Board'] + sorted([inst for inst in institution_colors.keys() if inst != 'Board'])\n","    sorted_items = [(h, l) for l, h in sorted(zip(legend_labels, handles),\n","                    key=lambda x: institution_order.index(x[0]) if x[0] in institution_order else 999)]\n","\n","    if sorted_items:\n","        sorted_handles, sorted_labels = zip(*sorted_items)\n","        legend1 = ax.legend(sorted_handles, sorted_labels,\n","                           title='Institution', loc='upper left',\n","                           fontsize=9, title_fontsize=10,\n","                           frameon=True, framealpha=0.95, edgecolor='black')\n","        ax.add_artist(legend1)\n","\n","    # Role legend\n","    from matplotlib.lines import Line2D\n","    role_legend_elements = [\n","        Line2D([0], [0], marker='*', color='w', markerfacecolor='gray',\n","               markersize=18, label='Chair', markeredgecolor='black', markeredgewidth=2),\n","        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray',\n","               markersize=11, label='Governor', markeredgecolor='black', markeredgewidth=1),\n","        Line2D([0], [0], marker='s', color='w', markerfacecolor='gray',\n","               markersize=10, label='Regional President', markeredgecolor='black', markeredgewidth=1)\n","    ]\n","    legend2 = ax.legend(handles=role_legend_elements, title='Role',\n","                       loc='upper right', fontsize=9, title_fontsize=10,\n","                       frameon=True, framealpha=0.95, edgecolor='black')\n","\n","    ax.grid(True, alpha=0.3, linestyle='--')\n","\n","    total_var = pca.explained_variance_ratio_[:2].sum()\n","    info_text = f'Total Variance: {total_var*100:.1f}%\\nN Officials: {len(official_profiles)}\\nOne dot per official\\nTranscripts only'\n","    ax.text(0.02, 0.02, info_text, transform=ax.transAxes, fontsize=10,\n","            verticalalignment='bottom',\n","            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9,\n","                     edgecolor='black', linewidth=1))\n","\n","    plt.tight_layout()\n","    filename_inst = f\"{filename_base}_by_institution.png\"\n","    plt.savefig(f'{VIZ_DIR}/{filename_inst}', dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","    print(f\"✓ Saved {filename_inst}\")\n","\n","    # ========================================================================\n","    # PLOT 2: COLORED BY TIME PERIOD\n","    # ========================================================================\n","\n","    fig, ax = plt.subplots(figsize=(14, 10))\n","\n","    from matplotlib import cm\n","    norm = plt.Normalize(vmin=years.min(), vmax=years.max())\n","    cmap = cm.get_cmap('viridis')\n","\n","    for i, (year, role, speaker) in enumerate(zip(years, roles, speakers)):\n","        color = cmap(norm(year))\n","        marker = role_markers.get(role, 'o')\n","        size = 400 if role == 'Chair' else 150 if role == 'Governor' else 120\n","\n","        ax.scatter(\n","            X_pca[i, 0],\n","            X_pca[i, 1],\n","            color=color,\n","            marker=marker,\n","            s=size,\n","            alpha=0.7,\n","            edgecolors='black',\n","            linewidth=2.0 if role == 'Chair' else 1.0,\n","            zorder=10 if role == 'Chair' else 5\n","        )\n","\n","    # Add labels for Chairs\n","    for i, (year, role, speaker) in enumerate(zip(years, roles, speakers)):\n","        if role == 'Chair':\n","            ax.annotate(\n","                speaker,\n","                (X_pca[i, 0], X_pca[i, 1]),\n","                xytext=(8, 8),\n","                textcoords='offset points',\n","                fontsize=11,\n","                fontweight='bold',\n","                bbox=dict(boxstyle='round,pad=0.4', facecolor='yellow', alpha=0.8,\n","                         edgecolor='black', linewidth=1.5)\n","            )\n","\n","    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',\n","                  fontsize=13, fontweight='bold')\n","    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',\n","                  fontsize=13, fontweight='bold')\n","    ax.set_title(f'{title_base} - Colored by Time Period\\n(Transcripts Only)',\n","                 fontsize=15, fontweight='bold', pad=20)\n","\n","    # Colorbar\n","    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n","    sm.set_array([])\n","    cbar = plt.colorbar(sm, ax=ax, label='Year', pad=0.02)\n","    cbar.set_label('Year', fontsize=12, fontweight='bold')\n","\n","    # Role legend\n","    legend_elements = [\n","        Line2D([0], [0], marker='*', color='w', markerfacecolor='gray',\n","               markersize=18, label='Chair', markeredgecolor='black', markeredgewidth=2),\n","        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray',\n","               markersize=11, label='Governor', markeredgecolor='black', markeredgewidth=1),\n","        Line2D([0], [0], marker='s', color='w', markerfacecolor='gray',\n","               markersize=10, label='Regional President', markeredgecolor='black', markeredgewidth=1)\n","    ]\n","    ax.legend(handles=legend_elements, title='Role',\n","             loc='upper left', fontsize=9, title_fontsize=10,\n","             frameon=True, framealpha=0.95, edgecolor='black')\n","\n","    ax.grid(True, alpha=0.3, linestyle='--')\n","\n","    info_text = f'Total Variance: {total_var*100:.1f}%\\nN Officials: {len(official_profiles)}\\nOne dot per official\\nTranscripts only'\n","    ax.text(0.02, 0.02, info_text, transform=ax.transAxes, fontsize=10,\n","            verticalalignment='bottom',\n","            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9,\n","                     edgecolor='black', linewidth=1))\n","\n","    plt.tight_layout()\n","    filename_time = f\"{filename_base}_by_time.png\"\n","    plt.savefig(f'{VIZ_DIR}/{filename_time}', dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","    print(f\"✓ Saved {filename_time}\")\n","    print(f\"  Explained variance: PC1={pca.explained_variance_ratio_[0]*100:.1f}%, PC2={pca.explained_variance_ratio_[1]*100:.1f}%\")\n","\n","# LABOR PCA BY OFFICIAL\n","create_pca_by_official(\n","    df,\n","    LABOR_EMPHASIS_VARS,\n","    LABOR_LABELS,\n","    'PCA of Labor Emphasis by Official',\n","    'viz_bonus_labor'\n",")\n","\n","# INFLATION PCA BY OFFICIAL\n","create_pca_by_official(\n","    df,\n","    INFLATION_EMPHASIS_VARS,\n","    INFLATION_LABELS,\n","    'PCA of Inflation Emphasis by Official',\n","    'viz_bonus_inflation'\n",")\n","\n","# ============================================================================\n","# SUMMARY\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"VISUALIZATION COMPLETE - TRANSCRIPTS ONLY VERSION!\")\n","print(\"=\"*70)\n","print(f\"\\nAll visualizations saved to: {VIZ_DIR}\")\n","print(\"\\nFiles created:\")\n","print(\"  VIZ 1 - Radar Charts:\")\n","print(\"    - viz1a_radar_labor_by_chair.png\")\n","print(\"    - viz1b_radar_inflation_by_chair.png\")\n","print(\"\\n  VIZ 2 - Stacked Area Charts:\")\n","print(\"    - viz2a_stacked_area_labor.png\")\n","print(\"    - viz2b_stacked_area_inflation.png\")\n","print(\"\\n  VIZ 3 - Board vs Regional Comparison:\")\n","print(\"    - viz3a_institution_labor.png\")\n","print(\"    - viz3b_institution_inflation.png\")\n","print(\"\\n  VIZ 4 - PCA Scatter by Chair:\")\n","print(\"    - viz4a_pca_labor_by_chair.png\")\n","print(\"    - viz4b_pca_inflation_by_chair.png\")\n","print(\"\\n  VIZ 5 - Event Studies:\")\n","print(\"    - viz5a_event_study_labor.png\")\n","print(\"    - viz5b_event_study_inflation.png\")\n","print(\"\\n  VIZ BONUS - PCA by Official:\")\n","print(\"    - viz_bonus_labor_by_institution.png\")\n","print(\"    - viz_bonus_labor_by_time.png\")\n","print(\"    - viz_bonus_inflation_by_institution.png\")\n","print(\"    - viz_bonus_inflation_by_time.png\")\n","print(\"\\n\" + \"=\"*70)\n","print(\"NOTE: All visualizations use ONLY transcripts data (~1993-2019)\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34yGyDAPKC_q","executionInfo":{"status":"ok","timestamp":1761761343553,"user_tz":300,"elapsed":7560,"user":{"displayName":"Amanda Michaud","userId":"14525267344116117436"}},"outputId":"4c42c09f-dccd-482a-fa6c-64e3778bb4db"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","VIZ BONUS: PCA SCATTER BY OFFICIAL (TRANSCRIPTS ONLY)\n","======================================================================\n","  Using transcripts data: 2414 communications\n","  Date range: 2000-02-02 00:00:00 to 2019-12-11 00:00:00\n","  Original unique speakers: 53\n","  Consolidated unique speakers: 53\n","✓ Aggregated 52 unique officials\n","✓ Saved viz_bonus_labor_by_institution.png\n","✓ Saved viz_bonus_labor_by_time.png\n","  Explained variance: PC1=30.1%, PC2=21.0%\n","  Using transcripts data: 2217 communications\n","  Date range: 2000-02-02 00:00:00 to 2019-12-11 00:00:00\n","  Original unique speakers: 53\n","  Consolidated unique speakers: 53\n","✓ Aggregated 51 unique officials\n","✓ Saved viz_bonus_inflation_by_institution.png\n","✓ Saved viz_bonus_inflation_by_time.png\n","  Explained variance: PC1=21.5%, PC2=13.5%\n","\n","======================================================================\n","VISUALIZATION COMPLETE - TRANSCRIPTS ONLY VERSION!\n","======================================================================\n","\n","All visualizations saved to: /content/drive/MyDrive/FedComs/SummaryStats/Visualizations_Transcripts_Only\n","\n","Files created:\n","  VIZ 1 - Radar Charts:\n","    - viz1a_radar_labor_by_chair.png\n","    - viz1b_radar_inflation_by_chair.png\n","\n","  VIZ 2 - Stacked Area Charts:\n","    - viz2a_stacked_area_labor.png\n","    - viz2b_stacked_area_inflation.png\n","\n","  VIZ 3 - Board vs Regional Comparison:\n","    - viz3a_institution_labor.png\n","    - viz3b_institution_inflation.png\n","\n","  VIZ 4 - PCA Scatter by Chair:\n","    - viz4a_pca_labor_by_chair.png\n","    - viz4b_pca_inflation_by_chair.png\n","\n","  VIZ 5 - Event Studies:\n","    - viz5a_event_study_labor.png\n","    - viz5b_event_study_inflation.png\n","\n","  VIZ BONUS - PCA by Official:\n","    - viz_bonus_labor_by_institution.png\n","    - viz_bonus_labor_by_time.png\n","    - viz_bonus_inflation_by_institution.png\n","    - viz_bonus_inflation_by_time.png\n","\n","======================================================================\n","NOTE: All visualizations use ONLY transcripts data (~1993-2019)\n","======================================================================\n"]}]}]}